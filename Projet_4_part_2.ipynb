{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rudy-Nzau/Buildings-Energy-Consumption-Forecast/blob/main/Projet_4_part_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QljYfVMPXD_d"
      },
      "source": [
        "# **Import et environnement**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dNyXCqYFXIUo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaPTRM2uXNeV",
        "outputId": "23626181-1eb6-4044-84db-3ccc6a143523"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
            "  after removing the cwd from sys.path.\n"
          ]
        }
      ],
      "source": [
        "#Modification des affichages de colonnes, lignes et largeurs de colonnes pour avoir un maximum d'information\n",
        "pd.set_option('display.max_columns', 200)\n",
        "pd.set_option('display.max_rows', 1000)\n",
        "pd.set_option('display.max_colwidth', -1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIJcpVYfXUWz"
      },
      "source": [
        "## **Chargement data set**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXWU-ctSXSsa"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Openclassrooms/Projet_4/data_energy_cleaned2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sOu3j__Xayy",
        "outputId": "8b2e4a70-a15d-455c-a7f7-60df38d9c2e6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b05e0f8b-1ff1-4283-8387-ec6bddd87ff8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>DataYear</th>\n",
              "      <th>BuildingType</th>\n",
              "      <th>PrimaryPropertyType</th>\n",
              "      <th>CouncilDistrictCode</th>\n",
              "      <th>Neighborhood</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>NumberofBuildings</th>\n",
              "      <th>NumberofFloors</th>\n",
              "      <th>PropertyGFATotal</th>\n",
              "      <th>PropertyGFAParking</th>\n",
              "      <th>PropertyGFABuilding(s)</th>\n",
              "      <th>LargestPropertyUseType</th>\n",
              "      <th>LargestPropertyUseTypeGFA</th>\n",
              "      <th>SecondLargestPropertyUseType</th>\n",
              "      <th>SecondLargestPropertyUseTypeGFA</th>\n",
              "      <th>ThirdLargestPropertyUseType</th>\n",
              "      <th>ThirdLargestPropertyUseTypeGFA</th>\n",
              "      <th>ENERGYSTARScore</th>\n",
              "      <th>SiteEnergyUseWN(kBtu)</th>\n",
              "      <th>Outlier</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>TotalGHGEmissions</th>\n",
              "      <th>Log2-SiteEnergyUseWN(kBtu)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>2015</td>\n",
              "      <td>NonResidential</td>\n",
              "      <td>Residence/Hotel/Senior Care/Housing</td>\n",
              "      <td>7</td>\n",
              "      <td>DOWNTOWN</td>\n",
              "      <td>1927</td>\n",
              "      <td>1.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>88434</td>\n",
              "      <td>0</td>\n",
              "      <td>88434</td>\n",
              "      <td>Residence/Hotel/Senior Care/Housing</td>\n",
              "      <td>88434.0</td>\n",
              "      <td>Other</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Other</td>\n",
              "      <td>0.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>7097539.0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>47.612190</td>\n",
              "      <td>-122.337997</td>\n",
              "      <td>249.43</td>\n",
              "      <td>22.758888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>2015</td>\n",
              "      <td>Nonresidential COS</td>\n",
              "      <td>Other</td>\n",
              "      <td>7</td>\n",
              "      <td>DOWNTOWN</td>\n",
              "      <td>1999</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>97288</td>\n",
              "      <td>37198</td>\n",
              "      <td>60090</td>\n",
              "      <td>Offices</td>\n",
              "      <td>88830.0</td>\n",
              "      <td>Other</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Other</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13045258.0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>47.616439</td>\n",
              "      <td>-122.336764</td>\n",
              "      <td>304.62</td>\n",
              "      <td>23.637022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "      <td>2015</td>\n",
              "      <td>NonResidential</td>\n",
              "      <td>Residence/Hotel/Senior Care/Housing</td>\n",
              "      <td>7</td>\n",
              "      <td>DOWNTOWN</td>\n",
              "      <td>1926</td>\n",
              "      <td>1.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>83008</td>\n",
              "      <td>0</td>\n",
              "      <td>83008</td>\n",
              "      <td>Residence/Hotel/Senior Care/Housing</td>\n",
              "      <td>81352.0</td>\n",
              "      <td>Other</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Other</td>\n",
              "      <td>0.0</td>\n",
              "      <td>25.0</td>\n",
              "      <td>6477493.0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>47.614114</td>\n",
              "      <td>-122.332741</td>\n",
              "      <td>208.46</td>\n",
              "      <td>22.627004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>7</td>\n",
              "      <td>2015</td>\n",
              "      <td>NonResidential</td>\n",
              "      <td>Other</td>\n",
              "      <td>7</td>\n",
              "      <td>DOWNTOWN</td>\n",
              "      <td>1926</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>102761</td>\n",
              "      <td>0</td>\n",
              "      <td>102761</td>\n",
              "      <td>Leisure</td>\n",
              "      <td>102761.0</td>\n",
              "      <td>Other</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Other</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7380086.0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>47.612902</td>\n",
              "      <td>-122.331309</td>\n",
              "      <td>199.99</td>\n",
              "      <td>22.815206</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>2015</td>\n",
              "      <td>NonResidential</td>\n",
              "      <td>Residence/Hotel/Senior Care/Housing</td>\n",
              "      <td>7</td>\n",
              "      <td>DOWNTOWN</td>\n",
              "      <td>1904</td>\n",
              "      <td>1.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>163984</td>\n",
              "      <td>0</td>\n",
              "      <td>163984</td>\n",
              "      <td>Residence/Hotel/Senior Care/Housing</td>\n",
              "      <td>163984.0</td>\n",
              "      <td>Other</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Other</td>\n",
              "      <td>0.0</td>\n",
              "      <td>46.0</td>\n",
              "      <td>13589025.0</td>\n",
              "      <td>Normal</td>\n",
              "      <td>47.602589</td>\n",
              "      <td>-122.332553</td>\n",
              "      <td>331.61</td>\n",
              "      <td>23.695939</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b05e0f8b-1ff1-4283-8387-ec6bddd87ff8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b05e0f8b-1ff1-4283-8387-ec6bddd87ff8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b05e0f8b-1ff1-4283-8387-ec6bddd87ff8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   index  DataYear        BuildingType                  PrimaryPropertyType  \\\n",
              "0  0      2015      NonResidential      Residence/Hotel/Senior Care/Housing   \n",
              "1  5      2015      Nonresidential COS  Other                                 \n",
              "2  6      2015      NonResidential      Residence/Hotel/Senior Care/Housing   \n",
              "3  7      2015      NonResidential      Other                                 \n",
              "4  8      2015      NonResidential      Residence/Hotel/Senior Care/Housing   \n",
              "\n",
              "   CouncilDistrictCode Neighborhood  YearBuilt  NumberofBuildings  \\\n",
              "0  7                    DOWNTOWN     1927       1.0                 \n",
              "1  7                    DOWNTOWN     1999       1.0                 \n",
              "2  7                    DOWNTOWN     1926       1.0                 \n",
              "3  7                    DOWNTOWN     1926       1.0                 \n",
              "4  7                    DOWNTOWN     1904       1.0                 \n",
              "\n",
              "   NumberofFloors  PropertyGFATotal  PropertyGFAParking  \\\n",
              "0  12.0            88434             0                    \n",
              "1  2.0             97288             37198                \n",
              "2  11.0            83008             0                    \n",
              "3  8.0             102761            0                    \n",
              "4  15.0            163984            0                    \n",
              "\n",
              "   PropertyGFABuilding(s)               LargestPropertyUseType  \\\n",
              "0  88434                   Residence/Hotel/Senior Care/Housing   \n",
              "1  60090                   Offices                               \n",
              "2  83008                   Residence/Hotel/Senior Care/Housing   \n",
              "3  102761                  Leisure                               \n",
              "4  163984                  Residence/Hotel/Senior Care/Housing   \n",
              "\n",
              "   LargestPropertyUseTypeGFA SecondLargestPropertyUseType  \\\n",
              "0  88434.0                    Other                         \n",
              "1  88830.0                    Other                         \n",
              "2  81352.0                    Other                         \n",
              "3  102761.0                   Other                         \n",
              "4  163984.0                   Other                         \n",
              "\n",
              "   SecondLargestPropertyUseTypeGFA ThirdLargestPropertyUseType  \\\n",
              "0  0.0                              Other                        \n",
              "1  0.0                              Other                        \n",
              "2  0.0                              Other                        \n",
              "3  0.0                              Other                        \n",
              "4  0.0                              Other                        \n",
              "\n",
              "   ThirdLargestPropertyUseTypeGFA  ENERGYSTARScore  SiteEnergyUseWN(kBtu)  \\\n",
              "0  0.0                             65.0             7097539.0               \n",
              "1  0.0                            NaN               13045258.0              \n",
              "2  0.0                             25.0             6477493.0               \n",
              "3  0.0                            NaN               7380086.0               \n",
              "4  0.0                             46.0             13589025.0              \n",
              "\n",
              "  Outlier   Latitude   Longitude  TotalGHGEmissions  \\\n",
              "0  Normal  47.612190 -122.337997  249.43              \n",
              "1  Normal  47.616439 -122.336764  304.62              \n",
              "2  Normal  47.614114 -122.332741  208.46              \n",
              "3  Normal  47.612902 -122.331309  199.99              \n",
              "4  Normal  47.602589 -122.332553  331.61              \n",
              "\n",
              "   Log2-SiteEnergyUseWN(kBtu)  \n",
              "0  22.758888                   \n",
              "1  23.637022                   \n",
              "2  22.627004                   \n",
              "3  22.815206                   \n",
              "4  23.695939                   "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT9gL-QGX0Ts",
        "outputId": "3c512305-31f8-4d20-b4ea-3917377359f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "index                              int64  \n",
              "DataYear                           int64  \n",
              "BuildingType                       object \n",
              "PrimaryPropertyType                object \n",
              "CouncilDistrictCode                int64  \n",
              "Neighborhood                       object \n",
              "YearBuilt                          int64  \n",
              "NumberofBuildings                  float64\n",
              "NumberofFloors                     float64\n",
              "PropertyGFATotal                   int64  \n",
              "PropertyGFAParking                 int64  \n",
              "PropertyGFABuilding(s)             int64  \n",
              "LargestPropertyUseType             object \n",
              "LargestPropertyUseTypeGFA          float64\n",
              "SecondLargestPropertyUseType       object \n",
              "SecondLargestPropertyUseTypeGFA    float64\n",
              "ThirdLargestPropertyUseType        object \n",
              "ThirdLargestPropertyUseTypeGFA     float64\n",
              "ENERGYSTARScore                    float64\n",
              "SiteEnergyUseWN(kBtu)              float64\n",
              "Outlier                            object \n",
              "Latitude                           float64\n",
              "Longitude                          float64\n",
              "TotalGHGEmissions                  float64\n",
              "Log2-SiteEnergyUseWN(kBtu)         float64\n",
              "dtype: object"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sT-EeobX3du",
        "outputId": "97650e79-632c-4695-d122-60c1cba47346"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5748, 25)"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DACr7QSfX6l2",
        "outputId": "928fb4a6-4d33-4d14-8542-d8cde7bef76b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1459"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.isna().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIqCaFM2X97t"
      },
      "outputs": [],
      "source": [
        "#sauvegarde séparée du energy star score\n",
        "energy_star_score = data['ENERGYSTARScore']\n",
        "data.drop('ENERGYSTARScore', axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ssjdwyhYE4t",
        "outputId": "635e18c4-8ba5-424c-e7e4-2fd3935ef91b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5748, 24)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rXO3Az1LYIFM",
        "outputId": "d8875a47-bcf1-4658-e197-368082a8ef3e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.isna().sum().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkN-b-DBYMyY"
      },
      "source": [
        "# **Vecteurs  et matrices**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQLZ0pvgMhDb"
      },
      "source": [
        "**X et y**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EL7OaekYLb8"
      },
      "outputs": [],
      "source": [
        "y = data.copy()[{'SiteEnergyUseWN(kBtu)', 'TotalGHGEmissions' , 'Log2-SiteEnergyUseWN(kBtu)'}]\n",
        "X = data.copy().drop(['SiteEnergyUseWN(kBtu)', 'TotalGHGEmissions', 'Log2-SiteEnergyUseWN(kBtu)'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tyFR-4wgYcZy",
        "outputId": "f1eb1188-c77f-4d71-e994-287947d793f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "BuildingType                    8 \n",
              "PrimaryPropertyType             12\n",
              "Neighborhood                    13\n",
              "LargestPropertyUseType          12\n",
              "SecondLargestPropertyUseType    12\n",
              "ThirdLargestPropertyUseType     12\n",
              "Outlier                         3 \n",
              "dtype: int64"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.select_dtypes(['category','object']).nunique()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMt4AZZnYe3S",
        "outputId": "d33862bd-a2da-4d69-94e2-cb9a9214a8ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5748, 21)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gliAbEErYiE6"
      },
      "outputs": [],
      "source": [
        "categorical_columns = X.select_dtypes(['category','object']).columns\n",
        "numerical_columns = X.select_dtypes(['int32','float64']).columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VIcT9UUYlbh",
        "outputId": "31787e12-f002-4015-84e4-8a6994e27c9b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['BuildingType', 'PrimaryPropertyType', 'Neighborhood',\n",
              "       'LargestPropertyUseType', 'SecondLargestPropertyUseType',\n",
              "       'ThirdLargestPropertyUseType', 'Outlier'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "categorical_columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFfrRm-FYrko",
        "outputId": "cb7faeb1-82eb-4e70-bbcf-601e98e3cbfa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "ohe = OneHotEncoder(sparse=False)\n",
        "ss = StandardScaler()\n",
        "X[numerical_columns] = ss.fit_transform(X[numerical_columns])\n",
        "ohe.fit_transform(X[categorical_columns])\n",
        "\n",
        "X = pd.merge(X[numerical_columns], \n",
        "          pd.DataFrame(columns = ohe.get_feature_names().tolist(),\n",
        "              data = ohe.fit_transform(X[categorical_columns])),\n",
        "        left_index = True, right_index = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YQ6EUsQYvbg"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "regex = re.compile(r'x\\d_')\n",
        "for column in X.columns:\n",
        "    if regex.search(column):\n",
        "        X[column] = X[column].astype('int32')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wde-vGVjYy7A",
        "outputId": "c80fd1a8-9f24-4e6a-a63c-b9c9bd334d60"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f382058d-4b90-4095-89f8-ab95c38583ca\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NumberofBuildings</th>\n",
              "      <th>NumberofFloors</th>\n",
              "      <th>LargestPropertyUseTypeGFA</th>\n",
              "      <th>SecondLargestPropertyUseTypeGFA</th>\n",
              "      <th>ThirdLargestPropertyUseTypeGFA</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>x0_Campus</th>\n",
              "      <th>x0_Multifamily HR (10+)</th>\n",
              "      <th>x0_Multifamily LR (1-4)</th>\n",
              "      <th>x0_Multifamily MR (5-9)</th>\n",
              "      <th>x0_NonResidential</th>\n",
              "      <th>x0_Nonresidential COS</th>\n",
              "      <th>x0_Nonresidential WA</th>\n",
              "      <th>x0_SPS-District K-12</th>\n",
              "      <th>x1_Education</th>\n",
              "      <th>x1_Facility</th>\n",
              "      <th>x1_Health</th>\n",
              "      <th>x1_Leisure</th>\n",
              "      <th>x1_Office</th>\n",
              "      <th>x1_Offices</th>\n",
              "      <th>x1_Other</th>\n",
              "      <th>x1_Residence/Hotel/Senior Care/Housing</th>\n",
              "      <th>x1_Retail</th>\n",
              "      <th>x1_Storage</th>\n",
              "      <th>x1_Supermarket / Grocery Store</th>\n",
              "      <th>x1_Warehouse</th>\n",
              "      <th>x2_BALLARD</th>\n",
              "      <th>x2_CENTRAL</th>\n",
              "      <th>x2_DELRIDGE</th>\n",
              "      <th>x2_DOWNTOWN</th>\n",
              "      <th>x2_EAST</th>\n",
              "      <th>x2_GREATER DUWAMISH</th>\n",
              "      <th>x2_LAKE UNION</th>\n",
              "      <th>x2_MAGNOLIA / QUEEN ANNE</th>\n",
              "      <th>x2_NORTH</th>\n",
              "      <th>x2_NORTHEAST</th>\n",
              "      <th>x2_NORTHWEST</th>\n",
              "      <th>x2_SOUTHEAST</th>\n",
              "      <th>x2_SOUTHWEST</th>\n",
              "      <th>x3_Education</th>\n",
              "      <th>x3_Facility</th>\n",
              "      <th>x3_Health</th>\n",
              "      <th>x3_Leisure</th>\n",
              "      <th>x3_Office</th>\n",
              "      <th>x3_Offices</th>\n",
              "      <th>x3_Other</th>\n",
              "      <th>x3_Parking</th>\n",
              "      <th>x3_Personal Services (Health/Beauty, Dry Cleaning, etc)</th>\n",
              "      <th>x3_Residence/Hotel/Senior Care/Housing</th>\n",
              "      <th>x3_Retail</th>\n",
              "      <th>x3_Storage</th>\n",
              "      <th>x4_Education</th>\n",
              "      <th>x4_Facility</th>\n",
              "      <th>x4_Health</th>\n",
              "      <th>x4_Leisure</th>\n",
              "      <th>x4_Office</th>\n",
              "      <th>x4_Offices</th>\n",
              "      <th>x4_Other</th>\n",
              "      <th>x4_Parking</th>\n",
              "      <th>x4_Personal Services (Health/Beauty, Dry Cleaning, etc)</th>\n",
              "      <th>x4_Residence/Hotel/Senior Care/Housing</th>\n",
              "      <th>x4_Retail</th>\n",
              "      <th>x4_Storage</th>\n",
              "      <th>x5_Education</th>\n",
              "      <th>x5_Facility</th>\n",
              "      <th>x5_Health</th>\n",
              "      <th>x5_Leisure</th>\n",
              "      <th>x5_Office</th>\n",
              "      <th>x5_Offices</th>\n",
              "      <th>x5_Other</th>\n",
              "      <th>x5_Parking</th>\n",
              "      <th>x5_Personal Services (Health/Beauty, Dry Cleaning, etc)</th>\n",
              "      <th>x5_Residence/Hotel/Senior Care/Housing</th>\n",
              "      <th>x5_Retail</th>\n",
              "      <th>x5_Storage</th>\n",
              "      <th>x6_High Outlier</th>\n",
              "      <th>x6_Low Outlier</th>\n",
              "      <th>x6_Normal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2813</th>\n",
              "      <td>-0.034419</td>\n",
              "      <td>0.489769</td>\n",
              "      <td>0.255048</td>\n",
              "      <td>-0.067008</td>\n",
              "      <td>-0.159065</td>\n",
              "      <td>-0.134186</td>\n",
              "      <td>1.188627</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3657</th>\n",
              "      <td>-0.034419</td>\n",
              "      <td>-0.601920</td>\n",
              "      <td>-0.447785</td>\n",
              "      <td>-0.362042</td>\n",
              "      <td>-0.159065</td>\n",
              "      <td>-1.318319</td>\n",
              "      <td>-1.572254</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3864</th>\n",
              "      <td>-0.034419</td>\n",
              "      <td>-0.874842</td>\n",
              "      <td>-0.620551</td>\n",
              "      <td>-0.362042</td>\n",
              "      <td>-0.159065</td>\n",
              "      <td>-0.293921</td>\n",
              "      <td>-0.012663</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1193</th>\n",
              "      <td>-0.034419</td>\n",
              "      <td>-0.601920</td>\n",
              "      <td>-0.104470</td>\n",
              "      <td>-0.362042</td>\n",
              "      <td>-0.159065</td>\n",
              "      <td>-1.755129</td>\n",
              "      <td>-1.450106</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>123</th>\n",
              "      <td>-0.034419</td>\n",
              "      <td>-0.874842</td>\n",
              "      <td>0.130560</td>\n",
              "      <td>-0.362042</td>\n",
              "      <td>-0.159065</td>\n",
              "      <td>-1.826153</td>\n",
              "      <td>1.455330</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f382058d-4b90-4095-89f8-ab95c38583ca')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f382058d-4b90-4095-89f8-ab95c38583ca button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f382058d-4b90-4095-89f8-ab95c38583ca');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      NumberofBuildings  NumberofFloors  LargestPropertyUseTypeGFA  \\\n",
              "2813 -0.034419           0.489769        0.255048                    \n",
              "3657 -0.034419          -0.601920       -0.447785                    \n",
              "3864 -0.034419          -0.874842       -0.620551                    \n",
              "1193 -0.034419          -0.601920       -0.104470                    \n",
              "123  -0.034419          -0.874842        0.130560                    \n",
              "\n",
              "      SecondLargestPropertyUseTypeGFA  ThirdLargestPropertyUseTypeGFA  \\\n",
              "2813 -0.067008                        -0.159065                         \n",
              "3657 -0.362042                        -0.159065                         \n",
              "3864 -0.362042                        -0.159065                         \n",
              "1193 -0.362042                        -0.159065                         \n",
              "123  -0.362042                        -0.159065                         \n",
              "\n",
              "      Latitude  Longitude  x0_Campus  x0_Multifamily HR (10+)  \\\n",
              "2813 -0.134186  1.188627   0          0                         \n",
              "3657 -1.318319 -1.572254   0          0                         \n",
              "3864 -0.293921 -0.012663   0          0                         \n",
              "1193 -1.755129 -1.450106   0          0                         \n",
              "123  -1.826153  1.455330   0          0                         \n",
              "\n",
              "      x0_Multifamily LR (1-4)  x0_Multifamily MR (5-9)  x0_NonResidential  \\\n",
              "2813  0                        0                        1                   \n",
              "3657  1                        0                        0                   \n",
              "3864  0                        0                        1                   \n",
              "1193  0                        0                        0                   \n",
              "123   0                        0                        0                   \n",
              "\n",
              "      x0_Nonresidential COS  x0_Nonresidential WA  x0_SPS-District K-12  \\\n",
              "2813  0                      0                     0                      \n",
              "3657  0                      0                     0                      \n",
              "3864  0                      0                     0                      \n",
              "1193  0                      0                     1                      \n",
              "123   0                      0                     1                      \n",
              "\n",
              "      x1_Education  x1_Facility  x1_Health  x1_Leisure  x1_Office  x1_Offices  \\\n",
              "2813  0             0            0          0           0          0            \n",
              "3657  0             0            0          0           0          0            \n",
              "3864  0             0            0          0           0          0            \n",
              "1193  1             0            0          0           0          0            \n",
              "123   1             0            0          0           0          0            \n",
              "\n",
              "      x1_Other  x1_Residence/Hotel/Senior Care/Housing  x1_Retail  x1_Storage  \\\n",
              "2813  0         1                                       0          0            \n",
              "3657  0         1                                       0          0            \n",
              "3864  0         0                                       1          0            \n",
              "1193  0         0                                       0          0            \n",
              "123   0         0                                       0          0            \n",
              "\n",
              "      x1_Supermarket / Grocery Store  x1_Warehouse  x2_BALLARD  x2_CENTRAL  \\\n",
              "2813  0                               0             0           1            \n",
              "3657  0                               0             0           0            \n",
              "3864  0                               0             0           0            \n",
              "1193  0                               0             0           0            \n",
              "123   0                               0             0           0            \n",
              "\n",
              "      x2_DELRIDGE  x2_DOWNTOWN  x2_EAST  x2_GREATER DUWAMISH  x2_LAKE UNION  \\\n",
              "2813  0            0            0        0                    0               \n",
              "3657  0            0            0        0                    0               \n",
              "3864  0            1            0        0                    0               \n",
              "1193  1            0            0        0                    0               \n",
              "123   0            0            0        1                    0               \n",
              "\n",
              "      x2_MAGNOLIA / QUEEN ANNE  x2_NORTH  x2_NORTHEAST  x2_NORTHWEST  \\\n",
              "2813  0                         0         0             0              \n",
              "3657  0                         0         0             0              \n",
              "3864  0                         0         0             0              \n",
              "1193  0                         0         0             0              \n",
              "123   0                         0         0             0              \n",
              "\n",
              "      x2_SOUTHEAST  x2_SOUTHWEST  x3_Education  x3_Facility  x3_Health  \\\n",
              "2813  0             0             0             0            0           \n",
              "3657  0             1             0             0            0           \n",
              "3864  0             0             0             0            0           \n",
              "1193  0             0             1             0            0           \n",
              "123   0             0             1             0            0           \n",
              "\n",
              "      x3_Leisure  x3_Office  x3_Offices  x3_Other  x3_Parking  \\\n",
              "2813  0           0          0           0         0            \n",
              "3657  0           0          0           0         0            \n",
              "3864  0           0          0           0         0            \n",
              "1193  0           0          0           0         0            \n",
              "123   0           0          0           0         0            \n",
              "\n",
              "      x3_Personal Services (Health/Beauty, Dry Cleaning, etc)  \\\n",
              "2813  0                                                         \n",
              "3657  0                                                         \n",
              "3864  0                                                         \n",
              "1193  0                                                         \n",
              "123   0                                                         \n",
              "\n",
              "      x3_Residence/Hotel/Senior Care/Housing  x3_Retail  x3_Storage  \\\n",
              "2813  1                                       0          0            \n",
              "3657  1                                       0          0            \n",
              "3864  0                                       1          0            \n",
              "1193  0                                       0          0            \n",
              "123   0                                       0          0            \n",
              "\n",
              "      x4_Education  x4_Facility  x4_Health  x4_Leisure  x4_Office  x4_Offices  \\\n",
              "2813  0             0            0          0           0          0            \n",
              "3657  0             0            0          0           0          0            \n",
              "3864  0             0            0          0           0          0            \n",
              "1193  0             0            0          0           0          0            \n",
              "123   0             0            0          0           0          0            \n",
              "\n",
              "      x4_Other  x4_Parking  \\\n",
              "2813  0         1            \n",
              "3657  1         0            \n",
              "3864  1         0            \n",
              "1193  1         0            \n",
              "123   1         0            \n",
              "\n",
              "      x4_Personal Services (Health/Beauty, Dry Cleaning, etc)  \\\n",
              "2813  0                                                         \n",
              "3657  0                                                         \n",
              "3864  0                                                         \n",
              "1193  0                                                         \n",
              "123   0                                                         \n",
              "\n",
              "      x4_Residence/Hotel/Senior Care/Housing  x4_Retail  x4_Storage  \\\n",
              "2813  0                                       0          0            \n",
              "3657  0                                       0          0            \n",
              "3864  0                                       0          0            \n",
              "1193  0                                       0          0            \n",
              "123   0                                       0          0            \n",
              "\n",
              "      x5_Education  x5_Facility  x5_Health  x5_Leisure  x5_Office  x5_Offices  \\\n",
              "2813  0             0            0          0           0          0            \n",
              "3657  0             0            0          0           0          0            \n",
              "3864  0             0            0          0           0          0            \n",
              "1193  0             0            0          0           0          0            \n",
              "123   0             0            0          0           0          0            \n",
              "\n",
              "      x5_Other  x5_Parking  \\\n",
              "2813  1         0            \n",
              "3657  1         0            \n",
              "3864  1         0            \n",
              "1193  1         0            \n",
              "123   1         0            \n",
              "\n",
              "      x5_Personal Services (Health/Beauty, Dry Cleaning, etc)  \\\n",
              "2813  0                                                         \n",
              "3657  0                                                         \n",
              "3864  0                                                         \n",
              "1193  0                                                         \n",
              "123   0                                                         \n",
              "\n",
              "      x5_Residence/Hotel/Senior Care/Housing  x5_Retail  x5_Storage  \\\n",
              "2813  0                                       0          0            \n",
              "3657  0                                       0          0            \n",
              "3864  0                                       0          0            \n",
              "1193  0                                       0          0            \n",
              "123   0                                       0          0            \n",
              "\n",
              "      x6_High Outlier  x6_Low Outlier  x6_Normal  \n",
              "2813  0                0               1          \n",
              "3657  0                0               1          \n",
              "3864  0                0               1          \n",
              "1193  0                0               1          \n",
              "123   0                0               1          "
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.sample(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgT7PAesY41i"
      },
      "source": [
        "## **Infos sur X et Y**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUfbE5oQY1q4",
        "outputId": "c76e3e2e-2f91-460d-e8f4-a8e8fa430071"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5748, 79)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWC_6F7TY-W_",
        "outputId": "d766697e-9946-4129-886e-2ddf1167a9fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(5748, 3)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.shape\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNBaVN-UZDBV"
      },
      "source": [
        "# **Séparation test/train**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrGyW555ZAyA"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "                        train_test_split(X, \n",
        "                                         y['Log2-SiteEnergyUseWN(kBtu)'],  \n",
        "                                         test_size = 0.2, \n",
        "                                         random_state = 42\n",
        "                                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KrBKyo9ZK_2",
        "outputId": "8822b5ee-32fa-44a1-d38c-15faa8a4550b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4598, 79)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaL-GNqMZNSF",
        "outputId": "9252f6ae-64d1-4066-d79e-97f1ab31514a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1150, 79)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPUv4HX8ZV7M",
        "outputId": "aeb5aa4c-419d-4dc7-8fbd-16ddc513c868"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4598,)"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0u3AHAqZiar",
        "outputId": "215fb7c7-28a8-4739-df76-2917a60432c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1150,)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0vMkWxGxSqj"
      },
      "outputs": [],
      "source": [
        "results = pd.DataFrame({})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPiNcIslxZM0",
        "outputId": "2dbecabf-6c64-4710-f6c5-854d44dc94dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n",
            "0\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "print(X_train.isna().sum().sum())\n",
        "print(X_test.isna().sum().sum())\n",
        "print(y_train.isna().sum().sum())\n",
        "print(y_test.isna().sum().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LCr1SrRdomT"
      },
      "source": [
        "# **Modelisation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emLMjhq8ZmRU"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saVuNmtbdyFW"
      },
      "source": [
        "## **Linear regression/ Ridge Regression / Elastic Net**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sesDp2itdu5f"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import ElasticNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJ4X96RUeBgZ",
        "outputId": "2521b38e-01fa-4223-f594-4b8c6fbf648e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.179e+03, tolerance: 1.146e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.819e+03, tolerance: 1.068e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.680e+03, tolerance: 1.058e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.691e+03, tolerance: 1.059e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.813e+03, tolerance: 1.062e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.179e+03, tolerance: 1.146e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.819e+03, tolerance: 1.068e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.680e+03, tolerance: 1.058e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.691e+03, tolerance: 1.059e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.813e+03, tolerance: 1.062e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.179e+03, tolerance: 1.146e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.819e+03, tolerance: 1.068e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.680e+03, tolerance: 1.058e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.691e+03, tolerance: 1.059e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.813e+03, tolerance: 1.062e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.179e+03, tolerance: 1.146e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.819e+03, tolerance: 1.068e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.680e+03, tolerance: 1.058e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.691e+03, tolerance: 1.059e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.813e+03, tolerance: 1.062e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.177e+02, tolerance: 1.068e+02\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.599e+02, tolerance: 1.058e+02\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.043e+02, tolerance: 1.146e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.177e+02, tolerance: 1.068e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.599e+02, tolerance: 1.058e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.210e+01, tolerance: 1.059e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.830e+01, tolerance: 1.062e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.043e+02, tolerance: 1.146e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.177e+02, tolerance: 1.068e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.599e+02, tolerance: 1.058e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.210e+01, tolerance: 1.059e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.830e+01, tolerance: 1.062e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.710e+01, tolerance: 1.146e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.703e+01, tolerance: 1.068e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.572e+01, tolerance: 1.058e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.617e+01, tolerance: 1.059e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.374e+01, tolerance: 1.062e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.710e+01, tolerance: 1.146e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.703e+01, tolerance: 1.068e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.572e+01, tolerance: 1.058e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.617e+01, tolerance: 1.059e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.374e+01, tolerance: 1.062e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.304e+01, tolerance: 1.146e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.577e+01, tolerance: 1.068e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.972e+01, tolerance: 1.058e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.836e+01, tolerance: 1.059e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.917e+01, tolerance: 1.062e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.304e+01, tolerance: 1.146e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.577e+01, tolerance: 1.068e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.972e+01, tolerance: 1.058e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.836e+01, tolerance: 1.059e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.917e+01, tolerance: 1.062e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.432e+01, tolerance: 1.146e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.395e+01, tolerance: 1.068e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.443e+01, tolerance: 1.058e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.259e+01, tolerance: 1.059e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.776e+01, tolerance: 1.062e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.432e+01, tolerance: 1.146e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.395e+01, tolerance: 1.068e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.443e+01, tolerance: 1.058e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.259e+01, tolerance: 1.059e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.776e+01, tolerance: 1.062e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.143e+02, tolerance: 1.146e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.001e+02, tolerance: 1.068e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.509e+01, tolerance: 1.058e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.029e+01, tolerance: 1.059e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.597e+01, tolerance: 1.062e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.143e+02, tolerance: 1.146e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.001e+02, tolerance: 1.068e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.509e+01, tolerance: 1.058e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.029e+01, tolerance: 1.059e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.597e+01, tolerance: 1.062e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.544e+02, tolerance: 1.146e+02\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.354e+02, tolerance: 1.068e+02\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.074e+02, tolerance: 1.059e+02\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.544e+02, tolerance: 1.146e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.354e+02, tolerance: 1.068e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.018e+02, tolerance: 1.058e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.074e+02, tolerance: 1.059e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+01, tolerance: 1.062e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.544e+02, tolerance: 1.146e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.354e+02, tolerance: 1.068e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.018e+02, tolerance: 1.058e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.074e+02, tolerance: 1.059e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.473e+01, tolerance: 1.062e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.502e+02, tolerance: 1.058e+02\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.561e+02, tolerance: 1.059e+02\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.466e+02, tolerance: 1.062e+02\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.520e+01, tolerance: 1.068e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.502e+02, tolerance: 1.058e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.561e+02, tolerance: 1.059e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.466e+02, tolerance: 1.062e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.346e+00, tolerance: 1.146e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.520e+01, tolerance: 1.068e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.502e+02, tolerance: 1.058e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.561e+02, tolerance: 1.059e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.466e+02, tolerance: 1.062e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.506e+01, tolerance: 1.068e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.786e+01, tolerance: 1.058e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.528e+01, tolerance: 1.059e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.548e+00, tolerance: 1.146e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.506e+01, tolerance: 1.068e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.786e+01, tolerance: 1.058e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.528e+01, tolerance: 1.059e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.805e+00, tolerance: 1.062e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.330e+01, tolerance: 1.058e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.105e+01, tolerance: 1.059e+01\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.410e+00, tolerance: 1.068e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.330e+01, tolerance: 1.058e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.105e+01, tolerance: 1.059e+00\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.218e+03, tolerance: 1.146e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.858e+03, tolerance: 1.068e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.721e+03, tolerance: 1.058e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.730e+03, tolerance: 1.059e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.853e+03, tolerance: 1.062e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.218e+03, tolerance: 1.146e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.858e+03, tolerance: 1.068e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.721e+03, tolerance: 1.058e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.730e+03, tolerance: 1.059e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.853e+03, tolerance: 1.062e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.218e+03, tolerance: 1.146e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.858e+03, tolerance: 1.068e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.721e+03, tolerance: 1.058e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.730e+03, tolerance: 1.059e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.853e+03, tolerance: 1.062e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.218e+03, tolerance: 1.146e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.858e+03, tolerance: 1.068e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.721e+03, tolerance: 1.058e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.730e+03, tolerance: 1.059e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.853e+03, tolerance: 1.062e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.384e+03, tolerance: 1.146e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.021e+03, tolerance: 1.068e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.892e+03, tolerance: 1.058e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+03, tolerance: 1.059e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.019e+03, tolerance: 1.062e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.384e+03, tolerance: 1.146e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.021e+03, tolerance: 1.068e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.892e+03, tolerance: 1.058e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+03, tolerance: 1.059e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.019e+03, tolerance: 1.062e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.384e+03, tolerance: 1.146e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.021e+03, tolerance: 1.068e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.892e+03, tolerance: 1.058e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+03, tolerance: 1.059e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.019e+03, tolerance: 1.062e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.384e+03, tolerance: 1.146e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.021e+03, tolerance: 1.068e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.892e+03, tolerance: 1.058e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.902e+03, tolerance: 1.059e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.019e+03, tolerance: 1.062e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.764e+03, tolerance: 1.146e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.394e+03, tolerance: 1.068e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.283e+03, tolerance: 1.058e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.311e+03, tolerance: 1.059e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.404e+03, tolerance: 1.062e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.764e+03, tolerance: 1.146e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.394e+03, tolerance: 1.068e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.283e+03, tolerance: 1.058e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.311e+03, tolerance: 1.059e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.404e+03, tolerance: 1.062e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.764e+03, tolerance: 1.146e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.394e+03, tolerance: 1.068e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.283e+03, tolerance: 1.058e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.311e+03, tolerance: 1.059e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.404e+03, tolerance: 1.062e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.764e+03, tolerance: 1.146e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.394e+03, tolerance: 1.068e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.283e+03, tolerance: 1.058e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.311e+03, tolerance: 1.059e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.404e+03, tolerance: 1.062e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.519e+03, tolerance: 1.146e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.149e+03, tolerance: 1.068e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.081e+03, tolerance: 1.058e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.093e+03, tolerance: 1.059e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.145e+03, tolerance: 1.062e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.519e+03, tolerance: 1.146e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.149e+03, tolerance: 1.068e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.081e+03, tolerance: 1.058e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.093e+03, tolerance: 1.059e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.145e+03, tolerance: 1.062e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.519e+03, tolerance: 1.146e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.149e+03, tolerance: 1.068e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.081e+03, tolerance: 1.058e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.093e+03, tolerance: 1.059e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.145e+03, tolerance: 1.062e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.519e+03, tolerance: 1.146e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.149e+03, tolerance: 1.068e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.081e+03, tolerance: 1.058e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.093e+03, tolerance: 1.059e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.145e+03, tolerance: 1.062e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.427e+03, tolerance: 1.146e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.048e+03, tolerance: 1.068e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.003e+03, tolerance: 1.058e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.003e+03, tolerance: 1.059e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.019e+03, tolerance: 1.062e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.427e+03, tolerance: 1.146e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.048e+03, tolerance: 1.068e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.003e+03, tolerance: 1.058e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.003e+03, tolerance: 1.059e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.019e+03, tolerance: 1.062e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.427e+03, tolerance: 1.146e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.048e+03, tolerance: 1.068e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.003e+03, tolerance: 1.058e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.003e+03, tolerance: 1.059e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.019e+03, tolerance: 1.062e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.427e+03, tolerance: 1.146e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.048e+03, tolerance: 1.068e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.003e+03, tolerance: 1.058e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.003e+03, tolerance: 1.059e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.019e+03, tolerance: 1.062e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.693e+03, tolerance: 1.146e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.306e+03, tolerance: 1.068e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.258e+03, tolerance: 1.058e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.260e+03, tolerance: 1.059e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.275e+03, tolerance: 1.062e+03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.693e+03, tolerance: 1.146e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.306e+03, tolerance: 1.068e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.258e+03, tolerance: 1.058e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.260e+03, tolerance: 1.059e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.275e+03, tolerance: 1.062e+02 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.693e+03, tolerance: 1.146e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.306e+03, tolerance: 1.068e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.258e+03, tolerance: 1.058e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.260e+03, tolerance: 1.059e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.275e+03, tolerance: 1.062e+01 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.693e+03, tolerance: 1.146e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.306e+03, tolerance: 1.068e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.258e+03, tolerance: 1.058e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.260e+03, tolerance: 1.059e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 5.275e+03, tolerance: 1.062e+00 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=ElasticNet(),\n",
              "             param_grid={'alpha': [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
              "                         'l1_ratio': array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]),\n",
              "                         'tol': [0.1, 0.01, 0.001, 0.0001]},\n",
              "             scoring='neg_mean_squared_error')"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parameters = {'tol' : [0.1,0.01,0.001,0.0001],\n",
        "              \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],  #alpha, coef qui multiplie le terme de pénalité)\n",
        "              \"l1_ratio\": np.arange(0.0, 1.0, 0.1)}#L1 ratio , =1 équivaut à un Lasso, 0 à un Ridge\n",
        "\n",
        "\n",
        "elastic_grid = GridSearchCV(estimator = ElasticNet(), \n",
        "                      param_grid = parameters,\n",
        "                      scoring = 'neg_mean_squared_error',\n",
        "                      cv=5,\n",
        "                      verbose=0\n",
        "                     )\n",
        "\n",
        "elastic_grid.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LMFS4I57w7kA",
        "outputId": "f754d6b3-3627-4b66-c305-016385307734"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'alpha': 0.001, 'l1_ratio': 0.6000000000000001, 'tol': 0.1}"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "elastic_grid.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4pO-jqD0xCYY"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "results = results.append(pd.DataFrame({\n",
        "    'Modèle' : ['Elasticnet Regression'],\n",
        "    'Score_RMSE' : [math.sqrt(mean_squared_error(elastic_grid.predict(X_test), y_test))]}),\n",
        "              ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ftg8kzhzMOK"
      },
      "source": [
        "## **Random Forest Regressor**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4YRJVDHIzUl6"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "parameters = {\n",
        "    'n_estimators' : [10,50,100,300,500], #nombre d'arbres de décision\n",
        "    'min_samples_leaf' : [1,3,5,10], #nombre de feuilles minimales dans un noeud\n",
        "    'max_features': ['auto', 'sqrt'] #nombre de features observées pour chaque arbre\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hduH8CT6zb42",
        "outputId": "19598d59-cbc0-4aa8-bd83-e4f717e3280d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n",
            "[CV] END max_features=auto, min_samples_leaf=1, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_features=auto, min_samples_leaf=1, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_features=auto, min_samples_leaf=1, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_features=auto, min_samples_leaf=1, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_features=auto, min_samples_leaf=1, n_estimators=10; total time=   0.3s\n",
            "[CV] END max_features=auto, min_samples_leaf=1, n_estimators=50; total time=   1.4s\n",
            "[CV] END max_features=auto, min_samples_leaf=1, n_estimators=50; total time=   1.4s\n",
            "[CV] END max_features=auto, min_samples_leaf=1, n_estimators=50; total time=   1.4s\n",
            "[CV] END max_features=auto, min_samples_leaf=1, n_estimators=50; total time=   1.5s\n",
            "[CV] END max_features=auto, min_samples_leaf=1, n_estimators=50; total time=   1.5s\n",
            "[CV] END max_features=auto, min_samples_leaf=1, n_estimators=100; total time=   2.9s\n",
            "[CV] END max_features=auto, min_samples_leaf=1, n_estimators=100; total time=   4.2s\n",
            "[CV] END max_features=auto, min_samples_leaf=1, n_estimators=100; total time=   3.1s\n",
            "[CV] END max_features=auto, min_samples_leaf=1, n_estimators=100; total time=   2.8s\n",
            "[CV] END max_features=auto, min_samples_leaf=1, n_estimators=100; total time=   2.8s\n",
            "[CV] END max_features=auto, min_samples_leaf=1, n_estimators=300; total time=   8.6s\n",
            "[CV] END max_features=auto, min_samples_leaf=1, n_estimators=300; total time=   8.5s\n",
            "[CV] END max_features=auto, min_samples_leaf=1, n_estimators=300; total time=   8.9s\n",
            "[CV] END max_features=auto, min_samples_leaf=1, n_estimators=300; total time=   8.7s\n",
            "[CV] END max_features=auto, min_samples_leaf=1, n_estimators=300; total time=   8.5s\n",
            "[CV] END max_features=auto, min_samples_leaf=1, n_estimators=500; total time=  15.3s\n",
            "[CV] END max_features=auto, min_samples_leaf=1, n_estimators=500; total time=  14.2s\n",
            "[CV] END max_features=auto, min_samples_leaf=1, n_estimators=500; total time=  14.3s\n",
            "[CV] END max_features=auto, min_samples_leaf=1, n_estimators=500; total time=  14.2s\n",
            "[CV] END max_features=auto, min_samples_leaf=1, n_estimators=500; total time=  14.0s\n",
            "[CV] END max_features=auto, min_samples_leaf=3, n_estimators=10; total time=   0.2s\n",
            "[CV] END max_features=auto, min_samples_leaf=3, n_estimators=10; total time=   0.2s\n",
            "[CV] END max_features=auto, min_samples_leaf=3, n_estimators=10; total time=   0.2s\n",
            "[CV] END max_features=auto, min_samples_leaf=3, n_estimators=10; total time=   0.2s\n",
            "[CV] END max_features=auto, min_samples_leaf=3, n_estimators=10; total time=   0.2s\n",
            "[CV] END max_features=auto, min_samples_leaf=3, n_estimators=50; total time=   1.1s\n",
            "[CV] END max_features=auto, min_samples_leaf=3, n_estimators=50; total time=   1.1s\n",
            "[CV] END max_features=auto, min_samples_leaf=3, n_estimators=50; total time=   1.1s\n",
            "[CV] END max_features=auto, min_samples_leaf=3, n_estimators=50; total time=   1.1s\n",
            "[CV] END max_features=auto, min_samples_leaf=3, n_estimators=50; total time=   1.3s\n",
            "[CV] END max_features=auto, min_samples_leaf=3, n_estimators=100; total time=   2.9s\n",
            "[CV] END max_features=auto, min_samples_leaf=3, n_estimators=100; total time=   2.1s\n",
            "[CV] END max_features=auto, min_samples_leaf=3, n_estimators=100; total time=   2.1s\n",
            "[CV] END max_features=auto, min_samples_leaf=3, n_estimators=100; total time=   2.1s\n",
            "[CV] END max_features=auto, min_samples_leaf=3, n_estimators=100; total time=   2.1s\n",
            "[CV] END max_features=auto, min_samples_leaf=3, n_estimators=300; total time=   6.4s\n",
            "[CV] END max_features=auto, min_samples_leaf=3, n_estimators=300; total time=   6.3s\n",
            "[CV] END max_features=auto, min_samples_leaf=3, n_estimators=300; total time=   7.4s\n",
            "[CV] END max_features=auto, min_samples_leaf=3, n_estimators=300; total time=   6.5s\n",
            "[CV] END max_features=auto, min_samples_leaf=3, n_estimators=300; total time=   6.2s\n",
            "[CV] END max_features=auto, min_samples_leaf=3, n_estimators=500; total time=  10.5s\n",
            "[CV] END max_features=auto, min_samples_leaf=3, n_estimators=500; total time=  12.2s\n",
            "[CV] END max_features=auto, min_samples_leaf=3, n_estimators=500; total time=  11.2s\n",
            "[CV] END max_features=auto, min_samples_leaf=3, n_estimators=500; total time=  10.6s\n",
            "[CV] END max_features=auto, min_samples_leaf=3, n_estimators=500; total time=  10.4s\n",
            "[CV] END max_features=auto, min_samples_leaf=5, n_estimators=10; total time=   0.2s\n",
            "[CV] END max_features=auto, min_samples_leaf=5, n_estimators=10; total time=   0.2s\n",
            "[CV] END max_features=auto, min_samples_leaf=5, n_estimators=10; total time=   0.2s\n",
            "[CV] END max_features=auto, min_samples_leaf=5, n_estimators=10; total time=   0.2s\n",
            "[CV] END max_features=auto, min_samples_leaf=5, n_estimators=10; total time=   0.2s\n",
            "[CV] END max_features=auto, min_samples_leaf=5, n_estimators=50; total time=   1.0s\n",
            "[CV] END max_features=auto, min_samples_leaf=5, n_estimators=50; total time=   1.0s\n",
            "[CV] END max_features=auto, min_samples_leaf=5, n_estimators=50; total time=   1.0s\n",
            "[CV] END max_features=auto, min_samples_leaf=5, n_estimators=50; total time=   1.0s\n",
            "[CV] END max_features=auto, min_samples_leaf=5, n_estimators=50; total time=   0.9s\n",
            "[CV] END max_features=auto, min_samples_leaf=5, n_estimators=100; total time=   1.9s\n",
            "[CV] END max_features=auto, min_samples_leaf=5, n_estimators=100; total time=   1.9s\n",
            "[CV] END max_features=auto, min_samples_leaf=5, n_estimators=100; total time=   1.9s\n",
            "[CV] END max_features=auto, min_samples_leaf=5, n_estimators=100; total time=   1.9s\n",
            "[CV] END max_features=auto, min_samples_leaf=5, n_estimators=100; total time=   1.9s\n",
            "[CV] END max_features=auto, min_samples_leaf=5, n_estimators=300; total time=   5.6s\n",
            "[CV] END max_features=auto, min_samples_leaf=5, n_estimators=300; total time=   5.7s\n",
            "[CV] END max_features=auto, min_samples_leaf=5, n_estimators=300; total time=   5.7s\n",
            "[CV] END max_features=auto, min_samples_leaf=5, n_estimators=300; total time=   6.8s\n",
            "[CV] END max_features=auto, min_samples_leaf=5, n_estimators=300; total time=   5.6s\n",
            "[CV] END max_features=auto, min_samples_leaf=5, n_estimators=500; total time=   9.3s\n",
            "[CV] END max_features=auto, min_samples_leaf=5, n_estimators=500; total time=   9.4s\n",
            "[CV] END max_features=auto, min_samples_leaf=5, n_estimators=500; total time=   9.5s\n",
            "[CV] END max_features=auto, min_samples_leaf=5, n_estimators=500; total time=   9.5s\n",
            "[CV] END max_features=auto, min_samples_leaf=5, n_estimators=500; total time=   9.2s\n",
            "[CV] END max_features=auto, min_samples_leaf=10, n_estimators=10; total time=   0.2s\n",
            "[CV] END max_features=auto, min_samples_leaf=10, n_estimators=10; total time=   0.2s\n",
            "[CV] END max_features=auto, min_samples_leaf=10, n_estimators=10; total time=   0.2s\n",
            "[CV] END max_features=auto, min_samples_leaf=10, n_estimators=10; total time=   0.2s\n",
            "[CV] END max_features=auto, min_samples_leaf=10, n_estimators=10; total time=   0.2s\n",
            "[CV] END max_features=auto, min_samples_leaf=10, n_estimators=50; total time=   0.8s\n",
            "[CV] END max_features=auto, min_samples_leaf=10, n_estimators=50; total time=   0.8s\n",
            "[CV] END max_features=auto, min_samples_leaf=10, n_estimators=50; total time=   0.8s\n",
            "[CV] END max_features=auto, min_samples_leaf=10, n_estimators=50; total time=   0.8s\n",
            "[CV] END max_features=auto, min_samples_leaf=10, n_estimators=50; total time=   0.8s\n",
            "[CV] END max_features=auto, min_samples_leaf=10, n_estimators=100; total time=   1.6s\n",
            "[CV] END max_features=auto, min_samples_leaf=10, n_estimators=100; total time=   1.6s\n",
            "[CV] END max_features=auto, min_samples_leaf=10, n_estimators=100; total time=   1.7s\n",
            "[CV] END max_features=auto, min_samples_leaf=10, n_estimators=100; total time=   1.9s\n",
            "[CV] END max_features=auto, min_samples_leaf=10, n_estimators=100; total time=   2.4s\n",
            "[CV] END max_features=auto, min_samples_leaf=10, n_estimators=300; total time=   4.8s\n",
            "[CV] END max_features=auto, min_samples_leaf=10, n_estimators=300; total time=   4.9s\n",
            "[CV] END max_features=auto, min_samples_leaf=10, n_estimators=300; total time=   4.9s\n",
            "[CV] END max_features=auto, min_samples_leaf=10, n_estimators=300; total time=   4.8s\n",
            "[CV] END max_features=auto, min_samples_leaf=10, n_estimators=300; total time=   4.8s\n",
            "[CV] END max_features=auto, min_samples_leaf=10, n_estimators=500; total time=   8.1s\n",
            "[CV] END max_features=auto, min_samples_leaf=10, n_estimators=500; total time=   8.0s\n",
            "[CV] END max_features=auto, min_samples_leaf=10, n_estimators=500; total time=   8.2s\n",
            "[CV] END max_features=auto, min_samples_leaf=10, n_estimators=500; total time=   8.0s\n",
            "[CV] END max_features=auto, min_samples_leaf=10, n_estimators=500; total time=   9.0s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=1, n_estimators=10; total time=   0.1s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=1, n_estimators=10; total time=   0.1s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=1, n_estimators=10; total time=   0.1s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=1, n_estimators=10; total time=   0.1s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=1, n_estimators=10; total time=   0.1s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=1, n_estimators=50; total time=   0.4s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=1, n_estimators=50; total time=   0.4s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=1, n_estimators=50; total time=   0.4s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=1, n_estimators=50; total time=   0.4s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=1, n_estimators=50; total time=   0.4s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=1, n_estimators=100; total time=   0.8s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=1, n_estimators=100; total time=   0.8s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=1, n_estimators=100; total time=   0.8s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=1, n_estimators=100; total time=   0.8s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=1, n_estimators=100; total time=   0.8s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=1, n_estimators=300; total time=   2.2s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=1, n_estimators=300; total time=   2.2s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=1, n_estimators=300; total time=   2.3s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=1, n_estimators=300; total time=   2.3s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=1, n_estimators=300; total time=   2.3s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=1, n_estimators=500; total time=   3.7s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=1, n_estimators=500; total time=   3.7s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=1, n_estimators=500; total time=   3.7s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=1, n_estimators=500; total time=   3.8s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=1, n_estimators=500; total time=   3.8s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=3, n_estimators=10; total time=   0.1s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=3, n_estimators=10; total time=   0.1s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=3, n_estimators=10; total time=   0.0s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=3, n_estimators=10; total time=   0.1s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=3, n_estimators=10; total time=   0.1s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=3, n_estimators=50; total time=   0.2s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=3, n_estimators=50; total time=   0.2s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=3, n_estimators=50; total time=   0.2s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=3, n_estimators=50; total time=   0.2s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=3, n_estimators=50; total time=   0.2s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=3, n_estimators=100; total time=   0.4s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=3, n_estimators=300; total time=   1.3s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=3, n_estimators=300; total time=   1.3s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=3, n_estimators=300; total time=   1.3s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=3, n_estimators=300; total time=   1.4s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=3, n_estimators=300; total time=   2.0s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=3, n_estimators=500; total time=   2.5s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=3, n_estimators=500; total time=   2.2s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=3, n_estimators=500; total time=   2.1s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=3, n_estimators=500; total time=   2.1s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=3, n_estimators=500; total time=   2.1s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=5, n_estimators=10; total time=   0.0s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=5, n_estimators=10; total time=   0.0s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=5, n_estimators=10; total time=   0.0s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=5, n_estimators=10; total time=   0.0s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=5, n_estimators=10; total time=   0.1s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=5, n_estimators=50; total time=   0.2s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=5, n_estimators=50; total time=   0.2s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=5, n_estimators=50; total time=   0.2s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=5, n_estimators=50; total time=   0.2s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=5, n_estimators=50; total time=   0.2s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=5, n_estimators=100; total time=   0.4s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=5, n_estimators=100; total time=   0.4s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=5, n_estimators=100; total time=   0.4s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=5, n_estimators=100; total time=   0.4s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=5, n_estimators=100; total time=   0.4s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=5, n_estimators=300; total time=   1.2s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=5, n_estimators=300; total time=   1.8s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=5, n_estimators=300; total time=   1.6s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=5, n_estimators=300; total time=   1.2s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=5, n_estimators=300; total time=   1.1s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=5, n_estimators=500; total time=   1.9s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=5, n_estimators=500; total time=   1.9s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=5, n_estimators=500; total time=   1.8s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=5, n_estimators=500; total time=   2.0s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=5, n_estimators=500; total time=   2.9s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=10, n_estimators=10; total time=   0.1s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=10, n_estimators=10; total time=   0.0s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=10, n_estimators=10; total time=   0.0s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=10, n_estimators=10; total time=   0.0s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=10, n_estimators=10; total time=   0.0s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=10, n_estimators=50; total time=   0.2s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=10, n_estimators=50; total time=   0.2s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=10, n_estimators=50; total time=   0.2s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=10, n_estimators=50; total time=   0.2s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=10, n_estimators=50; total time=   0.2s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=10, n_estimators=100; total time=   0.3s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=10, n_estimators=100; total time=   0.4s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=10, n_estimators=100; total time=   0.3s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=10, n_estimators=100; total time=   0.3s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=10, n_estimators=100; total time=   0.4s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=10, n_estimators=300; total time=   1.0s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=10, n_estimators=300; total time=   1.0s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=10, n_estimators=300; total time=   1.0s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=10, n_estimators=300; total time=   1.0s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=10, n_estimators=300; total time=   1.0s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=10, n_estimators=500; total time=   1.7s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=10, n_estimators=500; total time=   1.7s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=10, n_estimators=500; total time=   1.7s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=10, n_estimators=500; total time=   1.6s\n",
            "[CV] END max_features=sqrt, min_samples_leaf=10, n_estimators=500; total time=   1.7s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=RandomForestRegressor(),\n",
              "             param_grid={'max_features': ['auto', 'sqrt'],\n",
              "                         'min_samples_leaf': [1, 3, 5, 10],\n",
              "                         'n_estimators': [10, 50, 100, 300, 500]},\n",
              "             verbose=2)"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rfr_search = GridSearchCV(RandomForestRegressor(),\n",
        "                               param_grid = parameters,\n",
        "                               #scoring='mean_squared_error',\n",
        "                              verbose=2,\n",
        "                               cv=5)\n",
        "\n",
        "rfr_search.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DifjovfuzlHC",
        "outputId": "9b647400-2494-4157-83be-27a4bc2bc4c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'max_features': 'sqrt', 'min_samples_leaf': 1, 'n_estimators': 500}"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rfr_search.best_params_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GRdq_tupzqjV"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "results = results.append(pd.DataFrame({\n",
        "    'Modèle' : ['Random Forest Regressor'],\n",
        "    'Score_RMSE' : [math.sqrt(mean_squared_error(rfr_search.predict(X_test), y_test))]}),\n",
        "              ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r0qwmRTWaPTE"
      },
      "outputs": [],
      "source": [
        "coefficients = abs(rfr_search.best_estimator_.feature_importances_)\n",
        "liste_coefs_rer = pd.concat((pd.DataFrame(X.columns, columns = ['Variable']), \n",
        "                      pd.DataFrame(coefficients, columns = ['Coefficient'])), axis = 1).sort_values(by='Coefficient', ascending = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "A2MA1VJqaRxz",
        "outputId": "5faf6562-a2d8-4780-fcb0-f1449f2330f6"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.title('RandomForestRegressor - Importance des 20 premières Features')\n",
        "sns.barplot(y = liste_coefs_rer['Variable'].head(20),\n",
        "            x = liste_coefs_rer['Coefficient'].head(20))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXjQbN5_aX6N"
      },
      "source": [
        "# **Support vectore regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nFSyB0bcab2i",
        "outputId": "9e700808-1430-4a7b-89c6-d270ac09e841"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 160 candidates, totalling 800 fits\n",
            "[CV] END ................C=0.001, epsilon=0.001, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.001, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.001, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.001, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.001, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.001, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.001, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.001, gamma=1e-07; total time=   1.9s\n",
            "[CV] END ................C=0.001, epsilon=0.001, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.001, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.001, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.001, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.001, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.001, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.001, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.001, gamma=1e-05; total time=   2.9s\n",
            "[CV] END ................C=0.001, epsilon=0.001, gamma=1e-05; total time=   2.3s\n",
            "[CV] END ................C=0.001, epsilon=0.001, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.001, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.001, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ...............C=0.001, epsilon=0.001, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ...............C=0.001, epsilon=0.001, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ...............C=0.001, epsilon=0.001, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ...............C=0.001, epsilon=0.001, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ...............C=0.001, epsilon=0.001, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.001, gamma=0.001; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.001, gamma=0.001; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.001, gamma=0.001; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.001, gamma=0.001; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.001, gamma=0.001; total time=   2.0s\n",
            "[CV] END .................C=0.001, epsilon=0.001, gamma=0.01; total time=   2.0s\n",
            "[CV] END .................C=0.001, epsilon=0.001, gamma=0.01; total time=   2.0s\n",
            "[CV] END .................C=0.001, epsilon=0.001, gamma=0.01; total time=   2.0s\n",
            "[CV] END .................C=0.001, epsilon=0.001, gamma=0.01; total time=   2.0s\n",
            "[CV] END .................C=0.001, epsilon=0.001, gamma=0.01; total time=   2.0s\n",
            "[CV] END ..................C=0.001, epsilon=0.001, gamma=0.1; total time=   2.0s\n",
            "[CV] END ..................C=0.001, epsilon=0.001, gamma=0.1; total time=   2.0s\n",
            "[CV] END ..................C=0.001, epsilon=0.001, gamma=0.1; total time=   2.0s\n",
            "[CV] END ..................C=0.001, epsilon=0.001, gamma=0.1; total time=   2.0s\n",
            "[CV] END ..................C=0.001, epsilon=0.001, gamma=0.1; total time=   2.0s\n",
            "[CV] END .................C=0.001, epsilon=0.01, gamma=1e-08; total time=   1.9s\n",
            "[CV] END .................C=0.001, epsilon=0.01, gamma=1e-08; total time=   2.0s\n",
            "[CV] END .................C=0.001, epsilon=0.01, gamma=1e-08; total time=   2.0s\n",
            "[CV] END .................C=0.001, epsilon=0.01, gamma=1e-08; total time=   2.0s\n",
            "[CV] END .................C=0.001, epsilon=0.01, gamma=1e-08; total time=   2.0s\n",
            "[CV] END .................C=0.001, epsilon=0.01, gamma=1e-07; total time=   2.0s\n",
            "[CV] END .................C=0.001, epsilon=0.01, gamma=1e-07; total time=   2.0s\n",
            "[CV] END .................C=0.001, epsilon=0.01, gamma=1e-07; total time=   2.4s\n",
            "[CV] END .................C=0.001, epsilon=0.01, gamma=1e-07; total time=   2.7s\n",
            "[CV] END .................C=0.001, epsilon=0.01, gamma=1e-07; total time=   1.9s\n",
            "[CV] END .................C=0.001, epsilon=0.01, gamma=1e-06; total time=   2.0s\n",
            "[CV] END .................C=0.001, epsilon=0.01, gamma=1e-06; total time=   2.0s\n",
            "[CV] END .................C=0.001, epsilon=0.01, gamma=1e-06; total time=   1.9s\n",
            "[CV] END .................C=0.001, epsilon=0.01, gamma=1e-06; total time=   2.0s\n",
            "[CV] END .................C=0.001, epsilon=0.01, gamma=1e-06; total time=   2.0s\n",
            "[CV] END .................C=0.001, epsilon=0.01, gamma=1e-05; total time=   2.0s\n",
            "[CV] END .................C=0.001, epsilon=0.01, gamma=1e-05; total time=   1.9s\n",
            "[CV] END .................C=0.001, epsilon=0.01, gamma=1e-05; total time=   2.0s\n",
            "[CV] END .................C=0.001, epsilon=0.01, gamma=1e-05; total time=   2.0s\n",
            "[CV] END .................C=0.001, epsilon=0.01, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.01, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.01, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.01, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.01, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ................C=0.001, epsilon=0.01, gamma=0.0001; total time=   2.0s\n",
            "[CV] END .................C=0.001, epsilon=0.01, gamma=0.001; total time=   2.0s\n",
            "[CV] END .................C=0.001, epsilon=0.01, gamma=0.001; total time=   2.0s\n",
            "[CV] END .................C=0.001, epsilon=0.01, gamma=0.001; total time=   1.9s\n",
            "[CV] END .................C=0.001, epsilon=0.01, gamma=0.001; total time=   2.0s\n",
            "[CV] END .................C=0.001, epsilon=0.01, gamma=0.001; total time=   2.0s\n",
            "[CV] END ..................C=0.001, epsilon=0.01, gamma=0.01; total time=   2.0s\n",
            "[CV] END ..................C=0.001, epsilon=0.01, gamma=0.01; total time=   2.0s\n",
            "[CV] END ..................C=0.001, epsilon=0.01, gamma=0.01; total time=   2.0s\n",
            "[CV] END ..................C=0.001, epsilon=0.01, gamma=0.01; total time=   2.0s\n",
            "[CV] END ..................C=0.001, epsilon=0.01, gamma=0.01; total time=   2.0s\n",
            "[CV] END ...................C=0.001, epsilon=0.01, gamma=0.1; total time=   2.0s\n",
            "[CV] END ...................C=0.001, epsilon=0.01, gamma=0.1; total time=   2.0s\n",
            "[CV] END ...................C=0.001, epsilon=0.01, gamma=0.1; total time=   2.0s\n",
            "[CV] END ...................C=0.001, epsilon=0.01, gamma=0.1; total time=   2.0s\n",
            "[CV] END ...................C=0.001, epsilon=0.01, gamma=0.1; total time=   2.0s\n",
            "[CV] END ..................C=0.001, epsilon=0.1, gamma=1e-08; total time=   2.3s\n",
            "[CV] END ..................C=0.001, epsilon=0.1, gamma=1e-08; total time=   2.7s\n",
            "[CV] END ..................C=0.001, epsilon=0.1, gamma=1e-08; total time=   1.9s\n",
            "[CV] END ..................C=0.001, epsilon=0.1, gamma=1e-08; total time=   1.9s\n",
            "[CV] END ..................C=0.001, epsilon=0.1, gamma=1e-08; total time=   1.9s\n",
            "[CV] END ..................C=0.001, epsilon=0.1, gamma=1e-07; total time=   1.9s\n",
            "[CV] END ..................C=0.001, epsilon=0.1, gamma=1e-07; total time=   1.9s\n",
            "[CV] END ..................C=0.001, epsilon=0.1, gamma=1e-07; total time=   1.9s\n",
            "[CV] END ..................C=0.001, epsilon=0.1, gamma=1e-07; total time=   1.9s\n",
            "[CV] END ..................C=0.001, epsilon=0.1, gamma=1e-07; total time=   1.9s\n",
            "[CV] END ..................C=0.001, epsilon=0.1, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ..................C=0.001, epsilon=0.1, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ..................C=0.001, epsilon=0.1, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ..................C=0.001, epsilon=0.1, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ..................C=0.001, epsilon=0.1, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ..................C=0.001, epsilon=0.1, gamma=1e-05; total time=   1.9s\n",
            "[CV] END ..................C=0.001, epsilon=0.1, gamma=1e-05; total time=   1.9s\n",
            "[CV] END ..................C=0.001, epsilon=0.1, gamma=1e-05; total time=   1.9s\n",
            "[CV] END ..................C=0.001, epsilon=0.1, gamma=1e-05; total time=   1.9s\n",
            "[CV] END ..................C=0.001, epsilon=0.1, gamma=1e-05; total time=   1.9s\n",
            "[CV] END .................C=0.001, epsilon=0.1, gamma=0.0001; total time=   1.9s\n",
            "[CV] END .................C=0.001, epsilon=0.1, gamma=0.0001; total time=   1.9s\n",
            "[CV] END .................C=0.001, epsilon=0.1, gamma=0.0001; total time=   1.9s\n",
            "[CV] END .................C=0.001, epsilon=0.1, gamma=0.0001; total time=   1.9s\n",
            "[CV] END .................C=0.001, epsilon=0.1, gamma=0.0001; total time=   1.9s\n",
            "[CV] END ..................C=0.001, epsilon=0.1, gamma=0.001; total time=   1.9s\n",
            "[CV] END ..................C=0.001, epsilon=0.1, gamma=0.001; total time=   1.9s\n",
            "[CV] END ..................C=0.001, epsilon=0.1, gamma=0.001; total time=   1.9s\n",
            "[CV] END ..................C=0.001, epsilon=0.1, gamma=0.001; total time=   1.9s\n",
            "[CV] END ..................C=0.001, epsilon=0.1, gamma=0.001; total time=   1.9s\n",
            "[CV] END ...................C=0.001, epsilon=0.1, gamma=0.01; total time=   1.9s\n",
            "[CV] END ...................C=0.001, epsilon=0.1, gamma=0.01; total time=   1.9s\n",
            "[CV] END ...................C=0.001, epsilon=0.1, gamma=0.01; total time=   1.9s\n",
            "[CV] END ...................C=0.001, epsilon=0.1, gamma=0.01; total time=   1.9s\n",
            "[CV] END ...................C=0.001, epsilon=0.1, gamma=0.01; total time=   2.3s\n",
            "[CV] END ....................C=0.001, epsilon=0.1, gamma=0.1; total time=   2.9s\n",
            "[CV] END ....................C=0.001, epsilon=0.1, gamma=0.1; total time=   2.8s\n",
            "[CV] END ....................C=0.001, epsilon=0.1, gamma=0.1; total time=   2.1s\n",
            "[CV] END ....................C=0.001, epsilon=0.1, gamma=0.1; total time=   1.9s\n",
            "[CV] END ....................C=0.001, epsilon=0.1, gamma=0.1; total time=   1.9s\n",
            "[CV] END ....................C=0.001, epsilon=1, gamma=1e-08; total time=   1.0s\n",
            "[CV] END ....................C=0.001, epsilon=1, gamma=1e-08; total time=   1.0s\n",
            "[CV] END ....................C=0.001, epsilon=1, gamma=1e-08; total time=   1.0s\n",
            "[CV] END ....................C=0.001, epsilon=1, gamma=1e-08; total time=   1.0s\n",
            "[CV] END ....................C=0.001, epsilon=1, gamma=1e-08; total time=   1.0s\n",
            "[CV] END ....................C=0.001, epsilon=1, gamma=1e-07; total time=   1.0s\n",
            "[CV] END ....................C=0.001, epsilon=1, gamma=1e-07; total time=   1.0s\n",
            "[CV] END ....................C=0.001, epsilon=1, gamma=1e-07; total time=   1.0s\n",
            "[CV] END ....................C=0.001, epsilon=1, gamma=1e-07; total time=   1.0s\n",
            "[CV] END ....................C=0.001, epsilon=1, gamma=1e-07; total time=   1.0s\n",
            "[CV] END ....................C=0.001, epsilon=1, gamma=1e-06; total time=   1.0s\n",
            "[CV] END ....................C=0.001, epsilon=1, gamma=1e-06; total time=   1.0s\n",
            "[CV] END ....................C=0.001, epsilon=1, gamma=1e-06; total time=   1.0s\n",
            "[CV] END ....................C=0.001, epsilon=1, gamma=1e-06; total time=   1.0s\n",
            "[CV] END ....................C=0.001, epsilon=1, gamma=1e-06; total time=   1.0s\n",
            "[CV] END ....................C=0.001, epsilon=1, gamma=1e-05; total time=   1.0s\n",
            "[CV] END ....................C=0.001, epsilon=1, gamma=1e-05; total time=   1.0s\n",
            "[CV] END ....................C=0.001, epsilon=1, gamma=1e-05; total time=   1.0s\n",
            "[CV] END ....................C=0.001, epsilon=1, gamma=1e-05; total time=   1.0s\n",
            "[CV] END ....................C=0.001, epsilon=1, gamma=1e-05; total time=   1.0s\n",
            "[CV] END ...................C=0.001, epsilon=1, gamma=0.0001; total time=   1.0s\n",
            "[CV] END ...................C=0.001, epsilon=1, gamma=0.0001; total time=   1.0s\n",
            "[CV] END ...................C=0.001, epsilon=1, gamma=0.0001; total time=   1.0s\n",
            "[CV] END ...................C=0.001, epsilon=1, gamma=0.0001; total time=   1.0s\n",
            "[CV] END ...................C=0.001, epsilon=1, gamma=0.0001; total time=   1.0s\n",
            "[CV] END ....................C=0.001, epsilon=1, gamma=0.001; total time=   1.0s\n",
            "[CV] END ....................C=0.001, epsilon=1, gamma=0.001; total time=   1.0s\n",
            "[CV] END ....................C=0.001, epsilon=1, gamma=0.001; total time=   1.4s\n",
            "[CV] END ....................C=0.001, epsilon=1, gamma=0.001; total time=   1.4s\n",
            "[CV] END ....................C=0.001, epsilon=1, gamma=0.001; total time=   1.2s\n",
            "[CV] END .....................C=0.001, epsilon=1, gamma=0.01; total time=   1.0s\n",
            "[CV] END .....................C=0.001, epsilon=1, gamma=0.01; total time=   1.0s\n",
            "[CV] END .....................C=0.001, epsilon=1, gamma=0.01; total time=   1.0s\n",
            "[CV] END .....................C=0.001, epsilon=1, gamma=0.01; total time=   1.0s\n",
            "[CV] END .....................C=0.001, epsilon=1, gamma=0.01; total time=   1.0s\n",
            "[CV] END ......................C=0.001, epsilon=1, gamma=0.1; total time=   0.9s\n",
            "[CV] END ......................C=0.001, epsilon=1, gamma=0.1; total time=   0.9s\n",
            "[CV] END ......................C=0.001, epsilon=1, gamma=0.1; total time=   0.9s\n",
            "[CV] END ......................C=0.001, epsilon=1, gamma=0.1; total time=   0.9s\n",
            "[CV] END ......................C=0.001, epsilon=1, gamma=0.1; total time=   0.9s\n",
            "[CV] END .................C=0.01, epsilon=0.001, gamma=1e-08; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.001, gamma=1e-08; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.001, gamma=1e-08; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.001, gamma=1e-08; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.001, gamma=1e-08; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.001, gamma=1e-07; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.001, gamma=1e-07; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.001, gamma=1e-07; total time=   2.8s\n",
            "[CV] END .................C=0.01, epsilon=0.001, gamma=1e-07; total time=   2.2s\n",
            "[CV] END .................C=0.01, epsilon=0.001, gamma=1e-07; total time=   1.9s\n",
            "[CV] END .................C=0.01, epsilon=0.001, gamma=1e-06; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.001, gamma=1e-06; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.001, gamma=1e-06; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.001, gamma=1e-06; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.001, gamma=1e-06; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.001, gamma=1e-05; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.001, gamma=1e-05; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.001, gamma=1e-05; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.001, gamma=1e-05; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.001, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ................C=0.01, epsilon=0.001, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ................C=0.01, epsilon=0.001, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ................C=0.01, epsilon=0.001, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ................C=0.01, epsilon=0.001, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ................C=0.01, epsilon=0.001, gamma=0.0001; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.001, gamma=0.001; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.001, gamma=0.001; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.001, gamma=0.001; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.001, gamma=0.001; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.001, gamma=0.001; total time=   2.0s\n",
            "[CV] END ..................C=0.01, epsilon=0.001, gamma=0.01; total time=   2.0s\n",
            "[CV] END ..................C=0.01, epsilon=0.001, gamma=0.01; total time=   2.0s\n",
            "[CV] END ..................C=0.01, epsilon=0.001, gamma=0.01; total time=   2.0s\n",
            "[CV] END ..................C=0.01, epsilon=0.001, gamma=0.01; total time=   2.0s\n",
            "[CV] END ..................C=0.01, epsilon=0.001, gamma=0.01; total time=   2.0s\n",
            "[CV] END ...................C=0.01, epsilon=0.001, gamma=0.1; total time=   2.0s\n",
            "[CV] END ...................C=0.01, epsilon=0.001, gamma=0.1; total time=   2.0s\n",
            "[CV] END ...................C=0.01, epsilon=0.001, gamma=0.1; total time=   2.0s\n",
            "[CV] END ...................C=0.01, epsilon=0.001, gamma=0.1; total time=   2.0s\n",
            "[CV] END ...................C=0.01, epsilon=0.001, gamma=0.1; total time=   2.3s\n",
            "[CV] END ..................C=0.01, epsilon=0.01, gamma=1e-08; total time=   3.0s\n",
            "[CV] END ..................C=0.01, epsilon=0.01, gamma=1e-08; total time=   2.1s\n",
            "[CV] END ..................C=0.01, epsilon=0.01, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ..................C=0.01, epsilon=0.01, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ..................C=0.01, epsilon=0.01, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ..................C=0.01, epsilon=0.01, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ..................C=0.01, epsilon=0.01, gamma=1e-07; total time=   1.9s\n",
            "[CV] END ..................C=0.01, epsilon=0.01, gamma=1e-07; total time=   1.9s\n",
            "[CV] END ..................C=0.01, epsilon=0.01, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ..................C=0.01, epsilon=0.01, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ..................C=0.01, epsilon=0.01, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ..................C=0.01, epsilon=0.01, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ..................C=0.01, epsilon=0.01, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ..................C=0.01, epsilon=0.01, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ..................C=0.01, epsilon=0.01, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ..................C=0.01, epsilon=0.01, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ..................C=0.01, epsilon=0.01, gamma=1e-05; total time=   1.9s\n",
            "[CV] END ..................C=0.01, epsilon=0.01, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ..................C=0.01, epsilon=0.01, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ..................C=0.01, epsilon=0.01, gamma=1e-05; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.01, gamma=0.0001; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.01, gamma=0.0001; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.01, gamma=0.0001; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.01, gamma=0.0001; total time=   2.0s\n",
            "[CV] END .................C=0.01, epsilon=0.01, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ..................C=0.01, epsilon=0.01, gamma=0.001; total time=   2.0s\n",
            "[CV] END ..................C=0.01, epsilon=0.01, gamma=0.001; total time=   2.0s\n",
            "[CV] END ..................C=0.01, epsilon=0.01, gamma=0.001; total time=   2.0s\n",
            "[CV] END ..................C=0.01, epsilon=0.01, gamma=0.001; total time=   2.0s\n",
            "[CV] END ..................C=0.01, epsilon=0.01, gamma=0.001; total time=   2.0s\n",
            "[CV] END ...................C=0.01, epsilon=0.01, gamma=0.01; total time=   2.0s\n",
            "[CV] END ...................C=0.01, epsilon=0.01, gamma=0.01; total time=   2.1s\n",
            "[CV] END ...................C=0.01, epsilon=0.01, gamma=0.01; total time=   2.9s\n",
            "[CV] END ...................C=0.01, epsilon=0.01, gamma=0.01; total time=   2.2s\n",
            "[CV] END ...................C=0.01, epsilon=0.01, gamma=0.01; total time=   2.0s\n",
            "[CV] END ....................C=0.01, epsilon=0.01, gamma=0.1; total time=   2.0s\n",
            "[CV] END ....................C=0.01, epsilon=0.01, gamma=0.1; total time=   2.0s\n",
            "[CV] END ....................C=0.01, epsilon=0.01, gamma=0.1; total time=   2.0s\n",
            "[CV] END ....................C=0.01, epsilon=0.01, gamma=0.1; total time=   2.0s\n",
            "[CV] END ....................C=0.01, epsilon=0.01, gamma=0.1; total time=   2.0s\n",
            "[CV] END ...................C=0.01, epsilon=0.1, gamma=1e-08; total time=   1.9s\n",
            "[CV] END ...................C=0.01, epsilon=0.1, gamma=1e-08; total time=   1.9s\n",
            "[CV] END ...................C=0.01, epsilon=0.1, gamma=1e-08; total time=   1.9s\n",
            "[CV] END ...................C=0.01, epsilon=0.1, gamma=1e-08; total time=   1.9s\n",
            "[CV] END ...................C=0.01, epsilon=0.1, gamma=1e-08; total time=   1.9s\n",
            "[CV] END ...................C=0.01, epsilon=0.1, gamma=1e-07; total time=   1.9s\n",
            "[CV] END ...................C=0.01, epsilon=0.1, gamma=1e-07; total time=   1.9s\n",
            "[CV] END ...................C=0.01, epsilon=0.1, gamma=1e-07; total time=   1.9s\n",
            "[CV] END ...................C=0.01, epsilon=0.1, gamma=1e-07; total time=   1.9s\n",
            "[CV] END ...................C=0.01, epsilon=0.1, gamma=1e-07; total time=   1.9s\n",
            "[CV] END ...................C=0.01, epsilon=0.1, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ...................C=0.01, epsilon=0.1, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ...................C=0.01, epsilon=0.1, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ...................C=0.01, epsilon=0.1, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ...................C=0.01, epsilon=0.1, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ...................C=0.01, epsilon=0.1, gamma=1e-05; total time=   1.9s\n",
            "[CV] END ...................C=0.01, epsilon=0.1, gamma=1e-05; total time=   1.9s\n",
            "[CV] END ...................C=0.01, epsilon=0.1, gamma=1e-05; total time=   1.9s\n",
            "[CV] END ...................C=0.01, epsilon=0.1, gamma=1e-05; total time=   1.9s\n",
            "[CV] END ...................C=0.01, epsilon=0.1, gamma=1e-05; total time=   1.9s\n",
            "[CV] END ..................C=0.01, epsilon=0.1, gamma=0.0001; total time=   1.9s\n",
            "[CV] END ..................C=0.01, epsilon=0.1, gamma=0.0001; total time=   1.9s\n",
            "[CV] END ..................C=0.01, epsilon=0.1, gamma=0.0001; total time=   1.9s\n",
            "[CV] END ..................C=0.01, epsilon=0.1, gamma=0.0001; total time=   1.9s\n",
            "[CV] END ..................C=0.01, epsilon=0.1, gamma=0.0001; total time=   2.3s\n",
            "[CV] END ...................C=0.01, epsilon=0.1, gamma=0.001; total time=   2.7s\n",
            "[CV] END ...................C=0.01, epsilon=0.1, gamma=0.001; total time=   1.9s\n",
            "[CV] END ...................C=0.01, epsilon=0.1, gamma=0.001; total time=   1.9s\n",
            "[CV] END ...................C=0.01, epsilon=0.1, gamma=0.001; total time=   1.9s\n",
            "[CV] END ...................C=0.01, epsilon=0.1, gamma=0.001; total time=   1.9s\n",
            "[CV] END ....................C=0.01, epsilon=0.1, gamma=0.01; total time=   1.9s\n",
            "[CV] END ....................C=0.01, epsilon=0.1, gamma=0.01; total time=   1.9s\n",
            "[CV] END ....................C=0.01, epsilon=0.1, gamma=0.01; total time=   1.9s\n",
            "[CV] END ....................C=0.01, epsilon=0.1, gamma=0.01; total time=   1.9s\n",
            "[CV] END ....................C=0.01, epsilon=0.1, gamma=0.01; total time=   1.9s\n",
            "[CV] END .....................C=0.01, epsilon=0.1, gamma=0.1; total time=   1.8s\n",
            "[CV] END .....................C=0.01, epsilon=0.1, gamma=0.1; total time=   2.2s\n",
            "[CV] END .....................C=0.01, epsilon=0.1, gamma=0.1; total time=   2.7s\n",
            "[CV] END .....................C=0.01, epsilon=0.1, gamma=0.1; total time=   1.9s\n",
            "[CV] END .....................C=0.01, epsilon=0.1, gamma=0.1; total time=   1.8s\n",
            "[CV] END .....................C=0.01, epsilon=1, gamma=1e-08; total time=   1.0s\n",
            "[CV] END .....................C=0.01, epsilon=1, gamma=1e-08; total time=   1.0s\n",
            "[CV] END .....................C=0.01, epsilon=1, gamma=1e-08; total time=   1.0s\n",
            "[CV] END .....................C=0.01, epsilon=1, gamma=1e-08; total time=   1.0s\n",
            "[CV] END .....................C=0.01, epsilon=1, gamma=1e-08; total time=   1.0s\n",
            "[CV] END .....................C=0.01, epsilon=1, gamma=1e-07; total time=   1.0s\n",
            "[CV] END .....................C=0.01, epsilon=1, gamma=1e-07; total time=   1.0s\n",
            "[CV] END .....................C=0.01, epsilon=1, gamma=1e-07; total time=   1.0s\n",
            "[CV] END .....................C=0.01, epsilon=1, gamma=1e-07; total time=   1.0s\n",
            "[CV] END .....................C=0.01, epsilon=1, gamma=1e-07; total time=   1.0s\n",
            "[CV] END .....................C=0.01, epsilon=1, gamma=1e-06; total time=   1.0s\n",
            "[CV] END .....................C=0.01, epsilon=1, gamma=1e-06; total time=   1.0s\n",
            "[CV] END .....................C=0.01, epsilon=1, gamma=1e-06; total time=   1.0s\n",
            "[CV] END .....................C=0.01, epsilon=1, gamma=1e-06; total time=   1.0s\n",
            "[CV] END .....................C=0.01, epsilon=1, gamma=1e-06; total time=   1.0s\n",
            "[CV] END .....................C=0.01, epsilon=1, gamma=1e-05; total time=   1.0s\n",
            "[CV] END .....................C=0.01, epsilon=1, gamma=1e-05; total time=   1.0s\n",
            "[CV] END .....................C=0.01, epsilon=1, gamma=1e-05; total time=   1.0s\n",
            "[CV] END .....................C=0.01, epsilon=1, gamma=1e-05; total time=   1.0s\n",
            "[CV] END .....................C=0.01, epsilon=1, gamma=1e-05; total time=   1.0s\n",
            "[CV] END ....................C=0.01, epsilon=1, gamma=0.0001; total time=   1.0s\n",
            "[CV] END ....................C=0.01, epsilon=1, gamma=0.0001; total time=   1.0s\n",
            "[CV] END ....................C=0.01, epsilon=1, gamma=0.0001; total time=   1.0s\n",
            "[CV] END ....................C=0.01, epsilon=1, gamma=0.0001; total time=   1.0s\n",
            "[CV] END ....................C=0.01, epsilon=1, gamma=0.0001; total time=   1.0s\n",
            "[CV] END .....................C=0.01, epsilon=1, gamma=0.001; total time=   1.2s\n",
            "[CV] END .....................C=0.01, epsilon=1, gamma=0.001; total time=   1.4s\n",
            "[CV] END .....................C=0.01, epsilon=1, gamma=0.001; total time=   1.4s\n",
            "[CV] END .....................C=0.01, epsilon=1, gamma=0.001; total time=   1.1s\n",
            "[CV] END .....................C=0.01, epsilon=1, gamma=0.001; total time=   1.0s\n",
            "[CV] END ......................C=0.01, epsilon=1, gamma=0.01; total time=   0.9s\n",
            "[CV] END ......................C=0.01, epsilon=1, gamma=0.01; total time=   0.9s\n",
            "[CV] END ......................C=0.01, epsilon=1, gamma=0.01; total time=   0.9s\n",
            "[CV] END ......................C=0.01, epsilon=1, gamma=0.01; total time=   1.0s\n",
            "[CV] END ......................C=0.01, epsilon=1, gamma=0.01; total time=   1.3s\n",
            "[CV] END .......................C=0.01, epsilon=1, gamma=0.1; total time=   1.1s\n",
            "[CV] END .......................C=0.01, epsilon=1, gamma=0.1; total time=   1.1s\n",
            "[CV] END .......................C=0.01, epsilon=1, gamma=0.1; total time=   0.8s\n",
            "[CV] END .......................C=0.01, epsilon=1, gamma=0.1; total time=   0.8s\n",
            "[CV] END .......................C=0.01, epsilon=1, gamma=0.1; total time=   0.8s\n",
            "[CV] END ..................C=0.1, epsilon=0.001, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.001, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.001, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.001, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.001, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.001, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.001, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.001, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.001, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.001, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.001, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.001, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.001, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.001, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.001, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.001, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.001, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.001, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.001, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.001, gamma=1e-05; total time=   2.0s\n",
            "[CV] END .................C=0.1, epsilon=0.001, gamma=0.0001; total time=   2.0s\n",
            "[CV] END .................C=0.1, epsilon=0.001, gamma=0.0001; total time=   2.0s\n",
            "[CV] END .................C=0.1, epsilon=0.001, gamma=0.0001; total time=   2.0s\n",
            "[CV] END .................C=0.1, epsilon=0.001, gamma=0.0001; total time=   2.0s\n",
            "[CV] END .................C=0.1, epsilon=0.001, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.001, gamma=0.001; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.001, gamma=0.001; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.001, gamma=0.001; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.001, gamma=0.001; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.001, gamma=0.001; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.001, gamma=0.01; total time=   2.4s\n",
            "[CV] END ...................C=0.1, epsilon=0.001, gamma=0.01; total time=   2.8s\n",
            "[CV] END ...................C=0.1, epsilon=0.001, gamma=0.01; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.001, gamma=0.01; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.001, gamma=0.01; total time=   2.0s\n",
            "[CV] END ....................C=0.1, epsilon=0.001, gamma=0.1; total time=   2.0s\n",
            "[CV] END ....................C=0.1, epsilon=0.001, gamma=0.1; total time=   2.0s\n",
            "[CV] END ....................C=0.1, epsilon=0.001, gamma=0.1; total time=   2.0s\n",
            "[CV] END ....................C=0.1, epsilon=0.001, gamma=0.1; total time=   2.0s\n",
            "[CV] END ....................C=0.1, epsilon=0.001, gamma=0.1; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.01, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.01, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.01, gamma=1e-08; total time=   1.9s\n",
            "[CV] END ...................C=0.1, epsilon=0.01, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.01, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.01, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.01, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.01, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.01, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.01, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.01, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.01, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.01, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.01, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.01, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.01, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.01, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.01, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.01, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.01, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.01, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.01, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.01, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ..................C=0.1, epsilon=0.01, gamma=0.0001; total time=   2.7s\n",
            "[CV] END ..................C=0.1, epsilon=0.01, gamma=0.0001; total time=   2.5s\n",
            "[CV] END ...................C=0.1, epsilon=0.01, gamma=0.001; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.01, gamma=0.001; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.01, gamma=0.001; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.01, gamma=0.001; total time=   2.0s\n",
            "[CV] END ...................C=0.1, epsilon=0.01, gamma=0.001; total time=   2.0s\n",
            "[CV] END ....................C=0.1, epsilon=0.01, gamma=0.01; total time=   2.0s\n",
            "[CV] END ....................C=0.1, epsilon=0.01, gamma=0.01; total time=   2.0s\n",
            "[CV] END ....................C=0.1, epsilon=0.01, gamma=0.01; total time=   2.0s\n",
            "[CV] END ....................C=0.1, epsilon=0.01, gamma=0.01; total time=   2.0s\n",
            "[CV] END ....................C=0.1, epsilon=0.01, gamma=0.01; total time=   2.0s\n",
            "[CV] END .....................C=0.1, epsilon=0.01, gamma=0.1; total time=   2.0s\n",
            "[CV] END .....................C=0.1, epsilon=0.01, gamma=0.1; total time=   2.0s\n",
            "[CV] END .....................C=0.1, epsilon=0.01, gamma=0.1; total time=   2.0s\n",
            "[CV] END .....................C=0.1, epsilon=0.01, gamma=0.1; total time=   2.0s\n",
            "[CV] END .....................C=0.1, epsilon=0.01, gamma=0.1; total time=   2.0s\n",
            "[CV] END ....................C=0.1, epsilon=0.1, gamma=1e-08; total time=   1.9s\n",
            "[CV] END ....................C=0.1, epsilon=0.1, gamma=1e-08; total time=   1.9s\n",
            "[CV] END ....................C=0.1, epsilon=0.1, gamma=1e-08; total time=   1.9s\n",
            "[CV] END ....................C=0.1, epsilon=0.1, gamma=1e-08; total time=   1.9s\n",
            "[CV] END ....................C=0.1, epsilon=0.1, gamma=1e-08; total time=   1.9s\n",
            "[CV] END ....................C=0.1, epsilon=0.1, gamma=1e-07; total time=   1.9s\n",
            "[CV] END ....................C=0.1, epsilon=0.1, gamma=1e-07; total time=   1.9s\n",
            "[CV] END ....................C=0.1, epsilon=0.1, gamma=1e-07; total time=   1.9s\n",
            "[CV] END ....................C=0.1, epsilon=0.1, gamma=1e-07; total time=   1.9s\n",
            "[CV] END ....................C=0.1, epsilon=0.1, gamma=1e-07; total time=   1.9s\n",
            "[CV] END ....................C=0.1, epsilon=0.1, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ....................C=0.1, epsilon=0.1, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ....................C=0.1, epsilon=0.1, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ....................C=0.1, epsilon=0.1, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ....................C=0.1, epsilon=0.1, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ....................C=0.1, epsilon=0.1, gamma=1e-05; total time=   2.2s\n",
            "[CV] END ....................C=0.1, epsilon=0.1, gamma=1e-05; total time=   2.7s\n",
            "[CV] END ....................C=0.1, epsilon=0.1, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ....................C=0.1, epsilon=0.1, gamma=1e-05; total time=   1.9s\n",
            "[CV] END ....................C=0.1, epsilon=0.1, gamma=1e-05; total time=   1.9s\n",
            "[CV] END ...................C=0.1, epsilon=0.1, gamma=0.0001; total time=   1.9s\n",
            "[CV] END ...................C=0.1, epsilon=0.1, gamma=0.0001; total time=   1.9s\n",
            "[CV] END ...................C=0.1, epsilon=0.1, gamma=0.0001; total time=   1.9s\n",
            "[CV] END ...................C=0.1, epsilon=0.1, gamma=0.0001; total time=   1.9s\n",
            "[CV] END ...................C=0.1, epsilon=0.1, gamma=0.0001; total time=   1.9s\n",
            "[CV] END ....................C=0.1, epsilon=0.1, gamma=0.001; total time=   1.9s\n",
            "[CV] END ....................C=0.1, epsilon=0.1, gamma=0.001; total time=   1.9s\n",
            "[CV] END ....................C=0.1, epsilon=0.1, gamma=0.001; total time=   1.9s\n",
            "[CV] END ....................C=0.1, epsilon=0.1, gamma=0.001; total time=   1.9s\n",
            "[CV] END ....................C=0.1, epsilon=0.1, gamma=0.001; total time=   1.9s\n",
            "[CV] END .....................C=0.1, epsilon=0.1, gamma=0.01; total time=   1.8s\n",
            "[CV] END .....................C=0.1, epsilon=0.1, gamma=0.01; total time=   1.9s\n",
            "[CV] END .....................C=0.1, epsilon=0.1, gamma=0.01; total time=   1.8s\n",
            "[CV] END .....................C=0.1, epsilon=0.1, gamma=0.01; total time=   1.9s\n",
            "[CV] END .....................C=0.1, epsilon=0.1, gamma=0.01; total time=   1.9s\n",
            "[CV] END ......................C=0.1, epsilon=0.1, gamma=0.1; total time=   2.3s\n",
            "[CV] END ......................C=0.1, epsilon=0.1, gamma=0.1; total time=   2.4s\n",
            "[CV] END ......................C=0.1, epsilon=0.1, gamma=0.1; total time=   1.8s\n",
            "[CV] END ......................C=0.1, epsilon=0.1, gamma=0.1; total time=   1.8s\n",
            "[CV] END ......................C=0.1, epsilon=0.1, gamma=0.1; total time=   1.8s\n",
            "[CV] END ......................C=0.1, epsilon=1, gamma=1e-08; total time=   1.0s\n",
            "[CV] END ......................C=0.1, epsilon=1, gamma=1e-08; total time=   1.0s\n",
            "[CV] END ......................C=0.1, epsilon=1, gamma=1e-08; total time=   1.0s\n",
            "[CV] END ......................C=0.1, epsilon=1, gamma=1e-08; total time=   1.0s\n",
            "[CV] END ......................C=0.1, epsilon=1, gamma=1e-08; total time=   1.0s\n",
            "[CV] END ......................C=0.1, epsilon=1, gamma=1e-07; total time=   1.0s\n",
            "[CV] END ......................C=0.1, epsilon=1, gamma=1e-07; total time=   1.0s\n",
            "[CV] END ......................C=0.1, epsilon=1, gamma=1e-07; total time=   1.0s\n",
            "[CV] END ......................C=0.1, epsilon=1, gamma=1e-07; total time=   1.0s\n",
            "[CV] END ......................C=0.1, epsilon=1, gamma=1e-07; total time=   1.0s\n",
            "[CV] END ......................C=0.1, epsilon=1, gamma=1e-06; total time=   1.0s\n",
            "[CV] END ......................C=0.1, epsilon=1, gamma=1e-06; total time=   1.0s\n",
            "[CV] END ......................C=0.1, epsilon=1, gamma=1e-06; total time=   1.0s\n",
            "[CV] END ......................C=0.1, epsilon=1, gamma=1e-06; total time=   1.0s\n",
            "[CV] END ......................C=0.1, epsilon=1, gamma=1e-06; total time=   1.0s\n",
            "[CV] END ......................C=0.1, epsilon=1, gamma=1e-05; total time=   1.0s\n",
            "[CV] END ......................C=0.1, epsilon=1, gamma=1e-05; total time=   1.4s\n",
            "[CV] END ......................C=0.1, epsilon=1, gamma=1e-05; total time=   1.4s\n",
            "[CV] END ......................C=0.1, epsilon=1, gamma=1e-05; total time=   1.2s\n",
            "[CV] END ......................C=0.1, epsilon=1, gamma=1e-05; total time=   1.0s\n",
            "[CV] END .....................C=0.1, epsilon=1, gamma=0.0001; total time=   1.1s\n",
            "[CV] END .....................C=0.1, epsilon=1, gamma=0.0001; total time=   1.5s\n",
            "[CV] END .....................C=0.1, epsilon=1, gamma=0.0001; total time=   1.4s\n",
            "[CV] END .....................C=0.1, epsilon=1, gamma=0.0001; total time=   1.2s\n",
            "[CV] END .....................C=0.1, epsilon=1, gamma=0.0001; total time=   1.0s\n",
            "[CV] END ......................C=0.1, epsilon=1, gamma=0.001; total time=   0.8s\n",
            "[CV] END ......................C=0.1, epsilon=1, gamma=0.001; total time=   0.8s\n",
            "[CV] END ......................C=0.1, epsilon=1, gamma=0.001; total time=   0.8s\n",
            "[CV] END ......................C=0.1, epsilon=1, gamma=0.001; total time=   0.8s\n",
            "[CV] END ......................C=0.1, epsilon=1, gamma=0.001; total time=   0.8s\n",
            "[CV] END .......................C=0.1, epsilon=1, gamma=0.01; total time=   0.5s\n",
            "[CV] END .......................C=0.1, epsilon=1, gamma=0.01; total time=   0.6s\n",
            "[CV] END .......................C=0.1, epsilon=1, gamma=0.01; total time=   0.6s\n",
            "[CV] END .......................C=0.1, epsilon=1, gamma=0.01; total time=   0.6s\n",
            "[CV] END .......................C=0.1, epsilon=1, gamma=0.01; total time=   0.6s\n",
            "[CV] END ........................C=0.1, epsilon=1, gamma=0.1; total time=   0.5s\n",
            "[CV] END ........................C=0.1, epsilon=1, gamma=0.1; total time=   0.5s\n",
            "[CV] END ........................C=0.1, epsilon=1, gamma=0.1; total time=   0.5s\n",
            "[CV] END ........................C=0.1, epsilon=1, gamma=0.1; total time=   0.5s\n",
            "[CV] END ........................C=0.1, epsilon=1, gamma=0.1; total time=   0.5s\n",
            "[CV] END ....................C=1, epsilon=0.001, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.001, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.001, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.001, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.001, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.001, gamma=1e-07; total time=   1.9s\n",
            "[CV] END ....................C=1, epsilon=0.001, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.001, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.001, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.001, gamma=1e-07; total time=   1.9s\n",
            "[CV] END ....................C=1, epsilon=0.001, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.001, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.001, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.001, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.001, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.001, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.001, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.001, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.001, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.001, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ...................C=1, epsilon=0.001, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ...................C=1, epsilon=0.001, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ...................C=1, epsilon=0.001, gamma=0.0001; total time=   2.8s\n",
            "[CV] END ...................C=1, epsilon=0.001, gamma=0.0001; total time=   2.4s\n",
            "[CV] END ...................C=1, epsilon=0.001, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.001, gamma=0.001; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.001, gamma=0.001; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.001, gamma=0.001; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.001, gamma=0.001; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.001, gamma=0.001; total time=   2.0s\n",
            "[CV] END .....................C=1, epsilon=0.001, gamma=0.01; total time=   2.1s\n",
            "[CV] END .....................C=1, epsilon=0.001, gamma=0.01; total time=   2.1s\n",
            "[CV] END .....................C=1, epsilon=0.001, gamma=0.01; total time=   2.0s\n",
            "[CV] END .....................C=1, epsilon=0.001, gamma=0.01; total time=   2.1s\n",
            "[CV] END .....................C=1, epsilon=0.001, gamma=0.01; total time=   2.1s\n",
            "[CV] END ......................C=1, epsilon=0.001, gamma=0.1; total time=   2.1s\n",
            "[CV] END ......................C=1, epsilon=0.001, gamma=0.1; total time=   2.1s\n",
            "[CV] END ......................C=1, epsilon=0.001, gamma=0.1; total time=   2.2s\n",
            "[CV] END ......................C=1, epsilon=0.001, gamma=0.1; total time=   2.2s\n",
            "[CV] END ......................C=1, epsilon=0.001, gamma=0.1; total time=   2.1s\n",
            "[CV] END .....................C=1, epsilon=0.01, gamma=1e-08; total time=   1.9s\n",
            "[CV] END .....................C=1, epsilon=0.01, gamma=1e-08; total time=   1.9s\n",
            "[CV] END .....................C=1, epsilon=0.01, gamma=1e-08; total time=   1.9s\n",
            "[CV] END .....................C=1, epsilon=0.01, gamma=1e-08; total time=   1.9s\n",
            "[CV] END .....................C=1, epsilon=0.01, gamma=1e-08; total time=   1.9s\n",
            "[CV] END .....................C=1, epsilon=0.01, gamma=1e-07; total time=   1.9s\n",
            "[CV] END .....................C=1, epsilon=0.01, gamma=1e-07; total time=   2.0s\n",
            "[CV] END .....................C=1, epsilon=0.01, gamma=1e-07; total time=   2.0s\n",
            "[CV] END .....................C=1, epsilon=0.01, gamma=1e-07; total time=   1.9s\n",
            "[CV] END .....................C=1, epsilon=0.01, gamma=1e-07; total time=   1.9s\n",
            "[CV] END .....................C=1, epsilon=0.01, gamma=1e-06; total time=   1.9s\n",
            "[CV] END .....................C=1, epsilon=0.01, gamma=1e-06; total time=   1.9s\n",
            "[CV] END .....................C=1, epsilon=0.01, gamma=1e-06; total time=   1.9s\n",
            "[CV] END .....................C=1, epsilon=0.01, gamma=1e-06; total time=   2.0s\n",
            "[CV] END .....................C=1, epsilon=0.01, gamma=1e-06; total time=   2.6s\n",
            "[CV] END .....................C=1, epsilon=0.01, gamma=1e-05; total time=   2.5s\n",
            "[CV] END .....................C=1, epsilon=0.01, gamma=1e-05; total time=   2.0s\n",
            "[CV] END .....................C=1, epsilon=0.01, gamma=1e-05; total time=   2.0s\n",
            "[CV] END .....................C=1, epsilon=0.01, gamma=1e-05; total time=   2.0s\n",
            "[CV] END .....................C=1, epsilon=0.01, gamma=1e-05; total time=   1.9s\n",
            "[CV] END ....................C=1, epsilon=0.01, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.01, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.01, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.01, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ....................C=1, epsilon=0.01, gamma=0.0001; total time=   2.0s\n",
            "[CV] END .....................C=1, epsilon=0.01, gamma=0.001; total time=   2.0s\n",
            "[CV] END .....................C=1, epsilon=0.01, gamma=0.001; total time=   2.0s\n",
            "[CV] END .....................C=1, epsilon=0.01, gamma=0.001; total time=   2.0s\n",
            "[CV] END .....................C=1, epsilon=0.01, gamma=0.001; total time=   2.0s\n",
            "[CV] END .....................C=1, epsilon=0.01, gamma=0.001; total time=   1.9s\n",
            "[CV] END ......................C=1, epsilon=0.01, gamma=0.01; total time=   2.0s\n",
            "[CV] END ......................C=1, epsilon=0.01, gamma=0.01; total time=   2.0s\n",
            "[CV] END ......................C=1, epsilon=0.01, gamma=0.01; total time=   2.0s\n",
            "[CV] END ......................C=1, epsilon=0.01, gamma=0.01; total time=   2.1s\n",
            "[CV] END ......................C=1, epsilon=0.01, gamma=0.01; total time=   2.0s\n",
            "[CV] END .......................C=1, epsilon=0.01, gamma=0.1; total time=   2.1s\n",
            "[CV] END .......................C=1, epsilon=0.01, gamma=0.1; total time=   2.1s\n",
            "[CV] END .......................C=1, epsilon=0.01, gamma=0.1; total time=   2.1s\n",
            "[CV] END .......................C=1, epsilon=0.01, gamma=0.1; total time=   2.1s\n",
            "[CV] END .......................C=1, epsilon=0.01, gamma=0.1; total time=   2.1s\n",
            "[CV] END ......................C=1, epsilon=0.1, gamma=1e-08; total time=   1.9s\n",
            "[CV] END ......................C=1, epsilon=0.1, gamma=1e-08; total time=   1.9s\n",
            "[CV] END ......................C=1, epsilon=0.1, gamma=1e-08; total time=   1.9s\n",
            "[CV] END ......................C=1, epsilon=0.1, gamma=1e-08; total time=   1.9s\n",
            "[CV] END ......................C=1, epsilon=0.1, gamma=1e-08; total time=   1.9s\n",
            "[CV] END ......................C=1, epsilon=0.1, gamma=1e-07; total time=   1.9s\n",
            "[CV] END ......................C=1, epsilon=0.1, gamma=1e-07; total time=   2.7s\n",
            "[CV] END ......................C=1, epsilon=0.1, gamma=1e-07; total time=   2.2s\n",
            "[CV] END ......................C=1, epsilon=0.1, gamma=1e-07; total time=   1.9s\n",
            "[CV] END ......................C=1, epsilon=0.1, gamma=1e-07; total time=   1.9s\n",
            "[CV] END ......................C=1, epsilon=0.1, gamma=1e-06; total time=   1.8s\n",
            "[CV] END ......................C=1, epsilon=0.1, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ......................C=1, epsilon=0.1, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ......................C=1, epsilon=0.1, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ......................C=1, epsilon=0.1, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ......................C=1, epsilon=0.1, gamma=1e-05; total time=   1.8s\n",
            "[CV] END ......................C=1, epsilon=0.1, gamma=1e-05; total time=   1.9s\n",
            "[CV] END ......................C=1, epsilon=0.1, gamma=1e-05; total time=   1.9s\n",
            "[CV] END ......................C=1, epsilon=0.1, gamma=1e-05; total time=   1.9s\n",
            "[CV] END ......................C=1, epsilon=0.1, gamma=1e-05; total time=   1.9s\n",
            "[CV] END .....................C=1, epsilon=0.1, gamma=0.0001; total time=   1.8s\n",
            "[CV] END .....................C=1, epsilon=0.1, gamma=0.0001; total time=   1.9s\n",
            "[CV] END .....................C=1, epsilon=0.1, gamma=0.0001; total time=   1.9s\n",
            "[CV] END .....................C=1, epsilon=0.1, gamma=0.0001; total time=   1.8s\n",
            "[CV] END .....................C=1, epsilon=0.1, gamma=0.0001; total time=   1.9s\n",
            "[CV] END ......................C=1, epsilon=0.1, gamma=0.001; total time=   1.8s\n",
            "[CV] END ......................C=1, epsilon=0.1, gamma=0.001; total time=   1.8s\n",
            "[CV] END ......................C=1, epsilon=0.1, gamma=0.001; total time=   1.8s\n",
            "[CV] END ......................C=1, epsilon=0.1, gamma=0.001; total time=   1.8s\n",
            "[CV] END ......................C=1, epsilon=0.1, gamma=0.001; total time=   2.4s\n",
            "[CV] END .......................C=1, epsilon=0.1, gamma=0.01; total time=   2.3s\n",
            "[CV] END .......................C=1, epsilon=0.1, gamma=0.01; total time=   1.8s\n",
            "[CV] END .......................C=1, epsilon=0.1, gamma=0.01; total time=   1.8s\n",
            "[CV] END .......................C=1, epsilon=0.1, gamma=0.01; total time=   1.8s\n",
            "[CV] END .......................C=1, epsilon=0.1, gamma=0.01; total time=   1.8s\n",
            "[CV] END ........................C=1, epsilon=0.1, gamma=0.1; total time=   1.8s\n",
            "[CV] END ........................C=1, epsilon=0.1, gamma=0.1; total time=   1.8s\n",
            "[CV] END ........................C=1, epsilon=0.1, gamma=0.1; total time=   1.8s\n",
            "[CV] END ........................C=1, epsilon=0.1, gamma=0.1; total time=   1.8s\n",
            "[CV] END ........................C=1, epsilon=0.1, gamma=0.1; total time=   2.4s\n",
            "[CV] END ........................C=1, epsilon=1, gamma=1e-08; total time=   1.5s\n",
            "[CV] END ........................C=1, epsilon=1, gamma=1e-08; total time=   1.1s\n",
            "[CV] END ........................C=1, epsilon=1, gamma=1e-08; total time=   1.0s\n",
            "[CV] END ........................C=1, epsilon=1, gamma=1e-08; total time=   1.0s\n",
            "[CV] END ........................C=1, epsilon=1, gamma=1e-08; total time=   1.0s\n",
            "[CV] END ........................C=1, epsilon=1, gamma=1e-07; total time=   1.0s\n",
            "[CV] END ........................C=1, epsilon=1, gamma=1e-07; total time=   1.0s\n",
            "[CV] END ........................C=1, epsilon=1, gamma=1e-07; total time=   1.0s\n",
            "[CV] END ........................C=1, epsilon=1, gamma=1e-07; total time=   1.0s\n",
            "[CV] END ........................C=1, epsilon=1, gamma=1e-07; total time=   1.0s\n",
            "[CV] END ........................C=1, epsilon=1, gamma=1e-06; total time=   1.0s\n",
            "[CV] END ........................C=1, epsilon=1, gamma=1e-06; total time=   1.0s\n",
            "[CV] END ........................C=1, epsilon=1, gamma=1e-06; total time=   1.0s\n",
            "[CV] END ........................C=1, epsilon=1, gamma=1e-06; total time=   1.0s\n",
            "[CV] END ........................C=1, epsilon=1, gamma=1e-06; total time=   1.0s\n",
            "[CV] END ........................C=1, epsilon=1, gamma=1e-05; total time=   1.0s\n",
            "[CV] END ........................C=1, epsilon=1, gamma=1e-05; total time=   1.0s\n",
            "[CV] END ........................C=1, epsilon=1, gamma=1e-05; total time=   1.0s\n",
            "[CV] END ........................C=1, epsilon=1, gamma=1e-05; total time=   1.0s\n",
            "[CV] END ........................C=1, epsilon=1, gamma=1e-05; total time=   1.1s\n",
            "[CV] END .......................C=1, epsilon=1, gamma=0.0001; total time=   1.2s\n",
            "[CV] END .......................C=1, epsilon=1, gamma=0.0001; total time=   1.2s\n",
            "[CV] END .......................C=1, epsilon=1, gamma=0.0001; total time=   1.1s\n",
            "[CV] END .......................C=1, epsilon=1, gamma=0.0001; total time=   0.8s\n",
            "[CV] END .......................C=1, epsilon=1, gamma=0.0001; total time=   0.8s\n",
            "[CV] END ........................C=1, epsilon=1, gamma=0.001; total time=   0.6s\n",
            "[CV] END ........................C=1, epsilon=1, gamma=0.001; total time=   0.6s\n",
            "[CV] END ........................C=1, epsilon=1, gamma=0.001; total time=   0.6s\n",
            "[CV] END ........................C=1, epsilon=1, gamma=0.001; total time=   0.6s\n",
            "[CV] END ........................C=1, epsilon=1, gamma=0.001; total time=   0.6s\n",
            "[CV] END .........................C=1, epsilon=1, gamma=0.01; total time=   0.4s\n",
            "[CV] END .........................C=1, epsilon=1, gamma=0.01; total time=   0.5s\n",
            "[CV] END .........................C=1, epsilon=1, gamma=0.01; total time=   0.4s\n",
            "[CV] END .........................C=1, epsilon=1, gamma=0.01; total time=   0.4s\n",
            "[CV] END .........................C=1, epsilon=1, gamma=0.01; total time=   0.5s\n",
            "[CV] END ..........................C=1, epsilon=1, gamma=0.1; total time=   0.4s\n",
            "[CV] END ..........................C=1, epsilon=1, gamma=0.1; total time=   0.4s\n",
            "[CV] END ..........................C=1, epsilon=1, gamma=0.1; total time=   0.4s\n",
            "[CV] END ..........................C=1, epsilon=1, gamma=0.1; total time=   0.4s\n",
            "[CV] END ..........................C=1, epsilon=1, gamma=0.1; total time=   0.4s\n",
            "[CV] END ...................C=10, epsilon=0.001, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ...................C=10, epsilon=0.001, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ...................C=10, epsilon=0.001, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ...................C=10, epsilon=0.001, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ...................C=10, epsilon=0.001, gamma=1e-08; total time=   1.9s\n",
            "[CV] END ...................C=10, epsilon=0.001, gamma=1e-07; total time=   1.9s\n",
            "[CV] END ...................C=10, epsilon=0.001, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ...................C=10, epsilon=0.001, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ...................C=10, epsilon=0.001, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ...................C=10, epsilon=0.001, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ...................C=10, epsilon=0.001, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ...................C=10, epsilon=0.001, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ...................C=10, epsilon=0.001, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ...................C=10, epsilon=0.001, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ...................C=10, epsilon=0.001, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ...................C=10, epsilon=0.001, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ...................C=10, epsilon=0.001, gamma=1e-05; total time=   2.1s\n",
            "[CV] END ...................C=10, epsilon=0.001, gamma=1e-05; total time=   2.9s\n",
            "[CV] END ...................C=10, epsilon=0.001, gamma=1e-05; total time=   2.2s\n",
            "[CV] END ...................C=10, epsilon=0.001, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ..................C=10, epsilon=0.001, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ..................C=10, epsilon=0.001, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ..................C=10, epsilon=0.001, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ..................C=10, epsilon=0.001, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ..................C=10, epsilon=0.001, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ...................C=10, epsilon=0.001, gamma=0.001; total time=   2.1s\n",
            "[CV] END ...................C=10, epsilon=0.001, gamma=0.001; total time=   2.1s\n",
            "[CV] END ...................C=10, epsilon=0.001, gamma=0.001; total time=   2.1s\n",
            "[CV] END ...................C=10, epsilon=0.001, gamma=0.001; total time=   2.1s\n",
            "[CV] END ...................C=10, epsilon=0.001, gamma=0.001; total time=   2.0s\n",
            "[CV] END ....................C=10, epsilon=0.001, gamma=0.01; total time=   2.2s\n",
            "[CV] END ....................C=10, epsilon=0.001, gamma=0.01; total time=   2.2s\n",
            "[CV] END ....................C=10, epsilon=0.001, gamma=0.01; total time=   2.3s\n",
            "[CV] END ....................C=10, epsilon=0.001, gamma=0.01; total time=   2.3s\n",
            "[CV] END ....................C=10, epsilon=0.001, gamma=0.01; total time=   2.2s\n",
            "[CV] END .....................C=10, epsilon=0.001, gamma=0.1; total time=   2.9s\n",
            "[CV] END .....................C=10, epsilon=0.001, gamma=0.1; total time=   3.0s\n",
            "[CV] END .....................C=10, epsilon=0.001, gamma=0.1; total time=   2.9s\n",
            "[CV] END .....................C=10, epsilon=0.001, gamma=0.1; total time=   3.0s\n",
            "[CV] END .....................C=10, epsilon=0.001, gamma=0.1; total time=   3.0s\n",
            "[CV] END ....................C=10, epsilon=0.01, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ....................C=10, epsilon=0.01, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ....................C=10, epsilon=0.01, gamma=1e-08; total time=   1.9s\n",
            "[CV] END ....................C=10, epsilon=0.01, gamma=1e-08; total time=   2.0s\n",
            "[CV] END ....................C=10, epsilon=0.01, gamma=1e-08; total time=   1.9s\n",
            "[CV] END ....................C=10, epsilon=0.01, gamma=1e-07; total time=   1.9s\n",
            "[CV] END ....................C=10, epsilon=0.01, gamma=1e-07; total time=   2.5s\n",
            "[CV] END ....................C=10, epsilon=0.01, gamma=1e-07; total time=   2.6s\n",
            "[CV] END ....................C=10, epsilon=0.01, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ....................C=10, epsilon=0.01, gamma=1e-07; total time=   2.0s\n",
            "[CV] END ....................C=10, epsilon=0.01, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ....................C=10, epsilon=0.01, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ....................C=10, epsilon=0.01, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ....................C=10, epsilon=0.01, gamma=1e-06; total time=   1.9s\n",
            "[CV] END ....................C=10, epsilon=0.01, gamma=1e-06; total time=   2.0s\n",
            "[CV] END ....................C=10, epsilon=0.01, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ....................C=10, epsilon=0.01, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ....................C=10, epsilon=0.01, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ....................C=10, epsilon=0.01, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ....................C=10, epsilon=0.01, gamma=1e-05; total time=   2.0s\n",
            "[CV] END ...................C=10, epsilon=0.01, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ...................C=10, epsilon=0.01, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ...................C=10, epsilon=0.01, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ...................C=10, epsilon=0.01, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ...................C=10, epsilon=0.01, gamma=0.0001; total time=   2.0s\n",
            "[CV] END ....................C=10, epsilon=0.01, gamma=0.001; total time=   2.1s\n",
            "[CV] END ....................C=10, epsilon=0.01, gamma=0.001; total time=   2.1s\n",
            "[CV] END ....................C=10, epsilon=0.01, gamma=0.001; total time=   2.1s\n",
            "[CV] END ....................C=10, epsilon=0.01, gamma=0.001; total time=   2.1s\n",
            "[CV] END ....................C=10, epsilon=0.01, gamma=0.001; total time=   2.0s\n",
            "[CV] END .....................C=10, epsilon=0.01, gamma=0.01; total time=   2.2s\n",
            "[CV] END .....................C=10, epsilon=0.01, gamma=0.01; total time=   2.2s\n",
            "[CV] END .....................C=10, epsilon=0.01, gamma=0.01; total time=   2.2s\n",
            "[CV] END .....................C=10, epsilon=0.01, gamma=0.01; total time=   2.2s\n",
            "[CV] END .....................C=10, epsilon=0.01, gamma=0.01; total time=   2.2s\n",
            "[CV] END ......................C=10, epsilon=0.01, gamma=0.1; total time=   2.8s\n",
            "[CV] END ......................C=10, epsilon=0.01, gamma=0.1; total time=   2.8s\n",
            "[CV] END ......................C=10, epsilon=0.01, gamma=0.1; total time=   3.7s\n",
            "[CV] END ......................C=10, epsilon=0.01, gamma=0.1; total time=   2.9s\n",
            "[CV] END ......................C=10, epsilon=0.01, gamma=0.1; total time=   2.8s\n",
            "[CV] END .....................C=10, epsilon=0.1, gamma=1e-08; total time=   1.9s\n",
            "[CV] END .....................C=10, epsilon=0.1, gamma=1e-08; total time=   1.9s\n",
            "[CV] END .....................C=10, epsilon=0.1, gamma=1e-08; total time=   1.9s\n",
            "[CV] END .....................C=10, epsilon=0.1, gamma=1e-08; total time=   1.9s\n",
            "[CV] END .....................C=10, epsilon=0.1, gamma=1e-08; total time=   1.9s\n",
            "[CV] END .....................C=10, epsilon=0.1, gamma=1e-07; total time=   1.9s\n",
            "[CV] END .....................C=10, epsilon=0.1, gamma=1e-07; total time=   1.9s\n",
            "[CV] END .....................C=10, epsilon=0.1, gamma=1e-07; total time=   1.9s\n",
            "[CV] END .....................C=10, epsilon=0.1, gamma=1e-07; total time=   1.9s\n",
            "[CV] END .....................C=10, epsilon=0.1, gamma=1e-07; total time=   1.9s\n",
            "[CV] END .....................C=10, epsilon=0.1, gamma=1e-06; total time=   1.9s\n",
            "[CV] END .....................C=10, epsilon=0.1, gamma=1e-06; total time=   1.9s\n",
            "[CV] END .....................C=10, epsilon=0.1, gamma=1e-06; total time=   1.9s\n",
            "[CV] END .....................C=10, epsilon=0.1, gamma=1e-06; total time=   1.9s\n",
            "[CV] END .....................C=10, epsilon=0.1, gamma=1e-06; total time=   1.8s\n",
            "[CV] END .....................C=10, epsilon=0.1, gamma=1e-05; total time=   1.8s\n",
            "[CV] END .....................C=10, epsilon=0.1, gamma=1e-05; total time=   1.8s\n",
            "[CV] END .....................C=10, epsilon=0.1, gamma=1e-05; total time=   1.9s\n",
            "[CV] END .....................C=10, epsilon=0.1, gamma=1e-05; total time=   1.9s\n",
            "[CV] END .....................C=10, epsilon=0.1, gamma=1e-05; total time=   1.9s\n",
            "[CV] END ....................C=10, epsilon=0.1, gamma=0.0001; total time=   1.8s\n",
            "[CV] END ....................C=10, epsilon=0.1, gamma=0.0001; total time=   1.8s\n",
            "[CV] END ....................C=10, epsilon=0.1, gamma=0.0001; total time=   1.8s\n",
            "[CV] END ....................C=10, epsilon=0.1, gamma=0.0001; total time=   1.8s\n",
            "[CV] END ....................C=10, epsilon=0.1, gamma=0.0001; total time=   1.8s\n",
            "[CV] END .....................C=10, epsilon=0.1, gamma=0.001; total time=   1.8s\n",
            "[CV] END .....................C=10, epsilon=0.1, gamma=0.001; total time=   2.6s\n",
            "[CV] END .....................C=10, epsilon=0.1, gamma=0.001; total time=   2.1s\n",
            "[CV] END .....................C=10, epsilon=0.1, gamma=0.001; total time=   1.9s\n",
            "[CV] END .....................C=10, epsilon=0.1, gamma=0.001; total time=   2.7s\n",
            "[CV] END ......................C=10, epsilon=0.1, gamma=0.01; total time=   2.2s\n",
            "[CV] END ......................C=10, epsilon=0.1, gamma=0.01; total time=   1.9s\n",
            "[CV] END ......................C=10, epsilon=0.1, gamma=0.01; total time=   2.0s\n",
            "[CV] END ......................C=10, epsilon=0.1, gamma=0.01; total time=   1.9s\n",
            "[CV] END ......................C=10, epsilon=0.1, gamma=0.01; total time=   1.9s\n",
            "[CV] END .......................C=10, epsilon=0.1, gamma=0.1; total time=   2.2s\n",
            "[CV] END .......................C=10, epsilon=0.1, gamma=0.1; total time=   2.3s\n",
            "[CV] END .......................C=10, epsilon=0.1, gamma=0.1; total time=   2.2s\n",
            "[CV] END .......................C=10, epsilon=0.1, gamma=0.1; total time=   2.3s\n",
            "[CV] END .......................C=10, epsilon=0.1, gamma=0.1; total time=   2.3s\n",
            "[CV] END .......................C=10, epsilon=1, gamma=1e-08; total time=   1.0s\n",
            "[CV] END .......................C=10, epsilon=1, gamma=1e-08; total time=   1.0s\n",
            "[CV] END .......................C=10, epsilon=1, gamma=1e-08; total time=   1.0s\n",
            "[CV] END .......................C=10, epsilon=1, gamma=1e-08; total time=   1.0s\n",
            "[CV] END .......................C=10, epsilon=1, gamma=1e-08; total time=   1.0s\n",
            "[CV] END .......................C=10, epsilon=1, gamma=1e-07; total time=   1.0s\n",
            "[CV] END .......................C=10, epsilon=1, gamma=1e-07; total time=   1.0s\n",
            "[CV] END .......................C=10, epsilon=1, gamma=1e-07; total time=   1.0s\n",
            "[CV] END .......................C=10, epsilon=1, gamma=1e-07; total time=   1.0s\n",
            "[CV] END .......................C=10, epsilon=1, gamma=1e-07; total time=   1.0s\n",
            "[CV] END .......................C=10, epsilon=1, gamma=1e-06; total time=   0.9s\n",
            "[CV] END .......................C=10, epsilon=1, gamma=1e-06; total time=   1.0s\n",
            "[CV] END .......................C=10, epsilon=1, gamma=1e-06; total time=   1.0s\n",
            "[CV] END .......................C=10, epsilon=1, gamma=1e-06; total time=   0.9s\n",
            "[CV] END .......................C=10, epsilon=1, gamma=1e-06; total time=   0.9s\n",
            "[CV] END .......................C=10, epsilon=1, gamma=1e-05; total time=   0.8s\n",
            "[CV] END .......................C=10, epsilon=1, gamma=1e-05; total time=   0.8s\n",
            "[CV] END .......................C=10, epsilon=1, gamma=1e-05; total time=   0.8s\n",
            "[CV] END .......................C=10, epsilon=1, gamma=1e-05; total time=   0.8s\n",
            "[CV] END .......................C=10, epsilon=1, gamma=1e-05; total time=   0.9s\n",
            "[CV] END ......................C=10, epsilon=1, gamma=0.0001; total time=   0.9s\n",
            "[CV] END ......................C=10, epsilon=1, gamma=0.0001; total time=   0.9s\n",
            "[CV] END ......................C=10, epsilon=1, gamma=0.0001; total time=   0.9s\n",
            "[CV] END ......................C=10, epsilon=1, gamma=0.0001; total time=   0.9s\n",
            "[CV] END ......................C=10, epsilon=1, gamma=0.0001; total time=   0.6s\n",
            "[CV] END .......................C=10, epsilon=1, gamma=0.001; total time=   0.5s\n",
            "[CV] END .......................C=10, epsilon=1, gamma=0.001; total time=   0.5s\n",
            "[CV] END .......................C=10, epsilon=1, gamma=0.001; total time=   0.5s\n",
            "[CV] END .......................C=10, epsilon=1, gamma=0.001; total time=   0.5s\n",
            "[CV] END .......................C=10, epsilon=1, gamma=0.001; total time=   0.5s\n",
            "[CV] END ........................C=10, epsilon=1, gamma=0.01; total time=   0.4s\n",
            "[CV] END ........................C=10, epsilon=1, gamma=0.01; total time=   0.4s\n",
            "[CV] END ........................C=10, epsilon=1, gamma=0.01; total time=   0.4s\n",
            "[CV] END ........................C=10, epsilon=1, gamma=0.01; total time=   0.4s\n",
            "[CV] END ........................C=10, epsilon=1, gamma=0.01; total time=   0.4s\n",
            "[CV] END .........................C=10, epsilon=1, gamma=0.1; total time=   0.4s\n",
            "[CV] END .........................C=10, epsilon=1, gamma=0.1; total time=   0.4s\n",
            "[CV] END .........................C=10, epsilon=1, gamma=0.1; total time=   0.4s\n",
            "[CV] END .........................C=10, epsilon=1, gamma=0.1; total time=   0.4s\n",
            "[CV] END .........................C=10, epsilon=1, gamma=0.1; total time=   0.4s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=SVR(),\n",
              "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10],\n",
              "                         'epsilon': [0.001, 0.01, 0.1, 1],\n",
              "                         'gamma': [1e-08, 1e-07, 1e-06, 1e-05, 0.0001, 0.001,\n",
              "                                   0.01, 0.1]},\n",
              "             verbose=2)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.svm import SVR\n",
        "\n",
        "parameters = {'gamma' : [1e-8, 1e-7, 1e-6, 1e-5, 1e-4, 1e-3, 1e-2, 1e-1], #kernel coefficient [ici kernel = Radial Basis Function]\n",
        "              'epsilon' : [0.001, 0.01, 0.1, 1], #erreur tolérée par l'algorithme\n",
        "              'C' : [0.001, 0.01, 0.1, 1, 10]} #parametre de régularisation\n",
        "\n",
        "svm = GridSearchCV(estimator = SVR(), \n",
        "                      param_grid = parameters,\n",
        "                      #scoring = 'neg_mean_squared_error',\n",
        "                      cv=5,\n",
        "                   verbose=2\n",
        "                     )\n",
        "\n",
        "svm.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yb3Vs3fvaezr",
        "outputId": "dff819b9-efe4-4e63-88c2-1add06bd0fe8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'C': 10, 'epsilon': 0.1, 'gamma': 0.1}"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "svm.best_params_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gIOJCBqjaqnJ"
      },
      "outputs": [],
      "source": [
        "results = results.append(pd.DataFrame({\n",
        "    'Modèle' : ['Support Vector Machine'],\n",
        "    'Score_RMSE' : [math.sqrt(mean_squared_error(svm.predict(X_test), y_test))]}),\n",
        "              ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oP6eD_Oeavd6"
      },
      "source": [
        "# **XG BOOST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fwxOUm7ratPR"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LKNSMcQrazjo",
        "outputId": "a44b7d7b-13cb-48bd-b129-aed983915670"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
            "[22:21:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV 1/5] END ..................n_estimators=100;, score=0.730 total time=   0.7s\n",
            "[22:21:32] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV 2/5] END ..................n_estimators=100;, score=0.540 total time=   0.6s\n",
            "[22:21:33] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV 3/5] END ..................n_estimators=100;, score=0.514 total time=   0.6s\n",
            "[22:21:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV 4/5] END ..................n_estimators=100;, score=0.571 total time=   0.6s\n",
            "[22:21:34] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV 5/5] END ..................n_estimators=100;, score=0.597 total time=   0.6s\n",
            "[22:21:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV 1/5] END ..................n_estimators=500;, score=0.690 total time=   2.8s\n",
            "[22:21:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV 2/5] END ..................n_estimators=500;, score=0.623 total time=   2.8s\n",
            "[22:21:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV 3/5] END ..................n_estimators=500;, score=0.567 total time=   4.9s\n",
            "[22:21:45] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV 4/5] END ..................n_estimators=500;, score=0.630 total time=   2.8s\n",
            "[22:21:48] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV 5/5] END ..................n_estimators=500;, score=0.686 total time=   2.8s\n",
            "[22:21:51] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV 1/5] END .................n_estimators=1000;, score=0.645 total time=   5.6s\n",
            "[22:21:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV 2/5] END .................n_estimators=1000;, score=0.646 total time=   5.5s\n",
            "[22:22:02] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV 3/5] END .................n_estimators=1000;, score=0.554 total time=   5.5s\n",
            "[22:22:08] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV 4/5] END .................n_estimators=1000;, score=0.654 total time=   5.5s\n",
            "[22:22:13] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV 5/5] END .................n_estimators=1000;, score=0.718 total time=   5.5s\n",
            "[22:22:19] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV 1/5] END .................n_estimators=2000;, score=0.631 total time=  11.1s\n",
            "[22:22:30] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV 2/5] END .................n_estimators=2000;, score=0.653 total time=  11.0s\n",
            "[22:22:41] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV 3/5] END .................n_estimators=2000;, score=0.538 total time=  13.3s\n",
            "[22:22:54] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV 4/5] END .................n_estimators=2000;, score=0.666 total time=  11.2s\n",
            "[22:23:05] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n",
            "[CV 5/5] END .................n_estimators=2000;, score=0.727 total time=  11.1s\n",
            "[22:23:16] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=XGBRegressor(n_jobs=-1),\n",
              "             param_grid={'n_estimators': [100, 500, 1000, 2000]}, verbose=5)"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "parameters = {\n",
        "    'n_estimators' : [100,500,1000,2000]\n",
        "}\n",
        "xgb_grid = GridSearchCV(XGBRegressor(n_jobs=-1),\n",
        "                        param_grid = parameters,\n",
        "                      #scoring = 'neg_mean_squared_error',\n",
        "                        cv = 5,\n",
        "                        verbose=5)\n",
        "xgb_grid.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IRxIPZNsa2T9",
        "outputId": "2c07fe66-a69f-4528-88d3-88f8a4a79d71"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'n_estimators': 1000}"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xgb_grid.best_params_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IotbnM3la6d4",
        "outputId": "beeb6dc7-38a2-4b60-bdaf-dc66ac76a994"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "XGBRegressor(n_estimators=1000, n_jobs=-1)"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xgb_grid.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VvCip7Mza8K3",
        "outputId": "5407abde-a093-4730-c947-b2a0b67d1f34"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.02481932, 0.01140361, 0.05366834, 0.01004252, 0.01443709,\n",
              "       0.01100618, 0.0147924 , 0.01404577, 0.00426091, 0.05515345,\n",
              "       0.01187518, 0.02925246, 0.01515568, 0.        , 0.00219652,\n",
              "       0.01904273, 0.00519671, 0.00269755, 0.07855707, 0.00175823,\n",
              "       0.00260184, 0.02701766, 0.01034383, 0.00558681, 0.01035965,\n",
              "       0.06321386, 0.00251432, 0.00675284, 0.00365541, 0.00124523,\n",
              "       0.02201283, 0.00587489, 0.00854196, 0.00257208, 0.00521532,\n",
              "       0.00405444, 0.07507273, 0.00837072, 0.00202411, 0.01514752,\n",
              "       0.00708371, 0.00362145, 0.03059353, 0.01225298, 0.0032513 ,\n",
              "       0.0054728 , 0.00616565, 0.00541232, 0.00015615, 0.01732958,\n",
              "       0.03429193, 0.04425753, 0.        , 0.00120266, 0.0171863 ,\n",
              "       0.00778448, 0.00525778, 0.00530736, 0.01025098, 0.01165278,\n",
              "       0.00172739, 0.00578293, 0.00290981, 0.00043212, 0.00034407,\n",
              "       0.00075116, 0.00034501, 0.00501804, 0.00070965, 0.00096295,\n",
              "       0.        , 0.00138994, 0.0024797 , 0.00710026, 0.00164737,\n",
              "       0.00467726, 0.03893423, 0.03471707, 0.        ], dtype=float32)"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xgb_grid.best_estimator_.feature_importances_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hQI0gG47a9-Y"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "results = results.append(pd.DataFrame({\n",
        "    'Modèle' : ['XGBoost'],\n",
        "    'Score_RMSE' : [math.sqrt(mean_squared_error(xgb_grid.predict(X_test), y_test))]}),\n",
        "              ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8BBotl7ubBOO",
        "outputId": "a56d3826-fa03-44ee-9cd1-4d3b1bdb51a5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8fa6a7f4-7675-4879-b54f-aee440a6e0db\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Modèle</th>\n",
              "      <th>Score_RMSE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Elasticnet Regression</td>\n",
              "      <td>0.906957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Random Forest Regressor</td>\n",
              "      <td>0.629762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Support Vector Machine</td>\n",
              "      <td>0.685169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.653062</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8fa6a7f4-7675-4879-b54f-aee440a6e0db')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8fa6a7f4-7675-4879-b54f-aee440a6e0db button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8fa6a7f4-7675-4879-b54f-aee440a6e0db');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                    Modèle  Score_RMSE\n",
              "0  Elasticnet Regression    0.906957  \n",
              "1  Random Forest Regressor  0.629762  \n",
              "2  Support Vector Machine   0.685169  \n",
              "3  XGBoost                  0.653062  "
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwyDbKdNbEdA"
      },
      "source": [
        "# **Export des modèles**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CdJqk7DcbEF4",
        "outputId": "48a4b4b6-d08f-45e6-e142-9bad7550db3c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, estimator=RandomForestRegressor(),\n",
              "             param_grid={'max_features': ['auto', 'sqrt'],\n",
              "                         'min_samples_leaf': [1, 3, 5, 10],\n",
              "                         'n_estimators': [10, 50, 100, 300, 500]},\n",
              "             verbose=2)"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rfr_search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LtC425pUbC83"
      },
      "outputs": [],
      "source": [
        "dict_modeles = {\n",
        "    'Elastic Net': elastic_grid.best_estimator_,\n",
        "    'Random Forest Regressor' : rfr_search.best_estimator_,\n",
        "    'SVM' : svm.best_estimator_,\n",
        "    'XGBoost' : xgb_grid.best_estimator_,\n",
        "\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "utEDpVPGbK2V"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "for nom, model in dict_modeles.items():\n",
        "    filename = 'models/' + nom + '.obj' \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2veHTJO2cIWV"
      },
      "outputs": [],
      "source": [
        "with open('data_pick.pkl', 'wb') as pickle_file :\n",
        "        pickle.dump(dict_modeles, pickle_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-Atz8LwtbNgN",
        "outputId": "f8b3efd7-bb3d-444c-cc68-de7847e39b0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[22:23:24] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ]
        }
      ],
      "source": [
        "with open('data_pick.pkl','rb') as pickle_file:\n",
        "    new_data = pickle.load(pickle_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "k2OTJBhJgra3",
        "outputId": "c6af00d6-1ad2-4b7c-e17e-dbdded7d0e2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Elastic Net': ElasticNet(alpha=0.001, l1_ratio=0.6000000000000001, tol=0.1),\n",
              " 'Random Forest Regressor': RandomForestRegressor(max_features='sqrt', n_estimators=500),\n",
              " 'SVM': SVR(C=10, gamma=0.1),\n",
              " 'XGBoost': XGBRegressor(missing=nan, n_estimators=1000, n_jobs=-1)}"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NSBchYsVhG58"
      },
      "outputs": [],
      "source": [
        "#Export du one hot encoder\n",
        "\n",
        "with  open('onehotencoder', 'wb') as pickle_file :\n",
        "    pickle.dump(ohe, pickle_file)\n",
        "    \n",
        "#export du standardscaler\n",
        "with  open('standardscaler', 'wb') as pickle_file :\n",
        "    pickle.dump(ss, pickle_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sj0DYEtYbRM7"
      },
      "source": [
        "# **Chargement des modèles**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gvhqWO7kbPmH"
      },
      "outputs": [],
      "source": [
        "dict_modeles_new = dict_modeles\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rgyHQRMebVuO"
      },
      "outputs": [],
      "source": [
        "dict_modeles_new = {}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TCL49jnMbXE6"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pickle\n",
        "for filename in glob.glob('models/*'):\n",
        "    if filename not in ['onehotencoder', 'standardscaler']:\n",
        "        with open(filename, 'rb') as pickle_file:\n",
        "            print(filename)\n",
        "            dict_modeles_new[str(filename)] = pickle.load(pickle_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ydgvdQbUbaEV",
        "outputId": "0deb93b6-4f9f-4cb4-c55a-956dcad8c05e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.02481932, 0.01140361, 0.05366834, 0.01004252, 0.01443709,\n",
              "       0.01100618, 0.0147924 , 0.01404577, 0.00426091, 0.05515345,\n",
              "       0.01187518, 0.02925246, 0.01515568, 0.        , 0.00219652,\n",
              "       0.01904273, 0.00519671, 0.00269755, 0.07855707, 0.00175823,\n",
              "       0.00260184, 0.02701766, 0.01034383, 0.00558681, 0.01035965,\n",
              "       0.06321386, 0.00251432, 0.00675284, 0.00365541, 0.00124523,\n",
              "       0.02201283, 0.00587489, 0.00854196, 0.00257208, 0.00521532,\n",
              "       0.00405444, 0.07507273, 0.00837072, 0.00202411, 0.01514752,\n",
              "       0.00708371, 0.00362145, 0.03059353, 0.01225298, 0.0032513 ,\n",
              "       0.0054728 , 0.00616565, 0.00541232, 0.00015615, 0.01732958,\n",
              "       0.03429193, 0.04425753, 0.        , 0.00120266, 0.0171863 ,\n",
              "       0.00778448, 0.00525778, 0.00530736, 0.01025098, 0.01165278,\n",
              "       0.00172739, 0.00578293, 0.00290981, 0.00043212, 0.00034407,\n",
              "       0.00075116, 0.00034501, 0.00501804, 0.00070965, 0.00096295,\n",
              "       0.        , 0.00138994, 0.0024797 , 0.00710026, 0.00164737,\n",
              "       0.00467726, 0.03893423, 0.03471707, 0.        ], dtype=float32)"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dict_modeles['XGBoost'].feature_importances_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNiJFB9wbc8b"
      },
      "source": [
        "# **Comparaison des modèles**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBrfsyCdbiRu"
      },
      "source": [
        "Comparaison des modèles sur le critère de la division RMSE par la  moyenne de la valeur à prédire."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "h7gnrRGubb-7",
        "outputId": "61ff2ce8-5bec-4117-ba11-f5ef45d7cb37"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-cf8d1505-e7d8-42af-9870-3d91a5311971\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Modèle</th>\n",
              "      <th>Score_RMSE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Elasticnet Regression</td>\n",
              "      <td>0.906957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Random Forest Regressor</td>\n",
              "      <td>0.629762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Support Vector Machine</td>\n",
              "      <td>0.685169</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.653062</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cf8d1505-e7d8-42af-9870-3d91a5311971')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cf8d1505-e7d8-42af-9870-3d91a5311971 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cf8d1505-e7d8-42af-9870-3d91a5311971');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                    Modèle  Score_RMSE\n",
              "0  Elasticnet Regression    0.906957  \n",
              "1  Random Forest Regressor  0.629762  \n",
              "2  Support Vector Machine   0.685169  \n",
              "3  XGBoost                  0.653062  "
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_ySMvcXkbvwD",
        "outputId": "32b67e1d-bc8f-4231-9f77-0af26aff2b2d"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(10,4))\n",
        "plt.title('Comparaison des RMSE relatives des modèles (en %)')\n",
        "chart = sns.barplot(x = results['Modèle'],\n",
        "        y = results['Score_RMSE']*100)\n",
        "chart.set_xticklabels(labels = results['Modèle'], \n",
        "                      rotation=45,\n",
        "                     horizontalalignment='right',\n",
        "                     size=12,\n",
        "                     )\n",
        "ax = plt.gca()\n",
        "ax.set_ylim([0, 5])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iXf8-Ezxb7pl",
        "outputId": "d4c9f9d6-1853-4747-c950-e7fa9759147b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.4973121860465215"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "math.sqrt(mean_squared_error(y_train.mean()*np.ones(y_test.shape[0]), y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jQyzjtDecF2t",
        "outputId": "e257f2e8-03a0-41ed-aa24-b0eaf4d7ac74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RandomForestRegressor(max_features='sqrt', n_estimators=500)"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rfr_search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "z5dXdbbncHjl",
        "outputId": "4fb7d6f0-bf13-4b32-85d0-9f4964c42b84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Elastic Net', 'Random Forest Regressor', 'SVM', 'XGBoost']"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(dict_modeles.keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pejCSJZ0cVdp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "\n",
        "\n",
        "nom_modeles = ['Elastic Net', 'Random Forest Regressor', 'SVM', 'XGBoost']\n",
        "liste_modeles = [dict_modeles['Elastic Net'],\n",
        "                 dict_modeles['Random Forest Regressor'],\n",
        "                 dict_modeles['SVM'],\n",
        "                 dict_modeles['XGBoost']\n",
        "                ]\n",
        "\n",
        "comparaison_score = pd.DataFrame(index = nom_modeles, columns = ['RMSE', 'R2', 'RMSE_relative'])\n",
        "\n",
        "for nom_modele, modele in zip(nom_modeles, liste_modeles):\n",
        "    comparaison_score.loc[nom_modele, 'RMSE'] = math.sqrt(mean_squared_error(y_test, modele.predict(X_test)))\n",
        "    comparaison_score.loc[nom_modele, 'R2'] = r2_score(y_test, modele.predict(X_test))\n",
        "\n",
        "#normalisation RMSE\n",
        "comparaison_score['RMSE_relative'] = comparaison_score['RMSE'].divide(comparaison_score['RMSE'].max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uZdDasTJcZAI",
        "outputId": "51c7a26f-dd9f-4670-8128-2f179f2a06e3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7df41d11-5ea0-40cf-8b5d-758ecd318cee\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RMSE</th>\n",
              "      <th>R2</th>\n",
              "      <th>RMSE_relative</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Elastic Net</th>\n",
              "      <td>0.906957</td>\n",
              "      <td>0.631751</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random Forest Regressor</th>\n",
              "      <td>0.629762</td>\n",
              "      <td>0.82245</td>\n",
              "      <td>0.694368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SVM</th>\n",
              "      <td>0.685169</td>\n",
              "      <td>0.789834</td>\n",
              "      <td>0.755459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>XGBoost</th>\n",
              "      <td>0.653062</td>\n",
              "      <td>0.809069</td>\n",
              "      <td>0.720058</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7df41d11-5ea0-40cf-8b5d-758ecd318cee')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7df41d11-5ea0-40cf-8b5d-758ecd318cee button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7df41d11-5ea0-40cf-8b5d-758ecd318cee');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                             RMSE        R2 RMSE_relative\n",
              "Elastic Net              0.906957  0.631751  1.0         \n",
              "Random Forest Regressor  0.629762  0.82245   0.694368    \n",
              "SVM                      0.685169  0.789834  0.755459    \n",
              "XGBoost                  0.653062  0.809069  0.720058    "
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "comparaison_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fLyU7o54cbDR",
        "outputId": "845f1d7f-a40c-47b1-cada-99ccb915f841"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Elastic Net : \n",
            "3.97 ms ± 154 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n",
            "\n",
            "Random Forest Regressor : \n",
            "670 ms ± 5.92 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
            "\n",
            "SVM : \n",
            "2.29 s ± 174 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
            "\n",
            "XGBoost : \n",
            "103 ms ± 1.44 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "i=0\n",
        "for modele in liste_modeles:\n",
        "    print(nom_modeles[i], ': ')\n",
        "    i+=1\n",
        "    %timeit modele.predict(X_train)\n",
        "    print('')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7VWh9ctJc8qG",
        "outputId": "3d477b15-e2e6-477d-c0f1-a6d6ffb49ace"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.title('Temps d\\'exécution des algorithmes pour la prédiction \\n(jeu d\\'entrainement) - échelle logarithmique')\n",
        "sns.barplot(x=nom_modeles,\n",
        "           y = [5.32, 640, 2.14, 145])\n",
        "ax = plt.gca()\n",
        "ax.set_yscale('log')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VEiaX4U5c_au"
      },
      "outputs": [],
      "source": [
        "comparaison_score_melt = pd.melt(comparaison_score.reset_index(), id_vars = ['index'], value_name = 'score', value_vars=['R2', 'RMSE_relative'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iGx2SYBkdKTq",
        "outputId": "7de075ff-edba-43be-e7e8-b9c8fbc720b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Comparaison des performances des modèles (jeu de test)')"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(8,6))\n",
        "sns.barplot(x = comparaison_score_melt['index'],\n",
        "            y = comparaison_score_melt['score'], hue = comparaison_score_melt['variable'])\n",
        "plt.title('Comparaison des performances des modèles (jeu de test)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev0KN5CedQHA"
      },
      "source": [
        "# **Verification des prédictions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KMkRovFqdMuY"
      },
      "outputs": [],
      "source": [
        "X_extrait = X_test\n",
        "y_extrait = y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uiftTpkXdVXJ",
        "outputId": "b1a03d61-c24d-4995-dbd3-b1468343fbe1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-a921913a-270f-48ee-8a05-bcea96bb2466\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>NumberofBuildings</th>\n",
              "      <th>NumberofFloors</th>\n",
              "      <th>LargestPropertyUseTypeGFA</th>\n",
              "      <th>SecondLargestPropertyUseTypeGFA</th>\n",
              "      <th>ThirdLargestPropertyUseTypeGFA</th>\n",
              "      <th>Latitude</th>\n",
              "      <th>Longitude</th>\n",
              "      <th>x0_Campus</th>\n",
              "      <th>x0_Multifamily HR (10+)</th>\n",
              "      <th>x0_Multifamily LR (1-4)</th>\n",
              "      <th>x0_Multifamily MR (5-9)</th>\n",
              "      <th>x0_NonResidential</th>\n",
              "      <th>x0_Nonresidential COS</th>\n",
              "      <th>x0_Nonresidential WA</th>\n",
              "      <th>x0_SPS-District K-12</th>\n",
              "      <th>x1_Education</th>\n",
              "      <th>x1_Facility</th>\n",
              "      <th>x1_Health</th>\n",
              "      <th>x1_Leisure</th>\n",
              "      <th>x1_Office</th>\n",
              "      <th>x1_Offices</th>\n",
              "      <th>x1_Other</th>\n",
              "      <th>x1_Residence/Hotel/Senior Care/Housing</th>\n",
              "      <th>x1_Retail</th>\n",
              "      <th>x1_Storage</th>\n",
              "      <th>x1_Supermarket / Grocery Store</th>\n",
              "      <th>x1_Warehouse</th>\n",
              "      <th>x2_BALLARD</th>\n",
              "      <th>x2_CENTRAL</th>\n",
              "      <th>x2_DELRIDGE</th>\n",
              "      <th>x2_DOWNTOWN</th>\n",
              "      <th>x2_EAST</th>\n",
              "      <th>x2_GREATER DUWAMISH</th>\n",
              "      <th>x2_LAKE UNION</th>\n",
              "      <th>x2_MAGNOLIA / QUEEN ANNE</th>\n",
              "      <th>x2_NORTH</th>\n",
              "      <th>x2_NORTHEAST</th>\n",
              "      <th>x2_NORTHWEST</th>\n",
              "      <th>x2_SOUTHEAST</th>\n",
              "      <th>x2_SOUTHWEST</th>\n",
              "      <th>x3_Education</th>\n",
              "      <th>x3_Facility</th>\n",
              "      <th>x3_Health</th>\n",
              "      <th>x3_Leisure</th>\n",
              "      <th>x3_Office</th>\n",
              "      <th>x3_Offices</th>\n",
              "      <th>x3_Other</th>\n",
              "      <th>x3_Parking</th>\n",
              "      <th>x3_Personal Services (Health/Beauty, Dry Cleaning, etc)</th>\n",
              "      <th>x3_Residence/Hotel/Senior Care/Housing</th>\n",
              "      <th>x3_Retail</th>\n",
              "      <th>x3_Storage</th>\n",
              "      <th>x4_Education</th>\n",
              "      <th>x4_Facility</th>\n",
              "      <th>x4_Health</th>\n",
              "      <th>x4_Leisure</th>\n",
              "      <th>x4_Office</th>\n",
              "      <th>x4_Offices</th>\n",
              "      <th>x4_Other</th>\n",
              "      <th>x4_Parking</th>\n",
              "      <th>x4_Personal Services (Health/Beauty, Dry Cleaning, etc)</th>\n",
              "      <th>x4_Residence/Hotel/Senior Care/Housing</th>\n",
              "      <th>x4_Retail</th>\n",
              "      <th>x4_Storage</th>\n",
              "      <th>x5_Education</th>\n",
              "      <th>x5_Facility</th>\n",
              "      <th>x5_Health</th>\n",
              "      <th>x5_Leisure</th>\n",
              "      <th>x5_Office</th>\n",
              "      <th>x5_Offices</th>\n",
              "      <th>x5_Other</th>\n",
              "      <th>x5_Parking</th>\n",
              "      <th>x5_Personal Services (Health/Beauty, Dry Cleaning, etc)</th>\n",
              "      <th>x5_Residence/Hotel/Senior Care/Housing</th>\n",
              "      <th>x5_Retail</th>\n",
              "      <th>x5_Storage</th>\n",
              "      <th>x6_High Outlier</th>\n",
              "      <th>x6_Low Outlier</th>\n",
              "      <th>x6_Normal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1703</th>\n",
              "      <td>-0.034419</td>\n",
              "      <td>-0.328997</td>\n",
              "      <td>-0.540707</td>\n",
              "      <td>-0.308135</td>\n",
              "      <td>-0.159065</td>\n",
              "      <td>1.480401</td>\n",
              "      <td>0.670863</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5240</th>\n",
              "      <td>-0.034419</td>\n",
              "      <td>0.762691</td>\n",
              "      <td>0.633714</td>\n",
              "      <td>0.803847</td>\n",
              "      <td>-0.159065</td>\n",
              "      <td>-0.493824</td>\n",
              "      <td>0.349614</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5471</th>\n",
              "      <td>-0.034419</td>\n",
              "      <td>-0.601920</td>\n",
              "      <td>-0.422841</td>\n",
              "      <td>-0.362042</td>\n",
              "      <td>-0.159065</td>\n",
              "      <td>-0.043409</td>\n",
              "      <td>1.742348</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4210</th>\n",
              "      <td>-0.034419</td>\n",
              "      <td>-0.328997</td>\n",
              "      <td>-0.513409</td>\n",
              "      <td>-0.362042</td>\n",
              "      <td>-0.159065</td>\n",
              "      <td>2.244934</td>\n",
              "      <td>0.274828</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1480</th>\n",
              "      <td>-0.034419</td>\n",
              "      <td>-0.874842</td>\n",
              "      <td>-0.586135</td>\n",
              "      <td>-0.298820</td>\n",
              "      <td>-0.159065</td>\n",
              "      <td>-1.098073</td>\n",
              "      <td>-0.573856</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a921913a-270f-48ee-8a05-bcea96bb2466')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a921913a-270f-48ee-8a05-bcea96bb2466 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a921913a-270f-48ee-8a05-bcea96bb2466');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      NumberofBuildings  NumberofFloors  LargestPropertyUseTypeGFA  \\\n",
              "1703 -0.034419          -0.328997       -0.540707                    \n",
              "5240 -0.034419           0.762691        0.633714                    \n",
              "5471 -0.034419          -0.601920       -0.422841                    \n",
              "4210 -0.034419          -0.328997       -0.513409                    \n",
              "1480 -0.034419          -0.874842       -0.586135                    \n",
              "\n",
              "      SecondLargestPropertyUseTypeGFA  ThirdLargestPropertyUseTypeGFA  \\\n",
              "1703 -0.308135                        -0.159065                         \n",
              "5240  0.803847                        -0.159065                         \n",
              "5471 -0.362042                        -0.159065                         \n",
              "4210 -0.362042                        -0.159065                         \n",
              "1480 -0.298820                        -0.159065                         \n",
              "\n",
              "      Latitude  Longitude  x0_Campus  x0_Multifamily HR (10+)  \\\n",
              "1703  1.480401  0.670863   0          0                         \n",
              "5240 -0.493824  0.349614   0          0                         \n",
              "5471 -0.043409  1.742348   1          0                         \n",
              "4210  2.244934  0.274828   0          0                         \n",
              "1480 -1.098073 -0.573856   0          0                         \n",
              "\n",
              "      x0_Multifamily LR (1-4)  x0_Multifamily MR (5-9)  x0_NonResidential  \\\n",
              "1703  1                        0                        0                   \n",
              "5240  0                        1                        0                   \n",
              "5471  0                        0                        0                   \n",
              "4210  0                        0                        1                   \n",
              "1480  0                        0                        1                   \n",
              "\n",
              "      x0_Nonresidential COS  x0_Nonresidential WA  x0_SPS-District K-12  \\\n",
              "1703  0                      0                     0                      \n",
              "5240  0                      0                     0                      \n",
              "5471  0                      0                     0                      \n",
              "4210  0                      0                     0                      \n",
              "1480  0                      0                     0                      \n",
              "\n",
              "      x1_Education  x1_Facility  x1_Health  x1_Leisure  x1_Office  x1_Offices  \\\n",
              "1703  0             0            0          0           0          0            \n",
              "5240  0             0            0          0           0          0            \n",
              "5471  1             0            0          0           0          0            \n",
              "4210  1             0            0          0           0          0            \n",
              "1480  0             0            0          0           0          0            \n",
              "\n",
              "      x1_Other  x1_Residence/Hotel/Senior Care/Housing  x1_Retail  x1_Storage  \\\n",
              "1703  0         1                                       0          0            \n",
              "5240  0         1                                       0          0            \n",
              "5471  0         0                                       0          0            \n",
              "4210  0         0                                       0          0            \n",
              "1480  0         0                                       0          1            \n",
              "\n",
              "      x1_Supermarket / Grocery Store  x1_Warehouse  x2_BALLARD  x2_CENTRAL  \\\n",
              "1703  0                               0             0           0            \n",
              "5240  0                               0             0           0            \n",
              "5471  0                               0             0           1            \n",
              "4210  0                               0             0           0            \n",
              "1480  0                               0             0           0            \n",
              "\n",
              "      x2_DELRIDGE  x2_DOWNTOWN  x2_EAST  x2_GREATER DUWAMISH  x2_LAKE UNION  \\\n",
              "1703  0            0            0        0                    0               \n",
              "5240  0            1            0        0                    0               \n",
              "5471  0            0            0        0                    0               \n",
              "4210  0            0            0        0                    0               \n",
              "1480  0            0            0        1                    0               \n",
              "\n",
              "      x2_MAGNOLIA / QUEEN ANNE  x2_NORTH  x2_NORTHEAST  x2_NORTHWEST  \\\n",
              "1703  0                         1         0             0              \n",
              "5240  0                         0         0             0              \n",
              "5471  0                         0         0             0              \n",
              "4210  0                         0         0             1              \n",
              "1480  0                         0         0             0              \n",
              "\n",
              "      x2_SOUTHEAST  x2_SOUTHWEST  x3_Education  x3_Facility  x3_Health  \\\n",
              "1703  0             0             0             0            0           \n",
              "5240  0             0             0             0            0           \n",
              "5471  0             0             1             0            0           \n",
              "4210  0             0             1             0            0           \n",
              "1480  0             0             0             0            0           \n",
              "\n",
              "      x3_Leisure  x3_Office  x3_Offices  x3_Other  x3_Parking  \\\n",
              "1703  0           0          0           0         0            \n",
              "5240  0           0          0           0         0            \n",
              "5471  0           0          0           0         0            \n",
              "4210  0           0          0           0         0            \n",
              "1480  0           0          0           0         0            \n",
              "\n",
              "      x3_Personal Services (Health/Beauty, Dry Cleaning, etc)  \\\n",
              "1703  0                                                         \n",
              "5240  0                                                         \n",
              "5471  0                                                         \n",
              "4210  0                                                         \n",
              "1480  0                                                         \n",
              "\n",
              "      x3_Residence/Hotel/Senior Care/Housing  x3_Retail  x3_Storage  \\\n",
              "1703  1                                       0          0            \n",
              "5240  1                                       0          0            \n",
              "5471  0                                       0          0            \n",
              "4210  0                                       0          0            \n",
              "1480  0                                       0          1            \n",
              "\n",
              "      x4_Education  x4_Facility  x4_Health  x4_Leisure  x4_Office  x4_Offices  \\\n",
              "1703  0             0            0          0           0          0            \n",
              "5240  0             0            0          0           0          0            \n",
              "5471  0             0            0          0           0          0            \n",
              "4210  0             0            0          0           0          0            \n",
              "1480  0             0            0          0           1          0            \n",
              "\n",
              "      x4_Other  x4_Parking  \\\n",
              "1703  0         1            \n",
              "5240  0         1            \n",
              "5471  1         0            \n",
              "4210  1         0            \n",
              "1480  0         0            \n",
              "\n",
              "      x4_Personal Services (Health/Beauty, Dry Cleaning, etc)  \\\n",
              "1703  0                                                         \n",
              "5240  0                                                         \n",
              "5471  0                                                         \n",
              "4210  0                                                         \n",
              "1480  0                                                         \n",
              "\n",
              "      x4_Residence/Hotel/Senior Care/Housing  x4_Retail  x4_Storage  \\\n",
              "1703  0                                       0          0            \n",
              "5240  0                                       0          0            \n",
              "5471  0                                       0          0            \n",
              "4210  0                                       0          0            \n",
              "1480  0                                       0          0            \n",
              "\n",
              "      x5_Education  x5_Facility  x5_Health  x5_Leisure  x5_Office  x5_Offices  \\\n",
              "1703  0             0            0          0           0          0            \n",
              "5240  0             0            0          0           0          0            \n",
              "5471  0             0            0          0           0          0            \n",
              "4210  0             0            0          0           0          0            \n",
              "1480  0             0            0          0           0          0            \n",
              "\n",
              "      x5_Other  x5_Parking  \\\n",
              "1703  1         0            \n",
              "5240  1         0            \n",
              "5471  1         0            \n",
              "4210  1         0            \n",
              "1480  1         0            \n",
              "\n",
              "      x5_Personal Services (Health/Beauty, Dry Cleaning, etc)  \\\n",
              "1703  0                                                         \n",
              "5240  0                                                         \n",
              "5471  0                                                         \n",
              "4210  0                                                         \n",
              "1480  0                                                         \n",
              "\n",
              "      x5_Residence/Hotel/Senior Care/Housing  x5_Retail  x5_Storage  \\\n",
              "1703  0                                       0          0            \n",
              "5240  0                                       0          0            \n",
              "5471  0                                       0          0            \n",
              "4210  0                                       0          0            \n",
              "1480  0                                       0          0            \n",
              "\n",
              "      x6_High Outlier  x6_Low Outlier  x6_Normal  \n",
              "1703  0                0               1          \n",
              "5240  0                0               1          \n",
              "5471  0                0               1          \n",
              "4210  0                0               1          \n",
              "1480  0                0               1          "
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_extrait.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IrJRgsdKdXeo",
        "outputId": "1cf325cd-4096-4522-e234-2b896281f7a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1703    19.335591\n",
              "5240    22.219276\n",
              "5471    20.799023\n",
              "4210    20.276983\n",
              "1480    18.141942\n",
              "Name: Log2-SiteEnergyUseWN(kBtu), dtype: float64"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_extrait.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "O6ZGqAYYdZce"
      },
      "outputs": [],
      "source": [
        "prediction = dict_modeles['Random Forest Regressor'].predict(X_extrait)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2sJFrSUfdb8w",
        "outputId": "6fc7a7ec-c93e-408a-b8cc-e21a88c58f34"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.DataFrame(prediction).isna().sum()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tl0cMR90deQP",
        "outputId": "62922770-fdf7-4663-c083-677c80e52579"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(y_extrait)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FBgFf9msdf_t"
      },
      "outputs": [],
      "source": [
        "predictions = pd.DataFrame([y_extrait, \n",
        "                            pd.Series(prediction, index = y_extrait.index),\n",
        "                           np.exp2(y_extrait)-1,\n",
        "                           np.exp2(pd.Series(prediction, index = y_extrait.index))-1]).T\n",
        "predictions.columns = ['y_log', 'y_predict_log', 'y','y_predict']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "LDSU6pIudi6x",
        "outputId": "e04e9557-9127-42db-bee6-94ca341ab0c1"
      },
      "outputs": [
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAz4AAAFOCAYAAABHdsgwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeXxU1fn48c8z2SY7JIQQAkmMBFBWMVVqjbVQlVrcUHGp2lpaan9VaLHqV79asVj7pba0Um0rahdR60arFSm1gi1YtwYFZJHFQCAQAoSQPYRkzu+Pe2e8M5mZLJD9eb9evJiZe+695965mZnnnnOeI8YYlFJKKaWUUqovc3V3BZRSSimllFKqs2ngo5RSSimllOrzNPBRSimllFJK9Xka+CillFJKKaX6PA18lFJKKaWUUn2eBj5KKaWUUkqpPk8Dny4kIptF5Pzurkd/ISLni0iJ43mHzr+IFIjItpNaOf/t/0dEzuis7Tv2E3g+3CLyTxG5MkjZ+SLyTGfXqTUiYkRkxEnYzjIR+crJqJNSSimleqceGfiIyPUiUigiNSJSKiJ/F5Fzu7teJ8oYM8YY86/urkdHnKwfoN2prec/8FiNMWuNMaM6o04icglQbYz5qDO2H44xpgG4HJgtIhO7ev9dbCHwYHdXQimllFLdp8cFPiIyD/gV8BCQDmQBvwEu6856tUZEIru7Dn2diER0dx06wS3A0u7auTGm1hhzkTFmfXfVoSsYYz4AkkQkv7vropRSSqnu0aMCHxFJBn4MfM8Y8xf7R9lxY8xrxpg77DIxIvIrEdlv//uViMTYy84XkRIRuVNEDtqtRZeLyMUisl1EjojIPY79zReRl0XkBRGpFpEPRWSCY/n/iMin9rItInKFY9k37C5KvxSRcmC+iJwqIqtFpFxEDovIsyIywLHObhH5sv34LLtVq0pEykRkkaPcpXa3rKMi8i8ROS1gGz8UkY0iUmnX3R3kXEbbxzvO8dpgEakTkbQQ5/+bIrJVRCpE5B8ikm2/vsYussFuhbvmRPcV8H7dY5+v3SLyNcfyP4rIb0VkhYjUAl8SkaF2t6VDIrJLROY4ysfa61SIyBbgcwH7c57/CHu/3vd3nYgMD3as0rKL2Gn2+3LUfp8uDajzYyLyur3d90Xk1BDHHw1MAf7teG2+iLwkIs/Y638sIiNF5G77mt4rIhc6yg8Vkb/Z53+niHy7HefDeS53i8gPwrxXk0XkHfuYN0iYLoOtvEfzReRFEXnaPr7N0sZgRESS7fUOiUixiNwrIi57WYSI/MK+jnaJyK1itdw5b0j8C/hqW/allFJKqb6nRwU+wOcBN/DXMGX+F5gMTAQmAGcB9zqWD7G3kQn8CHgCuAE4EygA7hORUxzlLwNeAlKA54BXRCTKXvapvU4y8ADwjIhkONY9GyjCapn6CSDAT4GhwGnAcGB+iON4BHjEGJMEnAq8CCAiI4E/A98H0oAVwGv2j2SvmcA04BRgPPCNwI0bYxqB5+1j97oOWGWMORRYXkQuA+4BZtj7XWvXA2PMeXaxCcaYBGPMCyeyrwBDgEFY79fXgSUi4uxWdj3WuU0E3gFeAzbY5acC3xeRi+yy92Ody1OBi+zthTLPruPFQBLwTaCutWO1r43XgDeAwcBtwLMBdb4W63oZCOy06x9MHuAxxpQEvH4JVivQQOAj4B9Yf6uZWDcGHneUfR4owbrmrgIeEpEprZ0PO2B4DdgEDAO+DMwVkRaBgYhkAq9jdRVLAX4ILAsW1Dq2G+o9ArjUrvcA4G/AoyHOT6BfY/0t5gJfBG4CbraXfRv4CtbnwiSsLnyBtmJ9ZiillFKqPzLG9Jh/wNeAA62U+RS42PH8ImC3/fh8oB6IsJ8nAgY421F+HXC5/Xg+8J5jmQsoBQpC7Hs9cJn9+BvAnlbqejnwkeP5buDL9uM1WD+OBwWscx/wYkCd9gHnO7Zxg2P5z4Dfhdj/2cAeQOznhcDMEGX/DswK2G8dkG0/N8CIMMfa5n051jkfaALiHa+9CNxnP/4j8HTgPgK2cTfwB/txETDNsWw2UBLi/G/zvpdB6uV3rHY9S+zHBcABwOVY/mdgvqPOTzqWXQx8EmI/XyDgerevyX86nl8C1NDymh6AFVg3A4mO8j8F/tja+bDPZYn3/bJfu8ex7nzgGfvxXcDSgHr+A/h6qOsgzHs0H3jTsex0oD7MNWKAEUAE0Aic7lj2HeBf9uPVwHccy75srxvpeO3bwOpw16T+03/6T//pP/2n//ruv542LqUcGCQikcaYphBlhgLFjufF9mu+bRhjmu3H9fb/ZY7l9UCC4/le7wNjjMfu0jQUQERuwmoZyLGLJGC1TrRY1y6fjtWSU4D1A9UFVIQ4jllYd+8/EZFdwAPGmOWBx2fXaS/W3XOvA47Hdfgfv48x5n0RqQPOF5FSrB+QfwtRn2zgERH5hfOQ7P0WB1+lw/tyqjDG1DqeB76fznOcDQwVkaOO1yKwWqew13OWD1fv4VhBdHsNBfYaYzwB+wn3/jivN6cKrOskUOD1ejjINZ1g1+WIMaY6oC7ermPhzkc2VuvNVhHxvhaD1cIUKBu4WqxEDF5RwFshyoZ7j6Dl+XG38jcP1t9dFC3/9r3nPfBY/f42bYnA0SCvK6WUUqof6GmBz7vAMayWkpdDlNmP9eNqs/08y36to4Z7H9jddIYB+8Ua3/IEVledd40xzSKyHisY8DIB23rIfm2cMeaIiFxOiG48xpgdwHX2PmcAL4tIqn0szrEyYtdxXweP709YXdAOAC8bK5NXMHuBnxhjnu3gftqzL6eBIhLvCH6ysLpfeTnP8V5glzEmL8S2SrHOlfPaCGUvVhewTWHKBLMfGC4iLkfwkwVsb+d2wOoGJyKSaYzpyPu7H0gRkURH8JPFZ9dKuPOxFyg1xoxuw372YrX4fLvVkq2/Rx11GDiO9be/xX4t8FiHOcoPp6XTsLrgKaWUUqof6lFjfIwxlVjjch4TKylBnIhEichXRORndrE/A/eKSJqIDLLLn8h8I2eKyAx7EPT3sQKv94B4rB/dhwBE5GZgbCvbSsTqllRpj4u4I1RBEblBRNLsH8/eu9AerK5eXxWRqfZ4ktvtOr3TweN7BrgCKyB5Oky53wF3i8gYu37JInK1Y3kZ1tiKk7GvQA+IlSChAJiONeYqmA+AahG5yx64HyEiY0XEO2j/RfsYBorIMKzxN6E8CSwQkTyxjLcDTwh/rO9jtVLcaV+b52N1R3u+zUdrM9bYqDexxqu0mzFmL9Z18VOx5uQZj9WS6P17CHc+PsC6Tu8JcS6dngEuEZGL7HJusRI+DAtStrX3qEPsFq8XgZ+ISKJ9Y2JewLHOFZFMsRKK3BVkM1/E6tKplFJKqX6oRwU+AMaYX2D9oLkXK+jYC9wKvGIXeRBr/MhG4GPgQ05sfo5XgWuwuh3dCMwwVia5LcAvsFqhyrBaYf7TyrYewBpYXYk1GPwvYcpOAzaLSA1W97hrjTH1xphtWIHDr7Hucl8CXGL/SG43+8fxh1hB3Now5f6KNdfJ8yJShdUS4pzwcT7wJ7Gyes08kX0FOIB17vcDzwK3GGM+CbH9ZqzAaCKwC+v8PIk14B2s819sL3uD8GmiF2H9WH4DqAKeAmLtZfMJcaz2+3AJ1rk5jJVq/aZQdW6Dx7Guu466Dqsr5n6spCD3G2PetJeFPB+OczmO4OcSR9m9WElA7uGzv8k7CPL50Yb36ETcBtRijV16GysZye/tZU9gHeNGrO56K7DGjzUD2IFXjbHSWiullFKqH/IORO+XRGQ+1iD2G1or25uJyO+B/caYe1st3IX7sltLnjHGBGs56DdE5D/AraYbJjHtq0TkK1hJP7wp2ZcBTxljVnRvzZRSSinVXXraGB91kolIDtYYojP60r76EmPMF7q7Dr2diMQCX8Jq9UnHSuXtS4tvjLmym6qmlFJKqR6ix3V1UyePiCzA6rL2sDFmV3fsyx5DUhPkn461UCeTYHXtq8Dq6rYVa/yfUkoppRTQz7u6KaWUUkoppfoHbfFRSimllFJK9Xka+CillFKqy4jIZju5jeoC9vQDJY7nHTr/IlIgIttOauV6EBH5o4icSJbgTiciXxCR/4pIShvLzxeRZ+zHOSJi7Olb+q1ec/CDBg0yOTk53V0NpVQvsW7dusPGmLTurodSHSUi12NN7zAaqAbWY000/Xa3VuwEGWPGdHcdOkpEDJBnjNnZ3XXpqLae/8BjNcasBUZ1Zt1UaCIyHHgI+Kox5kh316e36jWBT05ODoWFhd1dDaVULyEixd1dB6U6SkTmAf8D3AL8A2jEmv/tMqx5rHokEYk0xjR1dz36MhGJsOdMU73AifxNONe159Tr0ITn6jPa1U0ppZTqQUQkGfgx8D1jzF+MMbX2xNqvGWPusMvEiMivRGS//e9XIhJjLztfREpE5E4ROSgipSJyuYhcLCLbReSIiNzj2N98EXlZRF4QkWoR+VBEJjiW/4+IfGov2yIiVziWfUNE/iMivxSRcmC+iJwqIqtFpFxEDovIsyIywLHObhH5sv34LBEpFJEqESkTkUWOcpfa3bKOisi/ROS0gG38UEQ2ikilXXd3kHMZbR/vOMdrg0WkTkSCtgiLyDdFZKuIVIjIP0TEOx/YGrvIBjs76TUnuq+A9+se+3ztFpGvOZb/UUR+KyIrRKQW+JKIDBWRZSJySER2icgcR/lYe50KEdkCfC5gf87zH2Hv1/v+rhOR4cGOVVp2mTvNfl+O2u/TpQF1fkxEXre3+76InBrmHEwWkXfsbW0QR1c8ex8L7OusWkTeEJFBIbazVUSmO55H2udokv38JRE5YF8za0QkZOuXiEwXkfV2nd4RkfGOZUZERgQc74P2Y+/7eZeIHAD+ICKDRGS5va0jIrJWRIL+Bre3/T0R2QHsaENdQl4L4YhIsog8Jdbnwz4ReVBEIuxlI0Tk3/Z5OiwiL7Rlm72BBj5KKaVUz/J5wI1jLqog/heYDEwEJgBnAc6Jo4fY28jESu3+BHADcCZQANwnIqc4yl8GvASkAM8Br4hIlL3sU3udZKy08c+ISIZj3bOBIqw5tH6ClV7+p8BQ4DRgODA/xHE8AjxijEkCTgVeBBCRkcCfge8DacAK4DURiXasOxOrFewUYDzwjcCNG2MageftY/e6DlhljDkUWF5ELgPuwZqTLg1Ya9cDY8x5drEJxpgEY4zfj8H27ivAEGAQ1vv1dWCJiDi7lV2PdW4TgXeA14ANdvmpwPdF5CK77P1Y5/JU4CJ7e6HMs+t4MZAEfBOoa+1Y7WvjNay50wYDtwHPBtT5WqzrZSCw065/CyKSCbwOPIh1/f0QWBYQLF4P3GzvK9ouE8yf7ePxugg4bIz50H7+dyDP3s6HwLMh6nQG8HvgO0Aq8DjwN7FvLrTBEPtYsoHZwO1ACdY1lY51jYVLq3w51t/V6eHqYgdP4a6FcP4INAEjsOZfvBD4lr1sAdZ7OxAYBvy6LQfdG2jgo5RSql8Qkd+L1QKyqQ1lf2nfYV0vVivJ0a6ooy0V68dauO4xXwN+bIw5aP+ofgC40bH8ONZ4oONYP8YHYQUY1caYzcAWrIDJa50x5mW7/CKsoGkygDHmJWPMfmOMx/4BvAMr0PLab4z5tTGmyRhTb4zZaYz5pzHmmF23RYTuonMcGCEig4wxNcaY9+zXrwFet7dzHPg5EAuc41h3sV2vI1g//iaG2MefgOtEROznNwJLQ5S9BfipMWarff4fAiaK3erTBu3ZV6D77HP2b6xAYKZj2avGmP8YYzzAOCDNGPNjY0yjMaYIK7C91i47E+u9P2J3j1ocZp/fAu41xmwzlg3GmPI21HUykAD8n12H1cBy/IOOvxpjPrDP47OEfn9uAFYYY1bY19g/gUKsYMzrD8aY7caYeqzgONS2ngMuFZE4+/n12IErgDHm9/bfwDGsYHyCWC2sgWYDjxtj3jfGNBtj/gQcs4+7LTzA/fb7WY91nWcA2Xbr7VoTfj6Zn9rvX30rdfkc4a+FoEQkHev8ft9uUT4I/NKx3nGsoG2oMaaht48rdNLARymlVH/xR6wWglYZY35gjJlojJmIdbfzL51ZsQDlwCAJn31pKOAcx1Zsv+bbhmMcSL39f5ljeT3WD1evvd4H9o/rEu/2ROQmRzebo8BYrECqxbp2+XQRed7uPlMFPBNQ3mkWMBL4RKxsVd5uSn7HZ9dpL9Zdba8Djsd1AcfjY4x5315+voiMxrrD/bcQ9ckGHnEc6xGsFqzMEOVPZF9OFcaYWsfzwPfTeY6zgaHeOtr1vAerJQF7PWf5cOMdh2O16LXXUGCv/b4499Pu9wfreK4OOJ5zsQKFdm3LTsSwFbjEDn4uxQqGvN36/k+sbn1VwG57tWDXZjZwe0CdhuP/noRzyBjT4Hj+MFar1xsiUiQi/9PK+oHvd6i6tHYthJINRAGljvUex2oJA7gT67r/QKxujN9s/ZB7h16T3EAppZQ6EcaYNSKS43xNrHEHj2F1QakDvm2M+SRg1euwug91lXex7uheDrwcosx+rB8vm+3nWfZrHTXc+8DuPjMM2G+3dDyB1YXmXWNMs4isx/pR5BV45/oh+7VxxpgjInI58GiwnRpjdmC1kLiwupe9LCKp9rE4x8qIXcd9HTy+P2G1LBwAXg74Ueq0F6u1JGgXqJO8L6eBIhLvCH6yAGfLpPMc7wV2GWPyQmyrFOtcOa+NUPZidYlrtRU0wH5guIi4HMFPFrC9ndvx1mGpMebbHVg3GG93NxewxXyWge96rC6dX8YKepKBCvyvZWedfmKMCdo9D+uzIs7xfAjWzQIvv78JY0w1Vne320VkLLBaRP5rjFkVYvuB73fQuojI5wl/LYSyF+szZlCwlmVjzAHg2/Y+zgXeFJE1phdnM/TSFh+llFL92RLgNmPMmVjjBn7jXGj/8D8FWN1VFTLGVGKNy3lMrKQEcSISJSJfEZGf2cX+DNwrImliDfT+EVbLSkedKSIz7Fam72P9KHoPiMf6EXYIQERuxmrxCScRqAEq7fEbd4QqKCI3iEia/ePZ253Qg9Wd6asiMtUeT3K7Xad3Onh8zwBXYAUkT4cp9zvgbrEHvdsDwK92LC8Dck/SvgI9IFaChAJgOtaYq2A+AKrFGjwfa7dkjBURbxKDF+1jGCgiw7DG34TyJLBARPLEMt4OPCH8sXpbtu60r83zgUuwulW21zNYLTQX2cfiFitBwLAObAu7DhcC38Vu7bElYl1D5VhBy0NhtvEEcIuInG2fl3gR+aqIJNrL1wPX2/WdRivZ1sRKTjDCDuArgWas67wtwtWltWshKGNMKdYYnl+ISJKIuMRKSvJFu75XO85/BdZnQFvr26Np4KOUUqpfEpEErDEjL9mtGI/j370GrD7vL5suTh9sjPkF1sDze7GCjr3ArcArdpEHscZBbAQ+xhqofSKTL76KNa6mAmtcygx7LMIW4BdYrVBlWK0w/2llWw8Ak7B+4L1O+G6C04DNIlKDlejgWnuc0DaswOHXwGGsH9WXGCuBQLvZY10+xPoBtzZMub8CC4Hn7e5Qm4CvOIrMB/5kdw+aGWQTbd5XgANY534/1niYW4K0PHq334wVGE0EdmGdnyexWjDAOv/F9rI3CD/GaBFWoPQGUAU8hTWWCsIcq/0+XIJ1bg5j3TC4KVSdw7HPlzephPdav4MO/ka1f9S/i/W37UzK8DTWedmHNcbtvZZr+7ZRiNXi8SjW+7IT/+QZc7GO/yjWeLtXCC8PeBPrhsC7wG+MMW+18XhC1qUN10I4N2Elithib/dlPvv8+xzwvv13+Tdgrj1+qNeT8GOreo78/Hyj8/go1b94PIbd5bWUVTWQnuQmJzUelytYr4SWRGSdMSa/k6uoehm7q9tyY8xYEUkCthljAoMdZ/mPsNJKd7SloccTkfnACGPMDa2V7c1E5PdYiRjubbVwF+7Lbi15xhjT0RYOpVQb6RgfpVS3ChXceDyGlZsPMO/F9TQc9+COcrFo5kSmjRnS5uBHqXCMMVVizXtxtTHmJbsbynhjzAYAsQanD8S6Q6t6MTvgnYGVtrfP7Esp1T7a1U0p1W28wc3Fi9dy3RPvc/HitazcfMAXDHmDHoCG4x7mvbie3eW1rWxVqeBE5M9YQcwosSYYnIXVTWWWiGzAGgx+mWOVa4HnW0k7q3o4EVmA1WXtYWPMru7Yl1iThNYE+ff3zqyPUsqfdnVTSnWbokM1XLx4rS+4AXBHuVgxp4Cyqgaue+L9Fus8P/tsJueGyoz7Ge3qppRSSiknbfFRSnWbsqoGv6AHrJadg9VWtzd3lP9HlDvKxeBEd1dWUSmllFJ9hAY+SqluEy64yUmNZ9HMib7l3jE+Oanx3VFVpZRSSvVymtxAKdVtvMFNYAIDb4KDaWOGMHpOAQerG3zBkCY2UB0xaNAgk5OT093VUEop1cnWrVt32BiTFmyZBj5KqW7TWnDjcgm5aQnkpiV0c01Vb5eTk4OOE1VKqb5PRIpDLdPARynVrTS4UUoppVRX0DE+SimllFJKqT5PW3yUUp0q1ASlSimllFJdSQMfpVSn8U5QGpi8YNqYIRr8KKWUUqpLaVc3pVSn2V1e6wt6wJqjZ96L69ldXtvNNVNKKaVUf6MtPkqpThNqgtIjtcd8y7X7m1JKKaW6ggY+SqkTEmoMj8djiIuOYM7UEXgMLFtXQmllA9mpsew72sANT32g3d+UUkop+uZ42J54TBr4KKU6rKnJwztF5RQWH8Fj4LUN+7hr2mlceFo6b2wt8xvbM2dKHi8U7mHBZeOYvbSwRfe30XMKNKW1UkqpfqcvjoftqcekY3yUUh3i8Rhe31TK7KWFLF61kyfXFnFNfhYLV25lc2lli7E9i1fvYPG1ZxAVIUG7vx2sbuiOw1BKKaW6VV8cD9tTj0kDH6VUUB6PoehQDe9+epiiQzV4PMZv+e7yWu5atrFFcDN9fCallcHH9tQfbyY9yY07yv+jxx3lYnCiu0P1UEoppXqzUONhe/MNwZ56TNrVTSnVQluaqEN9qEW4ICM5FneUy2+5N7jJSY1n0cyJLbadkxrfoXqo/ktEIoBCYJ8xZnp310cppTrCe0Mw2Hdmb9VTj0lbfJRSLbSliTpUy01+dgpjMpJYNHOib7kzuHG5hGljhrBiTgHPzz6bFXMKQgYyPbWpXPUYc4Gt3V0JpZQ6Ed4bgsG+M3urnnpM2uKjlGohXBO1NwFBsJabhVeO55zcVCIjXUwbM4TRcwo4WN3ga+nxBjcul5CbltBqMoO21EP1TyIyDPgq8BNgXjdXRymlOsx7QzDUd2Zv1FOPSQMfpVQLbWmibu1DzRvc5KTGs7u8lvd3lbc7nWVPbSpXPcKvgDuBxO6uiFJKnai23hDsTXriMWlXN6UU4J9EwCW0qYna+6E2OXcQuWkJLQIa7xidixev5bon3ufixWtZuflAmxMU9NSmctW9RGQ6cNAYs66VcrNFpFBECg8dOtRFtVNKKdVTiTG9I0NSfn6+KSws7O5qKNUnBUsi8Oj1Z3BKagKHajreRF10qIaLF69t0WKzoh1z9ngnQGtvU7mIrDPG5LerwqpXEJGfAjcCTYAbSAL+Yoy5IdQ6+h2ilFL9Q7jvf23xUUoFTSJw63MfATA40U1ZVQO7y2uDttSESzd9MtJZttaqpPofY8zdxphhxpgc4FpgdbigRymllAId46OUInSAsvVAFT98aUPIVNKtpZvWMTpKKaWU6im0xUcpFTI19fay6rCppFtLN61jdFRnM8b8S+fwUUop1Rba4qOUCpqa+qErxvHwP7b5lQtMJd1auumems5SKaWUUv2PBj5KqaABikugoq7Rr1xgN7W2pr3uaekslVJKKdX/aFc3pfqIcEkG2iIwiUBWSuvd1LQrm1JKKaV6C23xUaoPaC3JQEe0pZvaiXZl86aqLqtqaPfkpkoppZRS7aGBj1J9QKgkA6PbMV9OMG3pptbRrmydEawppZRSSoWigY9SfUBZVQMD46KZMWkYYscMy9aV+CUi6Gk6K1hTSimllApGAx+l+oCMZDc3fT6bR1bt8LWezJ2ax5Ckts2X0x1dzlrLCKeUUkopdTJp4KNUH9DswRf0gBVAPLJqBxeePqRF2cAgJ2tgHG9sLTuhLmcdCZx0clOllFJKdaVOzeomIsNF5C0R2SIim0VkbsDy20XEiMigzqyHUn3dwergrSeHahr8XvOOq7l48Vque+J9Ll68ltc3lbJw5dawE5WGE2ybKzcfaDWrnGaEU0oppVRX6ux01k3A7caY04HJwPdE5HSwgiLgQmBPJ9dBqT7P23riFKz1JNi4mruWbWT6+Ey/ct4uZ20RaqxOa4GTNyPcijkFPD/7bFbMKdDEBkoppZTqNJ0a+BhjSo0xH9qPq4GtgPcX1i+BO4H2TTailLK6lh2u4f2iclZ/UoZAm1pPQo2riQj4JGhPl7NwY3Vam1socO4gDXqUUkop1Vm6bIyPiOQAZwDvi8hlwD5jzAaR0D90RGQ2MBsgKyurC2qpVM/n8RhWbytjR1mNXzKDR68/g9dvK+BQTej5dEKNq8nPTvG93t4uZ6G2OSTJremqlVJKKdVjiDGd3+AiIgnAv4GfACuBt4ALjTGVIrIbyDfGHA63jfz8fFNYWNjpdVWqJ3ImD4iLjuT9osMsenNHi2BjhZ0K2ll+cKKbCBeUVjaQkexmS2k1C1duZfr4TCJc8LnsFM7OSaGksp6D1Q2kJXxWvi2JCkLNxzMqPZGv/nptyDp2NhFZZ4zJ7/QdqV5Bv0OUUqp/CPf93+ktPiISBSwDnjXG/EVExgGnAN7WnmHAhyJyljHmQGfXR6neJlhgcd/00xkYF01p5WfjcLzdy3JS41uUnzs1j6ffLaairpHHb5zEvAtGcdeyjS1aYoKt21orjXeszug5BRysbmBIkpumZsO2A1V8qyCXZetKfPXUdNVKKaWU6i6dndVNgKeArcaYRQDGmI+NMYONMTnGmBygBJikQY9SwQVLHrBg+Rauzh/mV847LidY+UdW7WDGpGE0HPewrvioL+jxLvcmIziRRAW5aaiqiU8AACAASURBVAmclZPKltJqpj/6Nv/vuY94cm0RN07OJiPZ7VdHpZRSSqmu1tlZ3b4A3AhMEZH19r+LO3mfSvUpoZIHnJqWwNypI8hIdvtaZrIGxlFcXhu0fEyk9efuMYRMRhAuUUFbBAucFq+2gi53lIuFV47XdNVKKaWU6had2tXNGPM2EHYUs93qo5QKIVTygP1H6zHA/3xlNKekxjM6PZE3tpbhMSZo+ZxBVsARIQRdLgix0RFkp8ZSXF7vt+xEM7xlpcTy8FUTyEiO0cQGSimllOoWnd3io5Q6QcEm+rx72mhcIixZU8Tc59czc8m7/H3LARau3EpJRR1zpuT5lZ8zJY/9R+twR7kYNyy5xfbmTs3j+y+s55ol73LblDyyU2N9yzqS4c3JHeVi39F6dhysJiU+5mSdFqWUUkqpdumSrG4ng2bkUf2ZN0vb9rJqPt5XRWyUi0ff2tmi1WbWubmIwGsb9jF9fCYiYAws37iPn105gbTEGHJS4/F4DO8UlXOgsp6So/W8VPhZAgJ3lIsXZk+m/nhzi7TYHo9hz5FayqqOUdvYRHZKPKcM8l8eLLFCfHQEQwfGMmVUepe1+GhWN+Wk3yFKKdU/dGtWN6XUifMmDwD4/gvr+VZBbsiJSF8qLOEb5+Twyze3+2Vm+1xOii/o2F1ey+ylhXyrIJdHV+9ssZ36481Mzh3k93qo+YOcWd+8Gd5G3VbAniO1REe6cEdGkJoQTVZK+LTYSimllFKdSQMfpXow53w86UlusgbGsWjmRLYdqAo5Eemr6/fhjnQx+7xcPAZcAtGR/gGHcyxOsO0EG9Ozu7yWjSWVLFlT1CLr22jH3Dwul3Dq4AROHawpq5VSSinVc2jgo1QPFWpi0AtPS+f0jESyU+O5568f+y07JzeVxdeewTVL3gs7cah3LM6ydSXMmZLH4tWfteCEyrxWVtUQNiNcsLl5AgO31iZDVUoppZTqLBr4KNVDBAYJHo8JOqfO67cVcMqgeJo98JuvTSI+JpL0xBhfV7K6xuZWgxNvwoR5L65n6XvFzD4vl6yUOIYku5mckxo0OElPcofMCBeshShU4BZuMtRg50GDJaWUUkqdDBr4KNUDBAYJ2amxzLtgVNAA5pOyKraVVbcIKLJSrFaaUOmvncFJ4FicuOhI0pNiwo7DyUmNZ9ywZOZOzWsxxidYC1GoyVCd3eJaOw9tDZaUUkoppVqjgY9SnaC9rRaBQcK1n8siQoQ5U0fgMbBsnZV1zR3lwnjg9pdDBxTO1pxwwUl7x+K4XMKUUemMSEtgUtZA6hqbyArI6uYUbjLUUIFPR4IlpZRSSqm20MBHqZOsI60WziAhI9lNUmwUP3x5g2/9OVPyeKFwD7PPO5WSo3VBA4ojtcd82zo9I5HXbyvgUE1Di5TUJ8LlEnIGJZAzqPUgJD3JTXZqrC+tNlhptsNNhtqRYEkppZRSqi008FHqJAvXapGTGu9rCRqc6CbCBaWVDcRFR/q6p82YNIwFy7f4rb949Q5+ftUEoiJdJMdGtejKlp0ay76jDdzw1AetBluhWqOcrzvr1tFxNlkD47htSh73vrLJV6cHLx9L1sC4kOu0pZueUkoppVRHaOCj1EkWqtXiSO0xPjlQ3WJyz6ffLSY6UvjxZWP50aubEAmeOe1YUzN7K+pwR7p48PJx3PvKZxnd5l8yhu8++2GLYCv15rN8k5Z6g5tQmeLe2FoWtG4VdY0dGmezp6LOF/R463TvK5uYlDUwZOtNW7vpKaWUUkq1lwY+Sp1koVotoiJcLVqCHlm1g1nn5vLYWzt57K0d/OyqCQjBM6cluaP4XzuQyE6NZdHMiew8WMOxJg+b91cxMC6aGZOG+bqVLVtXwtqdh3lybZEvcAnVGvXC7Mlh69aRcTYd6bbmTbowek4BB6tPbjc9pZRSSvVvru6ugFJ9jbfVwh1l/Xl5Wy1CpZn2BirF5fVsL6vmoRVbmTMlz2/9BZeN5cEVn3V/Ky6vZ96L6znW5OGxt3YSGSHc9Plsnnq7iEdX7+TJtUXc9Pls4qMjfMGNtxtbsDqUVgZ/3Vs3b8DSHt4A0Kkt3dZcLiE3LYHJuYPITUvQoEcppZRSJ4W2+Ch1koVqtdhdXhu0JccY/8ellQ0sfa+YuVPzGJLsZtfhWsprj1FcXu+3H2dgkhgTyYLXt/q1+jQcb2ZAbJSv7MHqhpCtUUOTg7/urFt7x9lotzWllFJK9SQa+CjVCbytFs4uXcECAe84GneUi4evGs+h6mMsnDGOeHckgxNjuOn3VrKCW6eMCBmY5Gcnk+COYmBcNDdOzmbx6s/m2Llv+ulkJLupqGv0BWDB6nCw5hiPXn8Gtz73UdC6dSRg0W5rSimllOpJxHhv6fZw+fn5prCwsLuroVQL4ebsCVyWNTCOPRV1HKxuIC3Bypx2oKqBIUluNu+v4vaXPkthff8lY1i8agellQ1kJLuDBjW1DceZMHwg73x6GAMsWVPUIjiafV4uo4ck+ZIT7D5cw18+2ofHgDHwlw9LqKhrZPmt57K/soHC4iNER7iIjhCGDozjtCFJIefq6clEZJ0xJr+766F6Bv0OUUqp/iHc97+2+Ch1AsLN2QP4LctOjWXBZeOIihC/AClnUAKfHqzxBT1gdU174LXNzD4vl8Wrdvq6v80+L5e8wYkAPLHmUwpGDqa0soEXC0uYd8HIoON0zhg+gC+OHOwLXEorG1i8ameLY9lbUeeXGQ6swGnFnIJeF/QopZTqH9o7Ybjq3zTwUeoEhJuzB/Aty0h2c01+FrOXFjIwLpqr84cxcnAip2VYrSnFR2qDZmU7ZVC8r4tbRV0j7sgISo/W0Wxg2tgMFq/ewcNXTaCirpGD1Q1Bu8NlB3wJhBrnExcdqZOHKqWU6jU6MmG46t80q5tSJyAwS1pGsptZ5+ayvayaQ9XHGBgXDcCMScNYvHqHbxzOkjVF3Prnj/jqr9eycvMBUuOi/bKyvbZhHz+afjrGwMNXTeCer4xi0cyJrP7kAMNT4mn2GF+3tyfWfMr908fwYuHeFtnggo3NCZV1Lj0ppkNZ2JRSSqnuEOrm4+7y2m6umeqptMVHqRPgbT0ZaAcuSbFRLFi+xS9BwN8/LmXs0CQeuGQMGQPc3PfqphYf0s/OOptHVu3wax36geMO1pwpeSxcuZUHLh2LiKGx2ePbxsZ9VfBBMbdfOJroCOHxG89k24Fqzj4lhXGZA1rc9QqVdADQLGxKKaV6jY7MF6f6Nw18lDoBOanxPHr9Gewoq6H+eLMveAHrw/f5/+7hli+OaBHELH2vmNLKBl+5A44Pb2/rkHM7i1dbk4nWNjZx0WlDGBAb45fIYOO+Ku58eYNvwlGA52efHbKpP1jWOUCzsCmllOo1QnXd1p4KKhQNfJQ6AS6XkJMSz63PfcS3CnJb3HmaPj6TB17bHDSI8QYo7igXie5I34e3NQdPyztYES7YdqCa04YkMS4zmUevP4ONJZV4DEQIpMRF87s1Rb5tduSDP1RApJRSSvU0Ol+cai8NfJQ6AR6P4ZMD1QyMi2ZUeqIveMlIdjNj0jCyUmJDBjGAI231duZMyWPx6h2+1wPvYI0eksSC5Vs459RUclLjaWwyvlYfd5SLeReM9JXVD36llFJ9nc4Xp9pLAx+lOsjjMXy87yjHmz387KrxFB+u4b7pp7Nkzadck5/F4tU7+FZBbtAgZuTgRBbOGMeQAW5+9Oomisvr2Xf0GLPOzSUpJoKHrhjHPX/92G/Ont/9a6dvItJgAzoX/XM7f7r5LNISY/SDXymlVL+gPRVUe2jgoxTtnwcgWArNOVPyWP3JPhZcNo7ZSwtpOO5hzbaDzL9kDPPt7m7ehAceDC+t28PXzzmF6eMz/VJYl1Y28PtvnMncqXmkJcSwp6KOR1dbQc/CK8eTkxrP+7vKg7YkGYx++CullFJKBaGBj+r3OjIPQLAWF+/YncLiI77XC0YO5rf/3smtXxpBWkIMcTGRlB6t48DRem6bmseh6kaeervIL3h6oXAPuw/XkRQbxa/f2sH08ZlcnT+M/OwUzslNxeUSHdCplFJKKdVOGviofi8wiBkYF80nB6qIihCS3FGkJ8UwbEAceyrqfC1C3hSa3rE83habhJgIao41+yUqKC6v5+dvbPfb561TRhAhLu59ZVOL4Omx6ydx7yub+O75udxx4WhcLsgcEMeYjCQiI63BQTqgUymllFKqfTTwUf2ecx6AjGQ3N07O9qWT9nZNG54Sxx//U0RhcSXuKBdPfT2f7NRY31ge51icN7ccYMFlY7nv1U1A8EQF8dERlNc0Bu2udrTuON87P5ehA+JYX3IUj4GFKz/hrmmnMW3MEAB2Ha4l0R3B0zefRW1jE1kp8ZwyyOqe5/EYdh2upfhILfHRkaQnxZCVomN+lFJKKdW/aeCj+j1nt7Fgc+g8smoHs8/L5Zvnnsq+o1sA2FdRx4+mj+F7z33oV3bB8i0sufFM9lXU8fOrJlBytM6X8GD6+EwiXDBh2AAS3BG8s7M8aFA0LMVNdUOUL+h5bcM+rsnPYuHKrZyekciW0mq/lp65U/NoNoZTBsUH7bY3d2oeeekJTBmVrsGP6hNExA2sAWKwvsdeNsbc3721Ukop1dNp4KP6PWe3sVBz6HgMfHKgihmThgFQcrSBg9XBW2yO1B7nv7uO8NUJmfzqzR2MHJzA7PNOZcHyLb5g5KErxvHWJwd9Kay9r1vZ4er40d8+S4bwgy+P5LkPipk+PpOyqmMtxhZ5A7PcQVZSg3DLNfGB6iOOAVOMMTUiEgW8LSJ/N8a8190VU0r1L+1NjqS6lwY+qt9zzgNwqOYYT64tatEK4xJo9uAby+Mx0NDkCdpiU3S4hssnDWfxqm3cN/10yqoafEEPWMHIPX/9mNnn5bL0vWJmnZuLCLgEhibHcsNT7/uV/eWb27n1SyNobPZQe6wpZGB2sLoBY0IHbgerGzTwUX2CMcYANfbTKPuf6b4aKaX6o44kR1Ldy9XdFVCqu/ndrUmM4RdXT8QdZf1peLuKpcZFs3zjPoyBCIGkmAiSYiJYcNlYv7JzpuTxUmEJh6obODs3jfKaRvIGJwYNRkamJ1JR18hjb+3kybVFjB6SREVd8FakwYkxnDYkiZgol29/Xt7AbHCi29dtL9RypfoKEYkQkfXAQeCfxpj3u7tOSqn+JViG13kvrmd3eW0310yFoi0+qtv0hOZh792ahSu3Mn18JrFRLiYOT+bOi0YRHx1JfEwk7mgXv/vXTuZdMIrMAW7SEmP47+4K7n1lE49cewazz8vFY8AYWPpeMRV1jSTHRfPU21bGtrlTRwRtGRqUEM3f5xRQ5pht+r+7jwRPU53kZtEb26hsOM7PrhzPncs2thjD483oFpjtLXC5Un2BMaYZmCgiA4C/ishYY8wmZxkRmQ3MBsjKyuqGWiql+jJnciSvhuMe7WHRg2ngo7pFT2ke3l1ey8KVW1tkZ5szJY8n1u4A4Or8YXzniyPIHZTAKYPi2XW41peG+rHVO7jurGweWP7ZmJwHLx/HAvs5wFufHOSXMyey9UCVX7KCu5ZtZN4Fo7hk/FDfMacnxTB3ah6PrPLPKrfrUA0b91UBMHSAm9dvK2DPkVrigmRtmzZmCKPCLFeqLzHGHBWRt4BpwKaAZUuAJQD5+fnaFU4pdVLpnHq9jwY+qluEah4ePaegS++SlFU1MH18ZotMbotX7+DWL43AJeIXEC2aORFjjG8On4KRgznuaebxG8/kaN1xRIRjx49TXF4PWOmxp43N4AeOAO++6afzwgd7KC6v565lGxmXmew75qyUePLSE3ytSC6BuKgIfremCLA+UFPiY8hNS+DUwcHPk8slnDo49HKlejsRSQOO20FPLHABsLCbq6WU6md0Tr3eRwMf1SWamjxsLq2ktLKBjOTYkIP0T6R5uCNd59KT3ES4rElLnRORLltXQlpCDPe/ttlv2bYDVYzJTA45h8+jq3fyf1eOC5see8HyLcw6N5eN+6paHLPLJUwZlU7uoATKqho43my479WPKa1s0A9UpT6TAfxJRCKwxqq+aIxZ3s11UkqdgJ7Q/b29nMmRDjq6rff0evdnGvioTtfU5OGVDft83cPcUS5++7VJJ7V5uKNd53JS4zknN5XYqIgW3csGxEczMC66xYSm8y8Zw/xLx/DdZ1rO4TPr3FyO1jX60lSHSo/tDbCCHbPLJeSmWamnPR7DH75xln6gKuVgjNkInNHd9VCqL+nOwKOndH/vCOd3tur5NKub6nSbSyt9QQ9YP/znv7aZn14xzi8j2om0ZnQ0s4rLJQxKiPEFPRnJbmadm0v98WYGxkVx8znZLVps5r+2mfKa4NnXIlyQEh/DC4V7mHVuLqPSE4NmWTPms2POGhhH0aEa3v30MEWHavB4jF/9ctMSmJw7iNy0hB7/BaCUUqr38QYeFy9ey3VPvM/Fi9eycvMBv++jzqTZ0VRX0RYf1elKK1tmPSkur2dAXCQrTlLzcEczq3g8hj0Vdb6gx9m6s2RNEQsuG8vAuGhKKxv8thsTGRG0xWrk4ESOHW/mu18cwXy7m1xgsoL7pp9OelIMr99WQHZKHG9sLeuVd7mUUkr1Xs4WnrjoSBau3Npt4241O5rqKhr4qE6XkRwbNEjwDtI/GR9qoTKrpCW4KTpU06Lp3uMx7Dpcy9bSKiIjBHeUK+h4nPte3cTs83JZvGqn33ZLKup83dmcmeB+smIrFXWNLJ31OZ64KZ/K+uOkxkfx62vPYOuBao41eViy5lMum5hJY5N1J60nJHlQSinVfwTrWjZnSh5L3yv23ejrysBDs6OprqJd3VSnG5ORxIOX+0/0+eDlYxmTkXxStu/xGIyBn181gblTR5CR7Pa1nOwqr2nRdN/U5GHl5gN89ddrufXPH7GnvJa5U/OIcPmPx/F2exuRluC33blT83j63WKWvlfMw1dN4NYpI5h1bq7vC6PhuIeyqkbufeVjDlTWc6DqGJv2V/LnD/bw1NtFXJOfxUuFJcx7cT3FR2pD3uVSSimlOkOwrmWLV+9gxqRhvjJdGXh4s6OdrO7vSoWiLT6q00VGurh8QiZ5gxM4UNnAkGQ3YzKSiYw88bg72F2rh64Yx6SsAQBMe2St3wf7wpVbSUuI5pMDVXyrIJc12w6SHBvNr1bt4J6LT/PdccpIdnPLebmU1zWy/WANEQJ3Xzyao7WNGAMVddYYn50Hq3lybVGLu1RjMpK4/YJRfhONeru4LX5zh++OWnxMpN7lUkop1aVCdS2LsL+Wuzrw0Oxoqqto4KO6RGSkiwnDBzJh+MndbrC7Vvf89WNWzClo8cGekezmmvwsbvz9BzQc95CdGsst541gf2U9FXWNPLHmU+6bfjoLlm/hps9nU3e8mSVrivwyvQE8/W4xs87NJdEdQXZKPAuvHE/RoRpeLCyhoq6Rn14xnmaP8QU93notWL6F2eflUjByMBv3VeGOcpGeGNPuOQB6Y8pPpZRSPUeormVTRw/mnFNTuyXw0Oxoqito4KN6tbKqhqBz8HiDAucH+4xJw3zZ1kRgVHoiP3/jExqbDHdPG20HOp/6srH9v+f801U/smoHP79qAqWVDfzlwxJunJzdYmLSqvrj/GrVNu64cHTQu2keAyKOjG4p8WSlxLf5LldvTvmplFKqZwg18ea4zAH6XaL6NA18VK+Wkezmps9nt5iDByBrYJzfB3uyO6LFpKPewZzHPR7fNh57aycLZ4wLGrhERgjZqbFMH5/p205GspsZk4ZRVtVA3uBEGpsMOw5WB72b5hL4wqmDmHFGpl+A09a7XKFSfmoyBKWUUm2lXctUf9WpgY+IDAeeBtIBAywxxjwiIg8DlwCNwKfAzcaYo51ZF9U3NXvwBSzwWcvM3Kl5pCe5mTZmCJnfnsyqbQcZlZHEd5auazGY89YvjSA5NtovSDlceyx44ILw0xnjqW5o4vtfziMmwkV0VAQLlm/xC6ZWbirloSvGcc9fP/YLyPLSE/hcTkqXp+1WSimlnLRrmepJuqobf2e3+DQBtxtjPhSRRGCdiPwT+CdwtzGmSUQWAncDd3VyXXo8HbvRfgergwcCtY3NvmAgOS6K2KgIPi6pDFo2OzWeKDultXf5M+/tYd4FI1n0z+2+wOWnM8bRbAwlR+pIiY8mJS6KkqMNLPnHthbB1OzzcpmUNYDXbytgz5Fa4qIjSU+KISvlxN5TTfmplFJKqb6kK7vxd2rgY4wpBUrtx9UishXINMa84Sj2HnBVZ9aju7UloNGxGx0TKhAYnZ5IXWMzRYdqKK89xtPvFvtlbXOWLamoIy4qwm+i0Yq6RuKiI1hy45nUNTYTFSF8eqjWLxC6/5IxxEVHBA2mRqYn+oKcUwe3vJvW0SA3VL9sTfmplFI9l97YVCq0ruzG32VjfEQkBzgDeD9g0TeBF7qqHl2trQGNjt34TFu/IDweg0to0aXs/kvG8PAbn1BcXm+11FwxnnGZCZRU1PmytgWOB/r9O7u4+szhPHDJGOJiIik9Wkd1QxOzl67jN9dPYn3JUV5dv8+XGAHgd//eyQ8vHB00mDptSFLYBAWrt5WxsaQSj4EIgXHDkpkyKr3VL0Ltl62UUr2L3thUKryu7MbfJYGPiCQAy4DvG2OqHK//L1Z3uGdDrDcbmA2QlZXVBTU9+doa0PS3sRuhgpu2fkE4yw2Mi2b2ebnkDU4gPcnNHS9voLi8HrDO4d1/3cjjN57Jlv1VNDQ289TX8/loz1GyU+N5aMVWvn5OdoukB/dNP53U+GgevmoCBkNcdPDECAerGpgzJc/v9YeuGEeEyw7Mgnyp7TlSy46ymhapskekJZAzqPX3WvtlK6VU76E3NpUKryu78Z/4DJKtEJEorKDnWWPMXxyvfwOYDnzNGGOCrWuMWWKMyTfG5KelpXV2VTtFuIDGyfumO/XVsRveoOXixWu57on3uXjxWlZuPuALhoJ9Qewur/XbhrNcaWUDi1ft5I6XN1JzrMkX9Hg1HPdwqPoYj6zawYLXtzLrT4W4RNh3tI6KukYyB8T5Ahdv+QXLt/DxvirueHkDZVXHyE6Jb1Fm8eodpCe5WfpeMbPPy+WRaycyd2oeD/9jG9Me+eyYApVVHQuakKGs6thJP9dKKaW6V1t/ByjVX3m78Xt/B3dmN/5ODXxERICngK3GmEWO16cBdwKXGmPqOrMO3a2tAU1XvundLVxw09YviFDlEmIi/c53RrKbOVNH0OwxfKsgl4xkty9oaWo2zJmSR3F5bdBtiVj//3j5FjzGBC3TbAxXnjmMc08dxKJ/bmPhym2UVjaEDNgAahubgm6rrrGpjWdQKaVUb9Gfbmwq1RHebvwr5hTw/OyzWTGnoNO6grapq5uIpAR5udoYc7yVVb8A3Ah8LCLr7dfuARYDMcA/rdiI94wxt7Styr1LWwej99WxGx6PYdfhWoqP1BJvZzYLF9yEa+50do+Li44kOzXWr3UnOzWWw9XH+NH00/nx8i0MjItuMcePd96e0soGGpo8rNl2kB9eNCroPr3tkN7Xg5XZXlbDU28XMSlrQNCWpmBdFbNT4v22lZHs5ur8YRgDRYdqWrzv4cY86YBZpZTq2TQpjVKt66pu/G0d4/MhMByoAAQYABwQkTLg28aYdcFWMsa8bZcPtKIDde2V2hPQ9LWxG8HG68ydmscZwweEDG5CfUFkDYxrsa0HLx/L8x8Uc3ZuGhEu+HxuKjf/8b+MHJzAL2dOpNlj+OHLG1p0T5t1bi5PvV2EO9LFV8ZlsHjVdhZcNpb7Xt3UIkDy1m1fRR33XzKGB17b7Csz/5IxvLxuD4tmTmwRzDiPKVB2ShwPXj6We1/ZFDQ4c45pCjfmCdABs0op1cP11RubSvVGEmJ4jX8hkSeAl40x/7CfXwhcCfwBeMQYc3an1hLIz883hYWFnb0bdRIVHarh4sVrWwQDc6fmMXRALHct2xjyx/7u8lq/L4jd5bVBt/W7G87k0dXbmXXuqRhjaDaw76iVnjot0c13n/2wRb1+eOFIhiS5SYqN4icrtnBNfhYvFO5h+vhMIlwwYdgAfvuvHRQWV/rqmxoXxbFmw6GaY3gMuAQGxUczftgA6o83k5HsZktpdZuCkKJDNdz8xw+YPj6T0UMSucMRnHmPa4U96DXUOVwxpwAg5LK+EjyfCBFZZ4zJ7+56qJ5Bv0OUUqp/CPf939YWn8nGmG97nxhj3hCRnxtjviMiMSellqrHOFndp0J1aattbCZzgJsVIe5+BWv5CrWtI7WNXHtWNk+9/Sk3nZNLfWMTY4Ym0+RpJjYqMmgrzKTsgWwsOcruI3VMH5/pS1rw2Fs7fWWeuDGfmmPHSYqNprz2GJEu4UcvtgxQfjVzIrc8+yHuKBePXn8Gr99WwKGa8Hf0yqoaKC6v57G3dnLrlBFBj6usyuoiF65boDGEXKaBj1JKKaWUv7YGPqUichfwvP38GqBMRCIAT+jVVG9zMucbGJwYfLxOfHQEKfEx7erWF2pb7kgXr6zfy7VnZXOn3XLijnLxwKVjGJxEi1TTc6bkUXy4hswBcew8WI2R4MHDR3uPEhUhPLLK2uacqcEDlPomq7WntLKBW5/7iBVzCpicOyjssQSOYwp2XHHREUHLepd7u9B1VfpHpZRSSqnerq1Z3a4HhgGv2P+y7NcigJmdUzXVHdqaTrotIlwwd2qeX6a6uVPzmDh8QKuDOj0eQ9GhGv67u5wNeyvYf7SOH00/vcW2XAIzP5fDva9s8qvz/X/bjDsykhcK93Drl0awcMY4Hr5qApEuyBwYx8/f+ISUuGhOy0gKmm0nZ1C8X8ppjyFouV2Ha5kxaZhvv21JT+rM4LdsXUmLczRnSh7Hmz0tynqXewfF9qdMgEoppZRSJ6pNLT7GmMPAbSKSaD01NY7FOzulnXdEwQAAIABJREFUZqpbnMyJVEsrG3j63WJmnZuLCBgDT79bzMThA8K2HjU1eXinqJyNJUc5NS2B/1u5leLyerJTY1k0cyJ7jtQxdEAs+4/WkZoYQ9Gh4Omo1xVXMO+Ckew/2sD9jqQEd1w0isYmw+/WFPHdL+YGTWyw/2id3zaXrSsJ2nq09L1irjzTCnza2triHOhaXF7LjrJqbv3SCBqaPBgDLxTuYdrYIS3KBusWqANmlVJKKaXapq3prMcBTwMp9vPDwNeNMZs6sW6qG5zM2XPTk9xU1DX6xs54t5WeFHpbHo/h9U2lfokPvAFGcXk9815cz9JZZ1Fd38SwgXFU1zdRUdsYtM7ZqfEcqm5oMVnow//Yxqxzc3nsrZ386G9byEh28/Q3z+Jg1TG2H6xm6XvFzMwf5rfN0soGXijcw8+vmsAnZdUYA0vfK6airhFj2t/a4h3HlJMaT73dqhYqzWm4bH99LROgUkoppVRnaesYn8eBecaYtwBE5HxgCXBOJ9VLdZOTOd9Ae7bV1ORhc2klpZUNFB2qYeTgBApGDkYEjjU1c9Pns1m4chsD46LZXlbDguVbfNu8e9pofvDlkfzyze1+wdLP3/iEW7+UF3Jy0oxkNzMmDSPCBRFitayMGpLI6CFJREcID181nh0Ha/AYiBAYmZ5Ianw0r72xj+LyetxRLhZeOZ7MAW6unJTpa21pT3IITXOqlFJKdYzOZafaq63prDcYYya09lpn0lSkXSdYOumOfpC0ZVtNTR5e2bDPN07HHeXi/ulj+N2anb4A477pp/Po6p3cfE42NY3NeOzLdtm6EirqGpk7NY+aY82IwKj0RB5asZXSygbmTh3B42uKWrQG3fOV0dQca/brurbwyvFMHJ7MRb9ay8C4aL5xTo5fMPWDL4/kuQ+KmXfBKDIHuEmJjwk62ajOrdMzaDpr5aTfIUr1Lfp9q0IJ9/3f1uQGRSJyn4jk2P/uBYpOXhVVT+LtPjU5dxC5aQkn9AHSlm1tLq1skZzggeWbmT4+0/d8wfIt3HxONgPiY1iypohHV+/kybVF3Dg5m4Fx0dQ2NvPYW9ZrnxyoprTSSjLwYmEJCy4b65cAYN4FIzklLcEX9GQku5l1bi6fHqph39EGRg5OYMakYb6gx1uHX765nenjM7lr2UZfVrrA4zmZySGU6g9EZG5bXlNKKSf9vlUd0daubt8EHgD+Yj9fa7+mVEje1p7y2mNER7ioa2wO2hRdWhk8oYKI//PRGUnMXrrO70Nu8eodzD4vl+gIly/T29PvFvvWq6hrpKL2GL/52iSO1DZyoLKB5Rv2kxIfTcNxD+Mzk7jmrCxf17kla4pYcNlYquqPh6xTuGQPJzM5hFL9xNeBRwJe+0aQ15TqVtqtqmfR71vVEW3N6lYBzOnkuqg+xNsEvXDlVq7Jz/LrUhbYFJ2RHBs0OYGzF6Y7ykVVfVPQD7mslDhEYMGlY4iJclFR1+hbZ86UPP7wjpWEwJvQ4HtfGsHeI3Vkp8Zyy/kjWtwxuu/VTTxxU37IOoVL9nAyk0Mo1ZeJyHVY0yKcIiJ/cyxKBI50T62UCk67VfU8+n2rOiJsVzcReU1E/hbqX1dVUvVM3rl23v30MJ8erGH3Yetx0aEa9hypZeHKrdx+4Wgampr5VkEuGcnuoE3RYzKS+P/snXt8k+XZx3/3k0OTtGl6PtiSYmgLtOVcER1l2qpjvlWUgzgd7vXFdSdsJ3PDOQ5TfN1wTl8Qd0DZpmxTUDwyxnTgBKdMC8j50FJJaS1t6blJ07TJ/f6RPg9J86RN27RN2+v7+fCBJM/z5E5K7/v53dd1/a4n7vBMR1t3WyZ2HauUHj955xQoFUy2l86lJht+9uYJJEZq8ev3z+FXi6dhRW4qls81YdtBM6qabIjUqTEpQY8VuakwRmqxo7gCq+ZPxplLzbJi6ujFRq8UucLcNOw6Vtmj2QP11iEIv/kYwK8BnOn6W/zzIwBfG8ZxEYQXlFYVfNB6S/SH3iI+Tw/JKIgRhdPJUV5vweHyRjz65nFp90tMM2uw2vHEHVlYm5+Bx3edkgwKRFvqqiYbzG7pAsZIHWYZI7Fl2SxUN7cjQqtCu8OBBdOT4OSAwACrvRPRoSqsvDkdz7zv6d627aAZtg4nvmxog7muDWerW/DigSuGBokGDe67LgU/fv0obB1OFOWlosFqR2mXY5vcjlFbhxMNlnZsL5gDq90BnVqBDocT87MSyKWNIAIA59wMwMwYuxfAl5xzGwAwxrRwNcy+MIzDIwgPKK0q+KD1lugPPQofzvmH/lyEMbaTc74oMEMiAoV7PnKcXgOF4KqnGUhushjuN9dZvPrjbNxbIqWTrX7rBArmmbA02yiJnU37XK9v/agMRy42YtPeUmSnGPCDG9Nw5GIjnBx492glHpk/GQ+/dsxLjDyWPxmTE/R4evE0nKtpgcMJ6doalQCtWgmNSvBqNrokO9ljrDuKK1CUlwZbhwPvHq30aky6Jj8DW/afx/oFUzAlqedmq3IIApN2nKqbXSYLNBkThE92wLM1ggPAawCuGZ7hEIQ3lFYVnFAvO6Kv+Gtu0BumAF2HCBBy+cjuEZn+5CY7nRzHKxtRUW9BQlfamjvuhgS2DiecHNi0rwS/WjwNZ6tbsP9sDSYn6PHEHVmobrZh4fREfCU9Dt/7y2FE6tRYkp2MFTemQaVgKMpLg8XuAOCyrK5qskGnUeNoRRP+8p9yLJuT4iFWHr89Ey99XCaJmG0HzSiYZ8KE2DColZ6LVVWTDS9/YsbGu6dhXFQontt3DsvnmqAQgEkJ4fjDR+ex8uaJuN4UPSBxSLngBOEXSs65XXzAObczxtTDOSCC6E4ge9wRBDF8BEr49N4MiBhS5PKR3SMyK3d8jkmFOX7vkjidHPvOVuNYRROSDFqoFAJSorUw17VJx7gbEoj/tnU4UdlohcCA/5lrglLBcLmxHSqBIX96Er7/l8NIjwvDd29IxZlLzTDXW7H5g0rcfY0ROw9VQK1kWJOfgbLaVoSqFWDM5dS27aAZy+eaoNcokByhg93hxPK5E7D1o/MeIuZyiw11FrvXTl2D1Y7oUA2yU6Ix0xiB6mablM721OLpA4rQ+MoF78v3TRBjiFrG2O2c83cAgDG2AMDlYR4TQXgwWGlVweYUF2zjIYhAEyjhQwQZvvKR3SMy/uQmi5Ngo9WOkupWbOlqBirXZPTRr09Cs60ThXmpmJwQjt/+qxQp0VroNSr83z+vRGcKc9PAGfD5xUZE6tRYOtvosYtWmJuGVz8rx73XGiEw5vHao1+fhIduSsez/zyHNw5X4L7rUvBwV+2ORiVg/YIs6DVKODnH7/5VinkT47xS3zQqAU/ckYWrY0IHJUxOueAE0Se+C+AvjLHNABiAiwDuG94hEYQ3gV4vgi07INjGQxCDQaCED/1GBBm+8pHdIzK95Sa7W1Kvuy3Tq6bnsV0n8dTiaThX3YK4MDXCNCo8+fczHql1afFhWPHXI169dx67LROXLe1Ykp0s9dBxf335XBNiw0Kw7t2TUpPRhTOTcdlix+zxUdh49wyoBIbv//WwlxW1WEdUmJsGpeAZIWLMZZYwKUE/aBM55YIThP9wzs8DmMMYC+t63DrMQyKIISHYsgOCbTwEMRj0aGct4kdn7VUBGxEREORsHovy0vDG4Qq/c5PFSTB/ahIut7TLRjHOdTmoxeg1ksOb+NrGvSVo73DKnhcRqoY+RAljlE729atjdIjoajKaaNBg2ZwUbP2oDJv2luKBl4txvqYVp6rkrajFJqOb9pVgfHQYfn5bJhqsdjz/QSlePFCG5EgdJsWH9+t79YW7tbfA4LfFpvt5ZbWtcDopa5QYGzDGvtn190rG2EoABQAK3B4TxKimp+wAGg9BDA7+Rnx67KzNOX8vgGMiAkD3fOTYMJer2wxjhN+5yeIkyBhQ29ouG8XITAzHU4unobVdvrmopb1T9jynk+PJv5/B6lsny75eXm9FSnQoHr89A5GhIZIVtXhd0TShp6iWrcOJxjY7LO2d+P2yWahtaYdGqcCU5HAolX5pfr+QSw/YfM8M/O3BHNS2+s4Fp7QCYowj7gToh3UUBDFMBFt2QLCNhyAGg94amH6DMfYuujpru/35ANRZO+gR85HnmGIwIS4M42Nc/zbFhvl1Yy1OggCwo/giHrop3SOKsfLmdNQ02/CT14+ivN4q21w0XKPCmvwMaFQCEg0aFOalYv2CLHBwpMeFISFCg5/flunVKPS14gr87M3juGyx42x1i6yoqmy0ojA3zevcNw5XSI8rGtuw/m+n8Z1th9De6YRaKcDpBM7XBC7KIpcesOKvR8AYevy+qSEeMZbhnP++6+/H5P4M9/gIYrAJtgacwTYeghgMeov4fAygCkAMXB21RVoAHBusQRHBgTgJbthzGkuzjfjrp646mUkJelQ0WGGKDZXqd+QMBJ68cwoe23UKsWFqbLx7BpraOrD27RMe/XIaLe0waNWS5TXnV3rzAIDTrSbJvdZHIQBJETq89HEZnlkyDWWXLZiUGI71u05KfX1E+27AJSrW7zqForw0nK9tleqVAhFl6a+ZAZkgEGMZxtimnl7nnBcO1VgIYjgItgacwTYeghgMemtgSp21xzDSJJigR72lHV9JjYbV7kBreyc27DmLlTenSzfuVU02yUAgJUqLiQl6cA7ce60RE+P1cDi5JHpE8VLdbMNXJsTgSHkD2jocePFAmWza2huHXaJqe3E5lmYbPcTVuvxM/O7D8zhW2YyNd0/DI/Mn42RVM2aOi8DP3johCSjAJSqSI3VeaXMDLd7sb3oApRUQY5xDXX9/BUAGgO1dj5cAODUsIyKIISbYGnAG23gIItD4W+iwA4D71rTYWZsY5YiTYPb4aEwbF4nrJsTAFBMGjUqA3eH0SG+rarJh60dlSIzQ4nytBXe/cBBPv3cOD756BK32Tkn0/Pf14yWjgm/98VOEhigRExaCorw0r1S6Nw5XSKLqx7dMkkQPcMVZLic9DhqVgPO1FtRb7Xj3aCX0GiUarHaPz6JRCbD6qEUaSPFmf9MDKK2AGMtwzl/inL8EYCqAGzjnz3HOnwOQB2D68I6OIAiCGI34a25AnbUJiatjrqTAdU9vW5OfAYUAL4e3i101QPdea8Sz/zzn8dr/7j6NVV+bCI1SwG/vnYnalnY02zoQFaqWxEuD1Y62DoesaEmJ0mL9gizUtNiwZf95rM3PRLwhBE/eOUUah1j/c9kib9IwkChLf9MDKK2AIAAAkQDCcaVuNKzrOYIgCIIIKP4KH+qsTXiQkajHU4umwdbRiacXu2ps2jud2LyvFPdea/QSKDuKK7A2PwMKgcmKlxi9Bk/uPg21kmFtfiZUCgGx4Wo8f89MHK1ohMMJ1PlwlqtobMOmvaWSuKlsbENoiAIzjREomGeCUhCQGheGDXtOw97JUZSX5lHjs2HR1AFHWfqbHkBpBQSBXwI40mWawwDMA/DzYR0RQRAEMSrxV/hQZ20CgKcFc3pcGL49bwIefOWIxzGm2FCkRGuRPzUJIUoBV8eEoqrRZYZg73TKihcG4Ee3pCM5QgMnB6x2Bs4ZHt91Ag/fMsllfADgoZvSpYiRRiVgbX4GXv20HMAVm+unF09DnF4DY1QoJiWEY+WOzxGpU+Nnt07GmeoWAMCKG1Nh63SCcyApQkNRFoIYJjjnf2SM/R3AtV1PreKcXxrOMREEQRCjE7+ED3XWHh04nRwX6iyobrYhPrxvaVWdnU6crGpCVZMNZy8147qro3BLZiJKa1okkcO6LqUQGL47LxWP7TrpkWa2aucxrL0twyviUpSXBpVSwLmaFsTrNVj99nGY69pQmJcKc10bznY1SRVrhJbPNUEhAKlxevz6vTNYmm1EbasdVU1dfYcEwBipw4U6CyJ1KmwvuA4dDgdUCgUedjM2AFyia9HMpD59h19ctsBcb0GoWon48BAYoyg9jSD6C2OMAbgJgIlz/jhjzMgYm805/3S4x0YQBEGMLnoUPoyxb3LO/9y9izbrusPlnD8ziGMjAoTTyVFeb8Hh8kaPmhd/bJxFwXOhzoqy2lbsKK5Ag9WO9QuysPmDEhg0Ki+Rs/Vb2Sh61bM/zaZ9JVg+14TH3z2FH96UjoJ5Jjg5IDAgIVyDJ3efgrmuTRJJ2w6a4eQuYeJulS0aKBTmpuEXu0+jqskmXfv5D1zpbgl6Df52ogqrdh7z+Ky3TI7HM3dN92oY6m+am1zD0aK8NKTFhyF3Yrz0PQ5EYBLEGOQ3cJnn5AJ4HK52CTsBXOPrBMbYOAAvA4gHwAFs4Zx3b7JNEARBEB70FvGhztojHPFm/cylZmzZX+YhRnqzce7sdOKto5VY/dYJj8jNtoNmrHn7RJd1tQYJBg1+uXAq4sPVKK224MtG+f40jAHmujYoBYbrTdH4uKxOitqY69qk40Qh4y54th00o2CeCRNiw3Cx3urR60e8tihEPimrw+YPSr0+60v3z8bEeD32FOXgUnPfzQTkGo5u3FuCgnkmmGJcdTpy4migfYIIYpRzLed8JmPsCABwzhv8MM/pBPAjzvlhxpgewCHG2Pucc7LBJgiCIHzSWx8fqbP20AyHCDTizfoDOaYem2W6Ryni9BooBKCmuV0SPeLx7tGVq6M1cHKGgm2HEKlT477rUrBxbwkeyDHJ1vHwrghOSU0rrjJoERaiRGlNiyR63MfFmMsee3txOZ5aPA3nqluQHqdHhE6JVTtLva6dnRIJgZnw8idmLJqVLPtZD5RexosHymSFiD9RGl8NR50c0vcoJ44G2ieIIEY5HYwxBVyRGzDGYuHZPsELznkVXM21wTlvYYydBpAE6v9DEARB9EBvqW7UWXuEU2dpx/K5JkyM1/u0ce7sdHqlhhXlpSFWHyJ7ox+iFKBRCRgXFYZv/fFTROrU+Omtk/GTrvoZ90iNe6Roe3E5HropHX/6+AIyEsMRohBwlcy4UqK1SI/XozAvFZMTwvHbf5XiXE0rls81waBR4NGvT8Jlix1ODigYEB2qhr3TiU17Sz0+m5zwkhMi/kZpfDUcFRgkO2xf4kgURgRBeLEJwJsA4hhj/wtgMYDV/p7MGBsPYAaA/wzG4IjBhVKDCYIYSnpLdaPO2n3Anwl8KCd5p5Pjy0ZXTUykTu1lKvDMXdNhjNTh47I6SfQAV1K4tiybJStKZoyLwJN3ToGtw4H0uDDMz3KZHIjHiQ1Hl881YXy0DmEhSggCw/dvSEW4VgW1kiFcq8IPt7vc1txFUkq0Fj+4MU0SUaII+1Z4CGpb2qFRK6BVKbHl72ek19cvyMJVhiuixJfw2nbQLH0+dyHiK0oz8cEcTIi7IlbEhqNyNT5inZAvcTSQPkEEMVphjAkAvgDwE7galzIAd3DOT/t5fhhc9UA/5Jw3y7xeAKAAAIxGY6CGTQQISg0mCGKoYZzz3g9i7CCAuZzzzq7HKgAHOOdzBnl8EtnZ2by4uHio3q7P+DOBD/UkX1bbils3HZBuwhMNGizJTsbkhHCkx+uREqXDyaomnLnUglU7jyPRoMHCmcmSO5sxSguFIOBnXYYIKdFaDyOD7BQDfnTLJJTXW5Fo0GDT3nMoNjdJ769RCXj4lnTYO7mHCHnijilQMCce2nFcGpf4vrPHR6Jg2yEv4fCTr03E47tOozAv1aNWSXxdjFCJqXkp0VqsXzAFDqcTRy424rXiCqkmSKMSsNst4vPJ+cv4xgvem8Wb75mBW7MSvVLivrhsQXm9BToZVzdayIMHxtghznn2cI+D6BnG2BHO+Yx+nKcCsAvAP/wx2gn2NWQs0n2NArznZ4IgiL7S0/rvbx8f6qzdC/7Udgx1/Uf3tKuqJhs27S3FK9++FlfHhGLf2Wocq2hCWpweKdFaLM02dhMoWYgOVeFXi6fBau9EdKgaxyqb8ECOCXF6NVQKBe7/02fS8Y/dngmgHMXmJkmMjIsKRdGrRzw+8+q3jmPzPTOREq2Fua4NVU02yZFtXESmbKpYuEaFRIOrx4/c6xa7A6/uK8H2gjlo63BIxgUA0NbhRIPVDgCyTm6+ojTnqluQkRju8bMRBIYJcWEekSB3BIFhfmYCJhXmoKal7wYKBDEG2csYWwTgDe7PThwkC+ytAE6Tu+jIhVKDibEApXMGF/4KH+qs3Qu9TeCdnU5cbmnHY7dlQheixAv7z+NYZfOgTvK+bugTDRqcqGxEo7UDDMAL+8/jkfmT8VA3Ubb6rRMomGfCpr2lrhS0G9KkaMvmb8zw6Ilj63Bi3Tsn8ftls3DI3IDUOD1+sfs0fnhTmuz3cqyiEWvzM/GDvx6WhNNDN6WjwWqXHXN5gxULZyZLj+Xqd8x1bWjrcGCOKcbj/XoTIsZIHZ64YwpWv3XF6ltMjbt+QnSffzaCwGCKDaOFmyD84zsAVgJwMMZsXc9xznl4D+d8BcAyAMcZY593Pfco53z3II6TCDCUGkyMdigLJPjwt4EpddbuhZ4mcDlb6HX5mcCnZpyraR20SV6uJmXzPTNwqqrF47nC3DSY66w+HcsA4O5rjKhotOKBHJPrtU6H7PHVTTZolAr8YvdpNFjtMEbpZL8XhxOw2h0e/Xy0KgFRYWo8eecUj35DoghZNCsZOw9VeNUqia/7WjB7EiJOJ8d7p6tR32qTxsI5sO2gGQ1WOy3ABDHIcM773C6Bc/4RXJtwxAhGbo3qS281ggh2yOk1+PBL+FBn7d7paQI/XtnoZQv92K6TeHrxNAgC69ck70/oVC7tinPgv5474DGWTftK8KvF03y6qz3y9YmI0qmx9p0rTUqfv2emrKBJMGjw7D9L0GC1Y01+Bs7XtOCJO7K8egFtLy6HQkjycmJbcWMqXjt0Ec/eNR0lNa1o73RKIoRzSGJqe8EcVDS24eylFun1/iyY4qQUqVNj2ZwUj1Q/WoAJYmhgjC0EMBcuS+sDnPO3hnlIxBBAqcHEaIfSOYMPf1Pd+txZe6zR0wRe1ST/H18Q0K9wZ19Cp92jHZ+cvywf2XE6sSY/A+t3nbpiZPDVVPzk9aNYPteE//vnSQ+x9Piuk16C5ue3ZSJUrcAjX58EhcCw5cPzOFfTirX5k/Hc3TNw/MsmOJzA9uJyFMybgM37Sr3GYet0wlzXhod2fI6CeSap9mf9giwkGkKwaGaO9L1OSYrA5IRwXD8hut8LpjgpuTvRMQbkpMbgmvFRtAATxCDDGPsNgFQAr3Q99V3G2M2c8x8M47CIIca/6i6CGFlQOmfw4a/w6U9n7TGHr5SqRINW9j9+UoS2XzfW7lEK0Q3t7KVmZCTqMT6m5x0EX7+E5y9b8FpxRZchgQ4GrQrffrlYaibaXSyZ69qgUynw4n3ZqLfYEaMPQUWdBfdu/dQjslN70IzHd53GihtTkRanR0lNC358yyRUNFolwwH3cYiLn63DiSSDVurlowsRcP2EWNnvayALpvv34W6ysHBGEokeghgacgFMFo0NGGMvATg5vEMihgKqfyBGO5TOGXz4K3z63FmbuEJmYrhXdOSJO7KQmWjo1/Wqm22yqVkp0aFIjtChvMGK6mZX1EkhuNzcxHQ4X71oXv7E1eOGc+Dh147igRyTlzjq/rjssgWxYSGobGqDwBjWvnvKK4Vu+VxX1MbucKKkpgWhaiUuNljBGDwiTN177WhUApKjtBgfEypZRgMu+9PqZhsSDRqvWqX+LJg0KRHEsFMKwAjA3PV4XNdzxChnJNQ/kCMXMRAonTP48Ff4DKiz9mijrxOhUingjmlJSIsLw6UmGxIMGmQmGqBUCv16//hwVz8eUfQArgXj0TePIyYsBAXbir1ETYPVjg2LpuKqCA2ykvT43Tdn4XB5A9Li9Hhh/3ksnJkMY6QWXza1IVLnCuZpVAIidWqEqhV4/PZMjxofsU5nXX4mLja2oeyyRTaFjjHXdWaMi0SsXo0f/PUwzHVtAFz9e4ry0pAUoYVKIeCXe06jqskmiY/rTDE+++N07+fT3wWTJiWCGHb0AE4zxj6Fa3NtNoBixtg7AMA5v304B0cMHsFe/0ARKSIQkNNrcNGr8BloZ+3RRn8nQqVSwLRxkZg2buBjMEbqMDkhXHbBKDbXe4iBjXuvRF1W7TyGVV+bCFucHofLG6BTKxCqFnDf9eNxsd6K/9vrMiUozE3DnhNV+On8SbB2OLBxbwkidWoUzDPBGKXDpSYbtheX4/s3pKK0pgXhIQpcFaFDYV4qnBzYeahCEjACA9bmZyBKp0JzWyd+uXAqjlc0orndgXePViJWH4KXP/kCeZMT8ONbXLVBaXGuCcL9++y+M+irn09/FkyalAhiWFk73AMghodgr38YCRGpoYIiX8RooVfhwzl3Msae7+qsfWYIxhTUDPdEKNovV9RbJKEBuMRGg9UOR7cERDHqAsAVvQlR4tvbihGpU+O+61Lw/b8e8Uo1E1PUWto7sfmDUqn+ZdNeV/3LlmWzkBIdisoGK/5TVoevZSVKPX3co0ErbkxDh8OBTocTS1846PH6u0crsfLmifh6RgJmGiN7jbbI7Qz2tGDSJE0QIwPO+Yc9vc4Y+4Rzft1QjYcYOoYy1bg/a0KwR6SGCop8EaMJf1Pd+txZe7Qy3BPhhToLNuw5jXtmp0ipXmJKW3x4CDbuLfE43t0wYEl2spSutnBmstQLR/wM7jU5jAF2h9NHVKlBEkFr8jPw2w9Lva7zwrJsnK5qwpRkAz6/2CT1/9l/tga2Tge+/9VUlNW24svmNr+iLd13BuX6+YgLJk3SBDGqCI7tfyLgDFWqcX/XhGCPSA0Vw73hSxCBxN8ik+8AeA2AnTHW0vWnubeTGGPjGGMfMMZOMcZOMsaKup6PYoy9zxgr6fo7cgCfYUgRJ0J3hnIirLO040e3TMKz/zznldJWb7GjKC9dGp8oiN44XAGNSsC4SJ10jpx+MRpbAAAgAElEQVRTm3tNzqR4PdLi9NCoBCQaNPjBjalYkZuKorxUaFUK6fj1u04hf2qS13U+M9fjjx+b8fnFJmzcW4LN+0rx4oEyfGN2Ct7+vBKr3jiO3+8vw+HyRjid3lra6eQoq23FJ+cvo6y2FckGLTYsmip9tgarHWnxYfjbgzl4teBa7C7MkRYxX5P0hTpLAH8SBEEMEWN6s220I6YazzHFeKU4B4r+rgliRMp9TR2L5jc9bfgSxEjDr4hPfzprd9EJ4Eec88OMMT2AQ4yx9wH8N4C9nPNfMsYeAfAIgFX9fI8hZShC875C8k4nx5eNNpyvbZWdhGwdToyL0uLpxdNgsXfC0t6JzKRwZF0VDqVCgK3D4bF7JbeTNSlej6cWTUWEToXQEAFPLZ6KyoY2j8jKQzelI9GgkfoTKbrJZ41KgMMJ2ajSY7tOSlEl0ZBh+rgIj10jud25J+7IwqufuvrsKAQgOyUK15uioVQKmBDnueM03FE5giAIInjo75pA5jcuKPJFjCb8TXXrV2dtznkVgKquf7cwxk4DSAKwAMANXYe9BOBfGCHCZzAnQqeTo7zegsPljXj0zeNeIfkLdRas2nkMD+SYZCehKckGLHPro/PQTekwX7agztIhGRSI6WFyqWKP356FX713Bua6NimNrb3L3MBdvDz7z3OSeNGoBExLjpDG414rtGhWss+okvvj7ouP3O7c6rdOSO8pft7dPsLsNEkTxMiBMfYggD9zzht8HTKU4yFGHwNZE8j8hto+EKMLv4RPIDprM8bGA5gB4D8A4rtEEQBcAhDv73UCxUCK3wWBSb/w1c2uUO9AxY8Y5ThzqdmnTbO4a7XzUAUKc9M8evgU5aXhbFWzl0B5evE0rH3nlEez06cXT4Ot04HkSC1+/82ZaGl3IFyjxJq3T0hW02Ia22O3ZfaYErd+QRYaLDY8c9d0lNa04uqYUDz93hlUNbm+F7nFxr1KTG7x8bU715tgEqFJmiBGFPEAPmOMHQbwBwD/6FZLumx4hkWMFmhNGBgU+SJGE/5GfAbUWZsxFgZgJ4Afcs6bmdsdLOecM8Zkc7gZYwUACgDAaDT6+3a9MtDi98EonhejHN0bhwJXbvLFXauqJhu2HbyS9pUap8cvdp/GolnJXudZ2jtlm50+dnsmVALD+UYb1u86hQdyTJLocT8/Vh8iK14mxuuxfK4JDZZ2RIaG4A8fnUexuQmJBo30XnJRpXX5mfjd/itRG7nFx9fuXG+CSaSnSZrc3ggiuOCcr2aMrQFwC4D7AWxmjO0AsJVzfp5zfmJ4R0iMdOjGfeAMZ+SL1m0ikPhrbiB21hbxu7M2Y0wFl+j5C+f8ja6nqxljiV2vJwKokTuXc76Fc57NOc+OjY31c6i9M9Di98EonnePcvgyT3AvtKxqsmHrR2XQKBX4xe7TaLDa0X0e0KgEhGqUss1O171zEha7A+t3nerxfUtrWlCYm+ZR3FmYm4YX9p+HQgAMWjUqGqwo7DJVqOrq8fPbb87CutsyoGDAs3dNR2FeKpbPNeGVT83In5qEwrxUbC+YIysW5QpKn7gjC7uOVUqPn7lrOgQGyfygu0GCXMGsKFhv3XQA33jhP7h10wHsOXlJ1lyBIIiho2tT7VLXn04AkQBeZ4w9NawDI/qNaFDz2YU6HL3Y4HOuHiqGwkSBCDy0bhOBxt+IT786azNXaGcrgNOc82fcXnoHwLcA/LLr77f7N/z+MdDi957OHx8d2q+dCTHKIZfGJkZFxF2riQ/m4PSlZpyrbsG2g2Y0WO349ZLpiApVAXA193z3aCV+dPNERIUqMSE2THa89ZYOD3to8X0jdWosyU6GMUqH6mYb/nHikhRdSo/TY8v+8/j6lESPSM76BVlYf3smzA1t4BzQhyiQMyEGfztRhdOXmrFp7xWdfKzSZQh4/YRo2e9GbnfOGKmT+v3EhmnwRV0r5m880KeIG1lyEkTw0eX2eR+AywBeBPBjznlHV/PsEriaZxMjCPFmdcOe01iabfRaz6i1AOEvtG4TgcZf4dPfztpfgSs/+zhj7POu5x6FS/DsYIwtB2AGcFc/r98vBlr87uv8hHBNv1Pg3HOQtx00o2CeCenxekxOCMfVMa5UsLLaVklQ3TIpHuMitchIDEeiQYvGtnbc/6fPpPfdsGgq5mck4MPSWsSHy6erqZVMel5Mn1t5UxqiwkKw+q0TXmYFVU02PH/PDBTMmyA1LAVcE9Gat094GB7kTozFyaomxIaFIDlCi7c/r/RIpevt+5YLq4uPy2pbsaKr8ar4/v5MhOT2RhBBSRSAhZxzs/uTXc2z84dpTMQAEG9Wl881eWUb0E0r0Rdo3SYCjb921v3qrM05/wi+HXny/HnvwWCghY6+znc40e+did7qUtwFVUq0Fg/mpnmIk6K8NETq1JLF9Kqdx5ASpUNNSzue+NspWTOE3/3rPB66KR1/7Uo/UwhAVpIB//NSsVdD0uVzTdj6URnUCgFNtg7ZiUg0PHj065NQbG6Qeg2JqWrP7SuBua4NKdFarF8wpd/GEP2dCMntjSCCD875uh5eOz2UYyECgzhH++oXRzethL/Quk0EGr/trHthRP0PHGiho6/z//NFXUAmed4tdbV7qDd/apIkesT32Li3xMPu2dbhREVDm1TD426GMC3ZgNb2TjTZOqBVCXjwxjSsfrvrenmpsp9BIQBr8zPwxO5TuCt7nOxEdJ0pCgIDmm2d2NzVp0c8f/VbJ7C9YA445/jishUF24r7nfrQ34mQnH0IgiACQ08F5+6Nvvt70+pPQTsVvY9+aN0mAk2ghM+IqzLrzaGktwlV7nxfN+SxYf5N8r7S5MTds0SDBgtnJsMYqZUVJ8YoLVbkpmLnoQo0WO2wtHdKx1U12SRR9NSiKSi+cBnrbsuEtd3hkbbm5PIL1ayUSJyuaoa5rg1/PljuFUHasGgq1EoBm/aWYkWuvHjqcDhhtTvx064eReLzfU196O9ESM4+BEEQA6c3Z1Nxjt6w57TPmtWBXN/fY+SuO9hCicRYYKF1mwg0gRI+o4r+2lXL3ZAX5aXhizpXf5v+Ft7Hh2uQEq2VikR9NTAtr2/D1o/KUJSXhrjwEISqldCoBETq1PjeV01IiQlFo6UDiREaKBUM3//LYS/7bF89gk592QyL3eHTTvsqQwiiQkO8dvlEsaYQAICh2FwvK4rOVbdI32FvE9pAJkJqRkcQBDEweis4l+boBD3qLe3YXjAHVrvDbyHgT0F7X4veB6MNxXC8x0jGlyjsz0YzQfQXfxuYjqnO2v11EREEhoxEPQrmmeDkrpS1lz9xua7t7uXcOks7ls81SU06dx6qQFWTDTUtNsweH431C6ZI6WFy4uThWyaiw+HEAzkmMABJBi1OVzVjy7JZaLS2w2Ln+M62Q141QYBnhEe0o3568TScqW6BwIBQtQK//bAMaiXD4wuysPbtE5KddmFuGl7+uAxr8jNR3WzDC8uysXHvWRTmpmHfmUu4c+Y4D8tsXxGl45XN+OH2z/1eJGgiJAiCGB78qbMcyBztz/X7Wus5FO5g5EDmG1+i8KaJcTh4oR7F5nrJkXbV/MkkFolBw9+Iz5jqrD0QF5GqJpuHdbNIT+c6nRxfNrqEhLuT2vbicsTpNRAEBqXAPMSJGHExRmlRb7EjJkyNR944Lp2/8uZ0/PHfF9BgteM3987ET3Ye9piMX/2sHD+9dTIuXLbg2bum45d7TsNc1waNSsA9s1NwscEKAJicEI6L9Rb86OZ01La2A9yJZ+6ajjOXmuFwAvvOXMLSa1KwdMtBj7Q3U4wOGVeF47t/PuQhet49Wukl2tbkZ2DzvlJaJAiCIEYAg11w7k/aeF/HMBTuYORA5htfovB335wl3SeI9z4b9pzGpAT9mP/OiMHBrwamnPPVANLg6snz3wBKGGNPMsYmdL0+qjpruxdmivg7qbufm2jQ4Ac3pqIwLxValdJnw60LdRas2nnMy0lt/YIpMEbqUFbbCjDuMSYx4lJe3waHk+ORN44jUqfGD25MxQM5JljaO/HD3AlYPteE2pZ2j8k40aDB0mwjfvL6UTzz/jk8tONzFMybgBfum4UVN6biTx9fwIY9Z/HigTJwAH/82IzVb59Aa7sDHAy/+1cpHE6AMeC+601Y87an0cKqncfQ3slR3WzDAzkmJBpc39vOQxVYmm3E9uJyLJ9rQmFeKp65azq2f1qOqiabdH5Ni83Pn5QnYsO84W6URxAEMZqRazL9zF3TpfVqoHOw3PXFtHHxmr7G4Kt+aCDrur8MxXuMVHyJwsPlDV73PvlTk/p9H0AQveF3jQ/nnDPG5Dprv885H1UN5gbiIuJe1OneuG3L/jKfaVy+JgStWsB7p6uxcsfniNSpUZSX5tE0VOyv88OutLVlc1Kk90uJ1uIHN6Ri60dleDA31WNnbOHMZK/eCut3ncJL98+G3eHEolnJePdoJZZmG7Fhz2ksnJmM5z8ohUIAKhvbMD8rUerrU+jDBW5/SS027S316gO0vbgcm+6egbYOB7QqBQpfPQJzXZtHLZAoEvsS5qbcaoIgiKHBV5Npcb0a6BzsT9p4X2s9h8IdjBzIfOMrQufwvH2QXGRJLBKDhb81PmOqs/ZAi+fnZyYgKUIjpX8BPef6xunlJwQGJk2gVU02vPyJq7Hp5IRwnL7ULIkJXYgSS7I9xcySWeOw9p2TsHU4ITDmIZoUgnxvhX+fvyyJlTX5Gdj+aTnMdW1Sf57MRAN+/u5JNFjtUl+fa1KiepzMxB0c8fiVN0/ElKQIqaBx1fzJfRKJvqDcaoIgiKGjew1PWW3rgObg7gXu1c29p433pY5oKNzByIHMN3KicMOiqXjm/bMex2lUArJTokgsEoOGvxGfMddZeyCFmYLAYLU7/M71VQjwiuasuy0TB8s8+wKJ9UMP35IOjVKBBqsdANBgaYcxSicdm2jQID5cIz222B3YeahCMk9Ii9P3KlbW7zqF5XNNOFfTCoEB627LRHVzm5SSNjUpHLsLc2CM1HlNZmKEx/1zp0RpsWVZNq43RQNwLZLVzTZMjNfj+XtmYvHvPhmQaKHcaoIgiOFjIHOwXMT+hWXZAa8jGgpTHDLekcdXlFClELzE0PWmaBKLxKDhl/AZjZ21B9trvy+Fl2I0RxQm6fF6/Pq9M7htWpLsNdo6nHjjcAWeWjwNpTUtiDdooWBASrQW+VOTYIzUQqtWeJzbYLVLfXwSDRqfaXMiYmRoTX4Gmts6sGlvCRbNSpbGkBZ/pfDQfTIT09dEgSQePzFeD4NOhc/M9eh0cKx++7hkpvDknVMQqVN7nNNX0ULdnQmCIIYPX3NwQrhG2ujytdbKRexXv30cGxZNlepfByNtrKf7AOrHE3jkRCFFyIihZkz28RmKepC+5PrGh2s8hMmK3FSY69pkbavd62XOVbdg8z5Xw9CTFY34/g2pWNeV3pYSrcW6/Ew8tuskdh6q8BA6DVY7EgwabP1WNioa2mCM0uGRN455iZVJ8Xqs/9tpVDXZoFEJ4Fy+gNR9MhPT17p/7lpLO5a+cFD2czz65nEUzDN5pDX0VbRQbjVBEMTwITcHb75nBk5VtfS61spFi8x1bUiK0GD3IN0U+7oPyEjUo7a1HV822rxEF9WMBh6KkBFDDfN0pQ5esrOzeXFxcUCuVVbbils3HfDameqt105fEXeMepu0u0/ARXmp+P1+l7W1e9H/rJRIrH37hGQG8Oitk/Hj14/igRwTBAZs6TpHJCVaix/dMgnnqlsQHqKAMSoUre2duNRsw2uHLuLhWybhx68fRVFeGjiHV9NSsX9Pg9WODYumIilCg6jQkF4Xn+6fW2DA/I3e3/fyuSZJ7G2+ZwYefu3ogBYZf79vYmzAGDvEOc8e7nEQwUEg1xBCnu5zMOfAfz3X+1o7VGuyO77es2CeCQ4npPYSQzWegUDRKYLwpKf1f0xGfIaqHsTfnYzuua8J4RpMTAjHyh2fS7bVRXlp+PU/zuLua4x49bNyLM024un3zkj9flbcmOb1meydHAyAwIAJcXpUNljx2w/LpMhOm70T6/Iz8bv9pbB3chTMM8EYqcOlZpvkoPPS/bMRq+9d7PT0uT85f1n2+xabtWpUAiYnhA94Z492jgiCIIYPf+f+7mvtcETsfd0HOLmrVUOw1ox2FzmBdNMjiLHAmBQ+wVgP0n3BMEaFYlJhDsx1Fpy51IIpyeHIvCocjdYO/HLhVDzyxjGY69qw7aAZ912XggSDBoV5qXByV78cALjvuhT8+PWjHullIhqVgMQILcyXW/GLhVPRaLXj5Jct+PX75zxS3jj4gCd6X9+3e+rc1TGh0ndAEARBjHz8XWt9Fb4PRx0u7xI+wXaPAMin521Zlk2OpgTRB8ak8Bnq3aW+hqHdj3cJAqCywSbV73Q3I+Ac+PbLxR6vMQappgfwtpV+6KZ0rH7rOL771VQ8894ZrMnPwoY9Z3DvtUbEhoVAF6JEVaMVCeEDn+h92VgmRWiwaGYSheUJgiBGIX1Za7vXig5HHW5RXhpe/sS1rnavrw2GmlE5E4hic33QRqcIIhgZk8JnKL32+zqByx3/x/uvkUQP4CliAHg1I920rwT/t3S67GSYHh+Gl+6fjQarHau+Ngl/+HcZ/mfuBEyO1+PB3DSsfuuExyJQWtsKY9TAvpv+ft+Ut0wQBDFy6e/cPxR92bqPLTZMgy/qWtFgtcPW4cT24nJsWZYNlYIFzfojl57n5MEZnSKIYGVMCh9g6OpB+jqByx1f2dAmK2ImJYQhVK0Ey3EJoJ2HKlDV5JoYw0KUspOhTqVAnD4EsfoQ1LTY8NRi1y7WhTqLJHrE62/cW4KCeSaYYgb+PfX1+x6KHT+CIAjCP/q7EdWftXa46nCvjgkdNBe5QCCXnvfu0cpBt/0miNHEqBQ+wRQp6OsELnd8nD7Ea7JLidai2daJH79+ZbIT098arHY4nE6smj8JG/ac8WiKqlAA4yJ1UCoFj/fvqdBzKELm3X9mAgPlLRMEQQQBQ70RNVx1uMFukCOXnrdq/mTcMjkeU5IMQSvYBkow3dMRI59RJ3yCLVLQ1wm8+/GJBg2a2jqwJj8D63edkj7TI/Mn46FuwmDTPleEJiU6FHHhIQgNUWLHd+agssEGh5PjYoMVv/uwFKvmc6/vw9c4BYZBX2zkfmaBaGpKEARBDBxfmQtJBXMwJSki4Gsr9WWTp6fUwWAWbAMh2O7piJGPMNwDCDS+JugLdZZhGY84gWtUrq9a/KU1RupQVtuKT85fRlltK5xO7nV8okGD+65LQWlNK+wdDiyfa8KK3FT8avE0lNS0+qjh0cPe0Ymmtk5kj49GWIgKD+34HCteOYINe87CXNeGlTs+xxeXLb2OsygvDVOTDYO+2Mj9zB598ziWZCd7HEd5ywRBEIOD08ll1yTAd0bA3jM12HPyksexvV3LH8Qb/N2FOXi14FrsLsyhG90uRJEzxxQDU2zYqP9Ogu2ebrQw0N/Rkcyoi/gMVW6wv/iy6ezJd188vralHd/646eI1Knx1KKp2LDN5dy2IjcVCh92mxfrrWjrcOBaUwwA39/H6UvNkoW0+zgnPpiD8noLdGol4sND/DI2GGgY2tcY0+P10mekHT+CIIjBoadddQDodHDZ9cbhhFcKcqB26EdzFIPwn2C7pxsNjPUo2qiL+IgpW+74ihQMleLtvkNT3mDtcQdDPN7JOWwdTlQ12fDFZQsKc9OgUQnYeagC0aFqFOWleURoVt6cjlC1wiNK4+v7OFfd4rVjIggME+LCcOOkeFxrisb4mN53k8RfoFs3HcA3XvgPbt10QHYHsCd8jVFsako7fgRBEINHT7vqF+osWP32cWn9AVzz85r8DLxxuEK6CfXnWgTRV/pyT0f4x1j/HR11ER9/c4OHU/H2tIMhOqxVN9ugUyuREq2Fua4NkaFqbP13GZbPNYExwGJ3IFStwIv3ZaPN7kCYRgm1QkB0mNojSjM+OhRP3jkFj7553MsE4foJ0QPeMQmE7aivnxk1NSUIgvCf/kbffa1J9ZZ2OJzAbdOSwMFRlJcGi90BzoEWWweqmmxeN6G0Q08EEqr3Cjxj/Xd01Akff/sGDEWfAF/IGQmkRGuhVSqw+3gVSmpasKO4Ag1WO564IwvP7StBZaMVS7ONHg3VxB24uWmxPt9LEBhmGiNQMM8EJ3c1OxWd3wKxYxKIX6Ch7KtEEAQxGhnIZp6vNamy0eZhk1yYm4adh1xr0/K5Jtmb0L4Y+pBbF+EPE+P1+M29MxEaokS83r8UfMI3w+WaGCyMOuED+JcbPJyKt/sORkq0Fg/mpmHpCwe9ojKr3zqB7QVzYOt04ievH5UiPpwD24vLMT8rodf3M0aFYlJC+KDsmATqF4jyuQmCIPpPT5t57pkEcgJDbld9/YIpKOiqKxWvJzqHTogNQ1KEBotmJvl1rWDLuiBGBr7+jxijKNozEMZ6FG1UCh9/GE7F2z3CoVUqJNEDXFlgls814fkPStHW4cDs8dFYNX9yv/6jDmZEZaz/AhEEQQQDPaWrnbnU4jFHb1g0FVdFaBAdGiKtBbdMjsf2gjmoarIh0aCBpd0he70Z4yLw1fQ4AC6x9Z8v6jzElC+jnO4MZdZFoCNL/l6PIloDYzgzc0YzYz3LZswKn+G+YXePcOw7Uy27wDB2RYwN9D/qYEVUxvovEEEQRDDgazNPpRC8bh5X7TyG5XNN2PpRGZ65azpumRzv5TT6wrJs2euldK2R4k58pE6NJdnJSI/TY3JiOK6Ocb1+trqlx2hOb1kXgRINgY4s+Xs9imgNnLFeizKYjOUsm1Hn6uYvwdQnIFStlHUtERg8xFiw+vcH67gIgiDGCr56xlnt8pEbxq7soJ+savISR6vfPo4Ni6Z6XU9MmxNFz7I5KdiyvwwrXjmC/3rO5epZXt+7a1RPbl2BcAsVCbSDlb/XG+vOWYGAHN2IwWDMRnyA4FG88eEhKMpLw8a9V4wL1uRnYFqyARmJBhISBEEQ3WCM/QFAPoAaznnWcI9nuPEVfb9QZ5GN3PAuDSG2TOgujsx1bUiK0GB31/ViwzRQKoDPLtSjztKOB3JM0CgFyXBHvNbKHZ/jpftn97pTLwq1DXtOI39qEhQCcE1KFIyRuoCmOAU6auDv9ShaMXCGOzOHGJ2MaeEzEAKZu2uMCkVafJjkvCYwlxgi0UMQBOGTPwHYDODlYR5H0CC3mSd38yia5wAuEZRokE+TiwoNgSk2DOOjQ7HvbDVKqls9NujWL8iSvbm32jt7raEV64o6HE4P57hn7pqOWL06YKIh0PW8/l5vrDtnBQJKpScGgzGb6jYQAhmGB1y/3LkT43HH9CTMTY3GHdOTkDsxflB+uYeqaStBEMRgwjnfD6B+uMcR7Lindb/y7WuxZVk2theXSz14nrlrOjITDbJpcuLO+oU6C45VNEmiB3AJkYoGq2wqkjFKPu1OvJ64Dh0qb5BEj3jNlTs+h1ohBCzFyVcKYH+jBv5eL9DvO1ahVHoi0DDOR8aNb3Z2Ni8uLh7uYQAAympbceumA147ObuD3GmEii2JsQRj7BDnPHu4x0EMHoyx8QB2+ZPqFkxryHAiZit030H39TwAfHL+Mv59vg6b95V6XCvRoMGPbknH6rdOeK0pAHy+z76z1ThW0YQkgxar3jguXWvhzGQwBuSkxqClvQMr/nokYIYE4lhiwzRQCEBVU/+zNfy9Xk/fKUEQg0dP6z+luvWDkZq7S9aQBEGMJRhjBQAKAMBoNA7zaIIDX7WtgsCkaER1sw0ttk7YHQ5Eh4Yg0aBBmFrhlbrVYLUjKUKLvz2Yg9rWKyLA3eK6+/uU11tQUt2KLfvLXHVCKkEySRDrhV484HKb21OUg0vNAxcN4mceHx0akM0/f68XLHXEBEFcgVLd+sFIdRrpSbARBEGMNjjnWzjn2Zzz7NjY2OEeTsDoKWW5v+nM3VO4l275BIfNjbj/T5+itLYVSZFaFOWleaRuPXRTOh554xgYA2aPj8bZ6hbM39hzCnh1c7uUMrfzUAUKc9OwJDtZ1iTByRHQFKfhcngjCCJ4oIhPPxipTiNUbEkQBDGy6SllGYDXa5vvmYGro8NQ09JzatcXl71v4p/95zmsuDEVxyqasGV/GSJ1aiyfawJjLhMeJ+cw17VJm2dyIiCjKAdODskIyNLeKR1T1WTDtoNm/DAvbUiyKIbL4Y0giOBh1AmfoeiUPFKdRkaqYCMIgugOY+wVADcAiGGMVQBYxznfOryjGnx6SlkGPMVHpE6NkupWv2plzPUWROrUUp0NAOw8VIHYsBBcbGyTbK+f/+BKnc+K3FRp80xOBETq1Dhc3ohH3zwuvf/Wb3k2Rq1qsuHLprYh2ZQbLoc3giCCh1ElfIayeH8k5u6OVMFGEATRHc75N4Z7DMNBT1EGzuHx2sKZyVJamWgecOZSM5IitJiS5NkuwaBR4b7rUjzsqh/9+iRcFamFQsGw+Z4ZqGiworXdgZ2HKtBgtXs12e4uApZkJ0uiRxzno2+6GqO6W1jH6kPwxB1ZXiYJgd6UC/TmH20mEsTIY1QJHyre752RKNgIgiAIF92jDIkGDZZkJ8NqdxkRpERrYa5rAwAwBkn0uJsHbNlf5rUpqFIwD7vqSJ0aFrsD33652KP/z7tHK3HfdSkwRumQeVU4jFGhkjFCdxGQHqeXbYwaHaqS+tZxDjz/wXmolQzbC+agrcMxaJtygd78o81Eghh5jCrhQ/m2BEEQxGjGXWCkx4Vh6Wwj1u86JYmNJ+7IwqufmnGtKRbjo3XYfM8MqBQCil490uOmoMXu8BktEs/ZtK8Ey+easHFvCXatmIvxMVfWVTkRwLl3FEijEqBSKLBpr6c1NgC0dTgwxxQzKN+b+zgDuflHm4kEMbIYVcKH8m0JggSezMQAACAASURBVCCI0YwoMDKKcvBFnRXf+/MhD3Hy3L4S/DAvHT91q6tZk5+BSJ0aVU1XHDxtHU6Y6yyos7RDrRAgMOaxforRIndsHU7p+TPVLV5ua91FgNPJZVPB4sNDaK0mCGJYGFV21kPRKbm/VqEEQRAEESia2jpwpLzBS5zkT02SRA/gEinrd53Ckuxkj+M0KgFnLrXgVGUz9p6pwZGLDVibnyGtnwoG2bYNYhTnXHULvrjcu23zxHg9fnPvTGz/zhzsKcrB/MwEGKMGf60mCIKQY1RFfAY733YozRMIgiAIojviOnTmUjOcMqlkCkE+UmOM1EnHalQCivLSEKpWwGJ3YMv+Mtg6nEiJ1uLZLkFydUwoJiaEe6x3hblp2F5cjsLcNGw7aMaMcRGYECef4uVrvRRrgqg2hiCI4YBxPjIiFtnZ2by4uHhYx1BW24pbNx3wCs/vJvMEggg6GGOHOOfZwz0OIjgIhjUkEFy43Io3jlTCGKVDVKgK9ZYOrH7rBCJ1aizJTsaslEh8Z9shr3VqxY2psDucyLrKAJVSwKNvHMejt07Gj18/6nXsS/fPxrWmaKk9RGlNK+ydTlQ0WtFic+CNwy5XN/E4OWi9JAhiuOhp/R9VEZ/BhswTCIIgiMHAnx50TifH4fJGKUKjUQl48s4srM2fDA6G9btOIVKnxsqb0/HM++e8IjXf+2oqtuwvxfKcCVArGcJCFHggxwTA1bOnqsm1xtVb7HA6uVSzIzDg7ycueVhdF+WlIT48xOfnofWSIIhghIRPHyDzBIIgCCLQ+JtGfaHOItMX5wS2LMtGwbZiqcmow8lRMM8EpSBgfEwovmy0YsH0JDS32XGtKRZbD5zHD25Mw/f+cthDHG07aEaD1Y4LdRZ8dqEesfoQjI8OhTEqFGnxYZIFtcCAtPgwGKN81+TQekkQRDBCwqcPULMygiAIItD46kE38cEcMAYpClTdbEOkTo2FM5PBuvSQK1LT5iEwLHYHNu/ztotekZsKxoAbJsZj7dsnvKyqC+aZkBCuwXP7SmGxO/DigSv9fm5Ii0NsWAiqmmxINGiRmRjeY00OrZcEQQQjgyp8GGN/AJAPoIZzntX13HQAvwOgAdAJ4Puc808D+b7+pAz0ByrIJAiCIAKNe1pYokEjCZsvG9tgrmvF8/8qQ4PVjj/99zW477oUr5SzCJ1aNrrS/bHAAK1KgVh9iGwaWpJBi4Y2OxqsdoSqFSjKS4PTybHvTDVClAqsfvs4zHVtPRr7uK+/xigtXv6f2bC0d8IYFYqrY/xbL51Oji8uW2CutyBUrUR8eIhkikAQBDEQBjvi8ycAmwG87PbcUwAe45z/nTF2a9fjGwL1hoPtvEbNygiCIIhAIqaFRerUWDYnBZv2uYTNiyoB627LxEN5qXh2bynO17Z6NRXduLcEv7wzC2vyM6RGpu8ercQv7pyCL+oscHKXNbUxWofWtk5YOxy4WG+VFUaVTW0AgMdvy0BUWAga2zpQUtOCHcUuMwMxHa6qyebVABWQX3/F+qJV8yfj6pjeoz1y1yjKS0NafBhyJ8aT+CEIYkAMah8fzvl+APXdnwYQ3vVvA4AvA/mevlIGLtT13m+AIAiCIIYaMS1sSXayJHoA1/r12LsnoQtR4d5rjTBo1fJNRQUBMaFKbFo6A5u+MR13X2NEncWOLfvLsHlfKX6/vwzgDNPGGTAxQY8dxRUozE3z6KOzJj8DaXF6xIWp0cmBFa8cwcOvHcPv95dhRW4q0uPCsGlfCRbOTJbet6bF5jEWufV3074S5E9N8nsdlrvGxr0lOFbRROs4QRADZjgamP4QwK8YYxcBPA3gp4G8eE9OMgRBEAQRbIhp1BmJ4bLrl8XeidiwECgVTLap6PnaVmjVahRuP4KL9Va0dTjw9HtnPcTDz946jnCtGuOjQ9FgtWPbQTOWzzVhRW4qCuaZMC3ZgK9nJsAYHYbHuyJH4rnrd53Cd29IRaROLdUWaVQCYsM8jQp8rb+M+b8O+7qGk4PWcYIgBsxwCJ/vAXiIcz4OwEMAtvo6kDFWwBgrZowV19bW+nVxMWXAHXKSIQiCIIIVsS4mKlQtu36FqpWIDFXDYu/E47dnekRqCnPT8FpxBWpbbFg+1wSHE0iP08uKh3PVLeAc2HzPDDRY7Xj+g1K8eKAMkxLCkZFogFIpQKVgsueeudSMJdnJ4F1NU4vy0vBFXSucziu9AH2tv+I5/qzDvq4hMNA6ThDEgBkO4fMtAG90/fs1ALN9Hcg538I5z+acZ8fGxvp1cTFlwH1hICcZgiAIYrBxOjnKalvxyfnLKKv1FAU9nbPvbDX+cfISGqx2PNZN2Dx2eyZiw9WI06vxszdPwKBTo2CeK1KzfK5JsqA26NTY+lEZnnn/HEpqWpASrcUPbkzFilzXn5RoLY5XNuO/njsAeyfHnqIcvFpwLXYX5njUwPoSHg4nYIoJg16jwPK5Jrz8iRkr/nrEI/1Mbv0tzE3DrmOVfq/DctcoykvD1GQDreMEQQyY4bCz/hLAVwH8C0AugJJAXpyc1wiCIIihpr/GOuX1FpRUXzEtyE4x4IVl2Wiw2iEwhi37z+NcTSs2LJoKW4cTr/7HjPlTErHunZPS+zxxxxSs33VSitR8cKYG3/1qKh5798ox6/Iz8cqnZqnudXdhDuaYYrzGMz46FBsWTcWqnce8DAqWzBqHp98753G8e0NS9/W3utkGnVqBDocT87MS/F6HxWtMfDAH5fUW6MjVjSCIADLYdtavwOXYFsMYqwCwDsC3AWxkjCkB2AAUBPp9++q8Nlj21wRBEMTox+nkOF7RKGusM6kwB+OjQ32uMdXN7Xj1s3Isn2uS6mdWv30c9842osnmwLyJcZg3MQ7VTW1Iidbimquj8VpxOZ5aPA02eyeuitTiywYrzHVt0nhy0uMk0SOO5bFdJ7F8rgnHKpulehu5NVIQGP4rKxGROjWKzfVwOIHtxeVYefNEPPP+WY9j5dLXAuF8KggME+LCMCGO3FMJgggsgyp8OOff8PHSrMF8374w2PbXBEEQxOhFXEPOXGqWrY2pbrbhzKUWn2uM3eHA0mwjNu0rQaROjSXZyXgwNw0xYWqse+ek1DfnoZvS8fPbM/G9Px+GrcOJYvMRAC7xsWXZLA97atFMoPtY3I0JRMEit/GnVAqYmxqD5EgtalpsWDQzCcZIHVQKgRqSEgQxohmOGp+gguyvCYIgiP4iriHOrgJ+dzQqATq1osc1JjxEJYmeZXNSsGV/GR5+7Ri+++fDWJptRKJBA1uHE8/+8xwsNoesoGmzOzzsqRVMfiyiyYAoWETRduumA/jGC//BrZsOYM/JS3A6uRS5mWOKgSk2DEqlgPmZCdhdKF8bRBAEMRIY88KH7K8JgiCI/iKuITsPeffGefLOKbA7nIjUqT2MBiJ1apyrbsEXta2oanGdv3Cmdw+f7n1zmA9B44QrHU20p9aoFFh5c7rHWNYvyIJBo8CWZdm4ZbKrEWhfN/66iyESPQRBjDSGw9wgqBAdbLp3sCbbTIIgCKI3xDWkqskm9cZRCC5L6awkV6/u+65LkcwLRJeyeks7zlyCJGbc09OmJoXjgXkT0NbeicQIDQ6cC8e5mlaU11uxJj8D67v67IjGA7/9oNTLzOB/78zCb+6diROVzeh0OnG5tR0b9pyFRiVgd2EOTLFhPW78DaRGhyAIIlgZ88JHtM6kvGWCIAiir7ivIVVNNmz9qAxFeWnQqAUYo0LxxWWLl3nBq5+V43/vzMLximbEh2vw/D0zUV5vgUYlID0uDN+YnYKfvH70iiPbbZkwaJT4w7/L8O15qXh68TRY7J2obWmX7KzT48JQMM8EJwc4B57+xzk0WO0omGdCYrgWr3xqBuApbAK98UdGQQRBBDtjXviQ/TVBEATRX3qzX663tkvmBaKQeeimdFQ3t3tEgTYsmoKVN6fjKoMWD3eJHqDLke3dk9j6rWwU5qXj5JfNsNgdCA9RYGK8Hvdea8SEuDDUWe3YtLfUa3xJBi1+82Ep8qcm4Vhls4ewCeTGHxkFEQQxEhjzwgcIjP0mQRAEMbrxFdHoyX6ZgXnV7jz7z3MomGfyeG7VzuMoyktDp5N7RGASDRosnJmMeksHQtUKvPpZOeydHPddl4IHXz0iiYy1+RlIidZ62FprVALKG9pgrmuTUurchU0gN/581QtN6kqrIwiCCAZI+BAEQRBEL3R2OvFxWR2KzfVwcuDdo5VYNX+yV0SjuzhqsNpl62iSIrRINGhQ1WSTxE2UTo2oUJUkYBINGnx3ngl1VjvOVrdAwYDvzDOhqa1TihaJ13t81yn89puz8L0/H/Ko/9l20AyNSkBOagyWzEpCo6UD7526hESDFpmJ4VAqhYBs/FG9EEEQIwESPgRBEATRA04nx99OVGHVzmMeomLDntOYlKCXbuw7O51ex/323pmydTSVjW1YNicFe05UYX5Wokcq3OO3Z+L5f5Xi7muMsHY4sGV/mYcxgik2TL5PD+f424M5OH2pGeeqW6T6n2fumo4ZyRF45/iXWP3WCelaT9yRhTumJUGpHLjBKxkFEQQxEhjzdtYEQRAE0RMX6iySmAGuWE3nT02SWh84nRwfl9V5Hffzd09iTX6Gh7X0mvwM6NQKtHc68L0bUr1S4da+cxKPL8jC5MRwr8jOxr0lUAlM1tZaEARcHROKW7MSccf0JDy7dJrUb+d0dbMkesRrrX7rBM5cakZZbSs+OX8ZZbWtcDp5j9+F08lljxfrhdw/p3tana/zCIIghhKK+BAEQRBED/hK41IIQJxeg85OJz6vaERVUxseyDFh56EKAMDCmclgDDDF6LBl2Sx8frEJqXFh2LDnNMx1bV2mBlNlr/3ZhQaMi9DKvlZnsaMoL83DHKEwNw1r3j6OP/73bCl1zT3FrKrJ+zNE6tQ4fakFa96+EgXqyZCgNwMDX/VCZHxAEESwQMKHIAiCIHrAVxrXdaZoXG5tx5GLjfjZm8c9XNs0SgG/2HMGtg4nDp434OGvTUKn0+llAFBW2yp7bY1SwGVLu+xrsXo1DFoVVtyYClunE5wD2w6aUdVk81lTk2jQel1rSXayJHrE8fRkSNCbgYEvoyAyPiAIIligVDeCIAiC6AG5NK71C7LwyBvH8OH/t3fvcVLX9R7HX5/ZZdkbC3uBhXZZYBFE7uBGZGCi1YOSjmZpWlmWPXhU3rJOaXnSk2VZ9tAs7WKKaXVSykvG8YgldKiOtwWVmyhEQLuuLiLscptddudz/pjZcZa9ALsz/naG9/Px8MHMb37zm8/3MeDMZ76f7+f78uvxpAfe7NpWWjiYz86v5pyZozh/zhiuemAto4vzu8y6LK2t61IKd8UZE8gKGb9+agdfeu/ETo996b0TaY84a+ubuG3lFm5bsYXbV26hoSnc65qaKaOKuP6sqZ2uVV3W/VqhjvK9w/XWwKA3fX2eiEiyacZHRESkF4eXceUNyuLy+56Lt4nurv10W8SZOXoo808o41N3PxP/on/4rMvuA600HzwU3+DUHe59cjsfPrmS3QdaGZwV6jSzc/fft/H1D5zEA6vruPz0CZ2aInznQ9MIWbQk7fASsuzsEO8YVxzf/DQvJ5udzV3j6S156msDAzU+EJGBQomPiIjIESSWcT35j9e77JcTPhRh1NBcLpw7hh+t2Exxfg6XLBhPOC/CZ+dXA7B8/atdkpWbPjKD3ftbqCzO50BLG6/vbyEn25hUPoSbPjKDHzy+qctrFeRk0dAU5v7aHSxdPJcduw/y8mt7uWn5S/Eubt2tnxldXMCGV/Zy3SMbCB+KMKY0j+sWTeGbyzYkbKQ6vccNTPu64WkyN0oVEekPc0+Pzio1NTVeW1sbdBgikibMbLW71wQdhwwMyfoMiUScdXV7eOKlRobmDmJCeQGvNbfyjT+s5+J51dz1t2jr6esWnUS4LdKlAcFj6xt4/7RRVJXkUzYkh62N+9iTsC9PRxndr57cxs59rfFEquOxbyyaTOuhdr63/CVuPm8mJ5YP4cwf/7XLbMqjPayfSdxn6FC786MnXuId1cPJCkHNmBJOqS7ttb11x/OPdcPTvj5PRORY9fb5r8RHRDKSEh9JlIzPkMO7k40pzePqhSexfdcBKkvyGJaXxRsH2jnY0saYsnx+sHwT9Xta4t3dsgxKC3IYlJVFQ/NBJo4YwsuNe+P79HTIHRTi4nnV3L5yS7x0bmxpPtt2HWB6ZRGlBTmUFAxmbGkBT/9zFxf84un4eRbLJU6bWEbN2NIjjkfJiIhkmt4+/1XqJiIichQSu5ONGprLR2uquDIhCbrktAlc+8ibraFvOHsaLW1tXL/sxU6bhv54xWY+OKOC/S1tRJxuF/53JDANTWHu+tvW+GzSf182n/EjCuNJS8iMMaV5fLSmqtPM0IQRhczuZq1Pd9Lk908RkX5T4iMiInIUEruTnTO7stPGo4umV8STHogmL9c8vI7Fp1Z32TT04nnRNT+7D7SSZXS78L8jX+kokbu/dgc3nzeTcWUFnWaeivNzuHbR5HgC1vE6Vz2wlmkVQ3tsF629dUTkeKTER0RE5Ch0dCcrzs+hqrjz5qKHd3eD6P3IYbMpHRuf/q62js+dWk1hbnaXzUivP2sKY0rzeWd1Kfk5WUTcmTOuhDf2t7Cuvolh+dnxhKWhKczmxn09tovuKfHR3joicjxS4iMiInIUxpYW8OMLZrGlcR+vNB3sdqamp5mbxGOzq4ox4IE1dSyaPoppFUX89OOzaW2LUFKQQ/nQwVQOi663aWuL8PAL9fzHw+sTEqOpTBxRyNr6ZgCyQt2/dm/tonvbW0eJj4hkKm1gKiIicgThcBtrduxmcHaIW5/YzNLa6D46HRuC/vGFeq5bNKXTBqFXvmciZQU5XTYn/dqD6/j5qq18dE4V7Q5f+f06Pv3LWta90syFS57huR1NRGJTRRsamuJJD0STk2v/sJ7Fp44HovsGFeZEZ40SX+dI7aI7Zq8SaW8dEcl0mvERERHpRTjcxiPrGrj2kfV8dn41xfk58U1K77jwZDY2NLM33M5vn9ke34j0xPIh/ODxTXx2XjWXLjiB4YWDqW86yL1PbqehKQzAt5Zt5OJ51TQ0RTcSdX9zfU5xfg7zTiijoan7mRknmqicM7uS7z62ieL8nPhrhwwmjxrS61od7a0jIscjJT4iIiK9WP9qM9c+sp7i/BymVQylICeL+57dwaLpFdRu381Jo4r46cotrK1vZm19M7mDQtxxYQ3nnjyan/zlHzQ0hbn09BO4bcWWTtcNH4pQVZLHFWecQEl+Dj9btTV+vHb7G1QW5zFqaF63ZWxVxXk8evl8Xn5tb3ytz+0r37z+KeNLGVvWc8laKGQsnDKSSZfPVztrETluKPERERHpQSTi1O85SHF+DhfOHUP97gPc9+yOLu2jv7FoMjtXbGH3gVa+86Fp1O/ez20rtxxxDdDOvS38fNVWrnzPxE7H2yPQuDdMTVUJ3z57aqc1Pt8+eypT3jY0vtHosa7v6RAKGdXDC7WmR0SOG1rjIyIi0oNtu/bTsOcg59ZE21eXFOSwaHpFp1bW4UMRvrVsI9d84CTuvujt5GaH+NmqrV3XAH1wSpf1PlkhI3wowi1/fplzZlfG21cvW1vPiCG5ZGeHOHtGBfcvnsvPPzGb+xfP5ewZFfGkp6Nk7VjW94iIHK804yMiItKD15rDFOdlM3JoHu8cV0LFsDyyQiFu+sgM6vcc4J7/2x5fhxNxJ+IRhhXk8LX3n8SQ3Gxu/9gsNryyl3FlBfxi1T/i63Dc4d4nt/PhkyuBaPI0piSPi+dVc3/tDq5aeFI8ecnODjFjdDEzRneNTyVrIiJHT4mPiIhID4rysikfls+OXfs5c8bbuHDJM/GSsyvOmMDnTq3mZ6u2svtAKyUFOTQ0hTuVpV23aAoPrPkXH5xRwcuN++ItqIF4Q4OO22NLCxg+ZDDnzKpgXNnRJy8qWRMROToqdRMRkbRjZgvN7CUz22JmV6fqdVoPRdjX0kZzuK1LW+lbn9jMrgOtnFtTyTcWTaalrb3LOd9ctoEbz5nO1FFDui11e3BNXbw8rWZsCQsmlTN+RKFmbEREUkAzPiIiklbMLAu4HXgvUAc8a2aPuPvGZL/W3pY26t44QGt7pNu20hGHquI83jYsj537Wro9p37PQX68YjOfOWUcly44gdb2CO+ZNIKivEHMqhqm8jQRkbeIZnxERCTdzAG2uPtWd28F7gPOSsULHTzUzj1PbmfW6GHdbvgZMhhelMvQvCxGFg3u9pzc7Cy+8r5J/GzVVm5buYVJI4uYWjGMsWWFzK0uo3q4ZnhERN4KSnxERCTdVAD/SrhfFzuWdMPyB7H7QCt/fL6eGz40rUup2onlQygfksMz/9zDuromvnXW1C7n5OaEmFpRxC0fncGjl89n4ZSRSnRERAKgUjfpUSTibNu1n9eaw5QXqRRDRNKLmS0GFgNUVVX16RojCnO5euEkbnxsE7sPHOLOT9awN9zGkNxsWtva+eGfNzN/4giyQjD/hDJmjS5mdlUxO97YT35ONuVFg6kqif6/s7cNRUVEJPWU+Ei3IhHnsQ2v8qWlz8e7E9183kz9UikiA0E9kNjcuTJ2rBN3vwO4A6Cmpsb78kJjSguoLNnHFWdMoKxwMIfaI+xvaWXPwUN8848bCB+K8HLjPq4/ayozKoaRnR1i/IhCxo9QkiMiMtAo8ZFubdu1P570QHSB7peWPs+ky+erZaqIBO1ZYIKZjSOa8JwPfCwVLxQKGaefWE51WSGNe8OMLMqlqqSA5nArv/rMHBr3tlBelMv0tw0lJycrFSGIiEiSKPHJIMksTXutOdxtd6LGvWElPiISKHdvM7NLgeVAFrDE3Tek6vW0T46ISGZQ4pMhkl2aVl6US+6gUKfkJ3dQiBFDcpMZtohIn7j7o8CjQcchIiLpQ13dMkRPpWnbdu3v0/XGlhZw83kzO3Unuvm8mYwtLUhazCIiIiIibxXN+GSIZJemhULGwikjmXT5fBr3hrXBnoiIiIikNSU+GSIVpWmqaxcRERGRTKFStwyh0jQRERERkZ5pxidDqDRNRERERKRnSnwySH9L0xLbYefnZNPa3k5pwWAlUCKS9lavXv26mW3vxyXKgNeTFc8AonGlF40rvWhcwRjT0wNKfATovh325adP4P7aHVy18KQ+t8UWERkI3H14f55vZrXuXpOseAYKjSu9aFzpReMaeLTGR4Du22H/aMVmFk2v6FdbbBERERGRgUCJjwA9t8M2e7MttoiIiIhIukpp4mNmS8ys0czWH3b8MjPbZGYbzOz7qYxBjk5HO+xEuYNCuPe/LbaISAa4I+gAUkTjSi8aV3rRuAYYc/fUXdzsVGAfcK+7T40dWwBcA5zp7i1mNsLdG490rZqaGq+trU1ZrMc7rfGRTGNmq9O1BllERESSL6XNDdx9lZmNPezw54Eb3b0lds4Rkx5JvcR22NGublkcao+wcOpIdXUTERERkbQXxBqficB8M3vazP7XzN4eQAzSjY522O8cX8aM0cXUjC2lenihkh4ROa6Z2UIze8nMtpjZ1UHHkww9laKnOzMbbWYrzWxjrJz+iqBjSgYzyzWzZ8zshdi4vhl0TMliZllm9pyZLQs6lmQys21mts7MnjezjChZMrNhZvb72HKVF83snUHHdKyCSHyygRJgLvAVYKmZdfvN2swWm1mtmdXu3LnzrYxRREQEM8sCbgfeD0wGLjCzycFGlRS/BBYGHUQKtAFfdvfJRL9nXJIh71cLcLq7zwBmAgvNbG7AMSXLFcCLQQeRIgvcfWYGlV3fCjzm7pOAGaTh+xZE4lMHPOhRzwARohshdeHud7h7jbvXDB/ery0YRERE+mIOsMXdt7p7K3AfcFbAMfWbu68C3gg6jmRz9wZ3XxO7vZfoF7OKYKPqv9h3pn2xu4Ni/6VukfZbxMwqgTOBO4OORXpnZkOBU4G7ANy91d33BBvVsQsi8XkYWABgZhOBHAb27q8iInL8qgD+lXC/jgz4In08iK0xngU8HWwkyRErCXseaAT+5O6ZMK4fAl8l+iN4pnHgcTNbbWaLgw4mCcYBO4G7Y6WJd5pZQdBBHatUt7P+LfAkcKKZ1ZnZxcASoDpWV3wf8ClPZWs5EREROa6YWSHwAPBFd28OOp5kcPd2d58JVAJzzGxq0DH1h5ktAhrdfXXQsaTIPHefTbRM9pJYp+N0lg3MBn7q7rOA/UDarXlMdVe3C3p46BOpfF0REZEkqQdGJ9yvjB2TAcrMBhFNen7j7g8GHU+yufseM1tJdI1WOjeneBfwb2b2ASAXKDKzX7t7RnxHdPf62J+NZvYQ0bLZVcFG1S91QF3CTOPvScPEJ4hSNxERkXTxLDDBzMaZWQ5wPvBIwDFJD2LNku4CXnT3m4OOJ1nMbLiZDYvdzgPeC2wKNqr+cfevuXulu48l+u9qRaYkPWZWYGZDOm4D7yO9k1Tc/VXgX2Z2YuzQGcDGAEPqk5TO+IiIiKQzd28zs0uB5UAWsMTdNwQcVr/FStFPA8rMrA64zt3vCjaqpHgXcCGwLrYeBuDr7v5ogDElwyjgnliXwRCw1N0zqv1zhikHHoo1Lc4G/svdHws2pKS4DPhN7EegrcCnA47nmFm6LK8xs53A9qDjOIIyMrdRQ6aOTeNKP0c7tjHurnaQIiIiAqRR4pMOzKw2g3q1d5KpY9O40k8mj01ERERSR2t8REREREQk4ynxERERERGRjKfEJ7nuCDqAFMrUsWlc6SeTxyYiIiIpojU+IiIiIiISODNbAnRsbtvrJr1mdguwIHY3Hxjh7sN6fY4SHxERERERCZqZnQrsA+49UuJz2PMuA2a5+2d6O0+lbn1kZkvMrNHM1iccm2lmT5nZ82ZWa2ZzgoyxwuT1PQAABhRJREFUL8xstJmtNLONZrbBzK6IHS8xsz+Z2ebYn8VBx3osehnXTWa2yczWmtlDHRvEpZOexpbw+JfNzM2sLKgY+6K3cZnZZbH3bYOZfT/IOEVERCQ53H0V8EbiMTMbb2aPmdlqM/urmU3q5qkXAL890vU149NH3WWkZvY4cIu7/4+ZfQD4qrufFmCYx8zMRgGj3H1NbNfh1cDZwEXAG+5+o5ldDRS7+1UBhnpMehlXJdHdotvM7HsA6TQu6Hls7r7RzEYDdwKTgJPdPW329unlPSsHrgHOdPcWMxvh7o1BxioiMlCY2fVEP69/GLt/A9GyoVuDjUzk6JjZWGBZwvfrJ4DPuftmM3sH8F13Pz3h/DHAU0Clu7f3dm3N+PRRdxkp4EBR7PZQ4JW3NKgkcPcGd18Tu70XeBGoAM4C7omddg/RL6Bpo6dxufvj7t4WO+0poolQWunlPQO4Bfgq0b+baaWXcX0euNHdW2KPKekREXnTEuCTAGYWAs4Hfh1oRCJ9ZGaFwCnA78zseeDnwKjDTjsf+P2Rkh6A7OSHeFz7IrDczH5ANKk8JeB4+iWWcc8CngbK3b0h9tCrRH91T0uHjSvRZ4D73+p4kilxbGZ2FlDv7i+YWaBx9ddh79lNwPzYr5hh4N/d/dngohMRGTjcfZuZ7TKzWUQ/q59z911BxyXSRyFgj7vP7OWc84FLjvZikjyfB65099HAlcBdAcfTZ7EM+wHgi+7enPiYR+sj024GAXoel5ldA7QBvwkqtv5KHBvRsXwduDbQoJKgm/csGygB5gJfAZZaumd2IiLJdSfREvVPE50BEklLsc/9f5rZuQAWNaPj8dh6n2LgyaO5nhKf5PoU8GDs9u+AtGtuAGBmg4h+0fyNu3eM57XYmouOtRdpV17Uw7gws4uItk78uKfporduxjYeGAe8YGbbiJbwrTGzkcFFeex6eM/qgAc96hkgAqRV4wYRkRR7CFgIvB1YHnAsIkfNzH5LNIk50czqzOxi4OPAxWb2ArCB6PKLDucD9x3t9zeVuiXXK8C7gb8ApwObA42mD2K/nN8FvOjuNyc89AjRxO7G2J9/CCC8PutpXGa2kOgamHe7+4Gg4uuP7sbm7uuAEQnnbANq0qy5QU9/Fx8m2rd/pZlNBHKAtBmXiEiquXurma0kWiJ0xHUPIgOFu1/Qw0MLezj/P4/l+urq1kexjPQ0or80vwZcB7wE3Eo0oQwDX3D31UHF2BdmNg/4K7CO6C/pEC2ZehpYClQB24Hz3P3w5g4DVi/j+hEwGOiof37K3T/31kfYdz2Nzd0fTThnG+mX+PT0nv2ZaOnGTKCV6BqfFYEEKSIyAMWaGqwBznX3tPsRViRVlPiIiIiIZAgzmwwsAx5y9y8HHY/IQKLER0REREREMp6aG4iIiIiISMZT4iMiIiIiIhlPiY+IiIiIiGQ8JT4iIiIiIpLxlPiIiIiIiEjGU+IjacnMxprZ+qDjEBEREZH0oMRHREREREQynhIfGVDM7Hoz+2LC/RvM7IojPCfXzO42s3Vm9pyZLYgdzzezpWa20cweMrOnzawm1WMQERERkYEnO+gARA6zBHgQ+KGZhYDzgTlHeM4lgLv7NDObBDxuZhOBLwC73X2ymU0Fnk9l4CIiIiIycGnGRwYUd98G7DKzWcD7gOfcfdcRnjYP+HXs+ZuA7cDE2PH7YsfXA2tTFLaIiIiIDHCa8ZGB6E7gImAk0RkgEREREZF+0YyPDEQPAQuBtwPLj+L8vwIfB4iVuFUBLwF/B86LHZ8MTEtFsCIiIiIy8GnGRwYcd281s5XAHndvP4qn/AT4qZmtA9qAi9y9xcx+AtxjZhuBTcAGoCllgYuIiIjIgGXuHnQMIp3EmhqsAc519839uE4WMMjdw2Y2HvgzcKK7tyYpVBERERFJE5rxkQElVpK2DHioP0lPTD6w0swGAQZ8QUmPiIiIyPFJMz4yoJnZNOBXhx1ucfd3BBGPiIiIiKQnJT4iIiIiIpLx1NVNREREREQynhIfERERERHJeEp8REREREQk4ynxERERERGRjKfER0REREREMt7/A5VVgUKTPLUdAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 1080x360 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15,5), sharey=False)\n",
        "sns.scatterplot(x = predictions['y_log'],\n",
        "               y = predictions['y_predict_log'],\n",
        "               ax = ax1)\n",
        "sns.scatterplot(x = predictions['y'],\n",
        "               y = predictions['y_predict'],\n",
        "               ax=ax2)\n",
        "ax1.set_aspect('equal', 'box')\n",
        "\n",
        "ax2.set_aspect('equal', 'box')\n",
        "\n",
        "ax1.title.set_text('Comparaison y et y_prediction (modèle en log)')\n",
        "ax2.title.set_text('Comparaison y et y_prediction en valeurs réelles')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjQpHbzydv-i"
      },
      "source": [
        "# **Pertinence des variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "y8XBD3OVdl6e",
        "outputId": "f3f79e18-27ac-4a42-bc61-83b4b2f5de2a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(4598, 79)"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_train.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e-szMQ7yd1e4",
        "outputId": "e099d899-4753-4e33-fdf5-ea8d156b2438"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1150, 79)"
            ]
          },
          "execution_count": 80,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "s-j9Bvbqd3fi",
        "outputId": "14a4fca5-8c10-4fbc-9d7f-8e604323f082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Boucle 1.\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-bb8e0e6b4327>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     52\u001b[0m                       \u001b[0mmin_weight_fraction_leaf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                       \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moob_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m                       verbose=0, warm_start=False)\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m#pour chaque feature de X_test:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'min_impurity_split'"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "#on créee un dictionnaire pour les colonnes crées par le OHE, pour les retrouver dans l'application des permutations\n",
        "text_columns =[] #les noms des colonnes qui correspondent à des vraies features (pré OHE)\n",
        "prefixes = [] #les préfixes des colonnes liées à l'application du OHE\n",
        "index_cols = [] #les indices des colonnes qui correspondent aux features pré OHE et à la première colonne de chaque feature du OHE\n",
        "regex = re.compile(r'x\\d_')\n",
        "for j, column in enumerate(X.columns):\n",
        "    if regex.search(column):\n",
        "        if column[:2] not in prefixes:\n",
        "            prefixes.append(column[:2])\n",
        "            index_cols.append(j)\n",
        "    else:\n",
        "        text_columns.append(column)\n",
        "        index_cols.append(j)\n",
        "        \n",
        "dict_OHE = {}\n",
        "for prefix in prefixes:\n",
        "    temp_list = []\n",
        "    for column in X.columns:\n",
        "        if prefix in column:\n",
        "            temp_list.append(column)\n",
        "    dict_OHE[prefix] = temp_list\n",
        "    text_columns.append(prefix)\n",
        "\n",
        "    \n",
        "    \n",
        "#préparation du jeu de données\n",
        "\n",
        "X_train_permut, X_validation_permut, y_train_permut, y_validation = train_test_split(X_train,\n",
        "                                        y_train,\n",
        "                                        test_size=0.2,\n",
        "                                        random_state=42\n",
        "                                        )\n",
        "#choix du modèle utilisé pour la permutation : \n",
        "\n",
        "\n",
        "#on boucle sur le nombre de features:\n",
        "list_features = text_columns\n",
        "features_score = pd.DataFrame({})\n",
        "\n",
        "for num_feature in range(len(text_columns)):\n",
        "    print ('\\nBoucle {}.'.format(num_feature+1))\n",
        "    scores = []\n",
        "    #on entraine le modèle sur le jeu de données sans les éventuelles features supprimées\n",
        "    model = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
        "                      max_features='sqrt', max_leaf_nodes=None,\n",
        "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
        "                      min_samples_leaf=1, min_samples_split=2,\n",
        "                      min_weight_fraction_leaf=0.0, n_estimators=500,\n",
        "                      n_jobs=-1, oob_score=False, random_state=None,\n",
        "                      verbose=0, warm_start=False)\n",
        "    \n",
        "    #pour chaque feature de X_test:\n",
        "    for j, feature in zip(index_cols, list_features):\n",
        "        \n",
        "        X_validation_temp = X_validation_permut.copy()\n",
        "        \n",
        "        #si on identifie une feature en \"X0_\" : on fait une permutation sur plusieurs colonnes\n",
        "\n",
        "        if regex.search(X_validation_permut.iloc[:,j].name):\n",
        "            #print('chaine reconnue')\n",
        "            prefixe = X_validation_permut.iloc[:,j].name[:2]\n",
        "            for prefixe_col in dict_OHE[prefixe]:\n",
        "                index_col = X_validation_permut.columns.get_loc(prefixe_col)\n",
        "                X_columns_temp = X_validation_temp.iloc[:,index_col]\n",
        "                #X_test_temp.iloc[:,index_col] = np.random.permutation(X_test_temp.iloc[:,index_col])\n",
        "                #on permute l'ensemble du bloc du one hot encoding, ligne par ligne (pour ne pas voir plus de une valeur à 1 par ligne)\n",
        "                #X_validation_temp.iloc[:,index_col] = np.take(X_columns_temp,\n",
        "                #                                        np.random.permutation(X_columns_temp.shape[0]),\n",
        "                #                                        axis=0,\n",
        "                #                                        out=X_columns_temp)\n",
        "                \n",
        "                X_validation_temp.iloc[:,index_col] = np.random.permutation(X_columns_temp.T).T\n",
        "                         \n",
        "        #sinon on fait une permutation sur une seule colonne\n",
        "        else:             \n",
        "            X_validation_temp.iloc[:,j] = np.random.permutation(X_validation_temp.iloc[:,j])\n",
        "        \n",
        "        #entrainement du modèle / ajout du score\n",
        "        scores.append(cross_val_score(model, \n",
        "                                      X_validation_temp, \n",
        "                                      y_validation, \n",
        "                                      cv=3, #################################################TEMP A MODIFIER !!\n",
        "                                      scoring = 'neg_mean_squared_error',\n",
        "                                      n_jobs = -1).mean())\n",
        "        print('    j : {}, feature : {}, score : {}'.format(j, feature, scores[-1]))\n",
        "        \n",
        "    #on identifie la feature avec la RMSE la plus faible en valeur absolue\n",
        "    #i.e. la feature qui a le moins d'impact sur le score\n",
        "    feature_to_remove_index = scores.index(max(scores))\n",
        "    \n",
        "    \n",
        "    #on stocke le nom de la feature et le score\n",
        "    feature_name = X_train_permut.columns[index_cols[feature_to_remove_index]]\n",
        "    print('feature name : ', feature_name)\n",
        "    if regex.search(feature_name):\n",
        "        feature_name = feature_name[:3]\n",
        "        print('feature name : ', feature_name)\n",
        "\n",
        "    features_score = pd.concat([features_score,\n",
        "               pd.DataFrame({''\n",
        "                   'feature' : feature_name,\n",
        "                             'RMSE' : scores[feature_to_remove_index]\n",
        "                            }, index = [0])\n",
        "                               ])\n",
        "    #Affichage des informations\n",
        "    print ('\\nBoucle {}. Feature {} supprimée  : score = {}'.format(\n",
        "        num_feature+1, \n",
        "        list_features[feature_to_remove_index],\n",
        "        scores[feature_to_remove_index], \n",
        "        min(scores)))\n",
        "                \n",
        "    #on identifie les numero de colonne des features à supprimer:\n",
        "    \n",
        "    if regex.search(feature_name):\n",
        "        print('      indice colonne de base à supprimer : ', index_cols[feature_to_remove_index])\n",
        "        print('      indice de la colonne correspondant à la prochaine feature OHE', index_cols[feature_to_remove_index]+len(dict_OHE[feature_name[:2]]))\n",
        "        liste_index = list(\n",
        "            range(index_cols[feature_to_remove_index],\n",
        "                  index_cols[feature_to_remove_index]+len(dict_OHE[feature_name[:2]])\n",
        "                  ,1)\n",
        "        )\n",
        "    else:\n",
        "        liste_index = index_cols[feature_to_remove_index]\n",
        "    #on actualise la liste des features avec la feature en moins pour le prochain tour de boucle\n",
        "    for k, indice in enumerate(index_cols):\n",
        "        if index_cols.index(indice) > feature_to_remove_index:\n",
        "            if type(liste_index) is int :\n",
        "                index_cols[k] -= 1\n",
        "            else:\n",
        "                index_cols[k] -= len(liste_index)\n",
        "        \n",
        "    print('      index à supprimer ', liste_index)\n",
        "    \n",
        "    del list_features[feature_to_remove_index]\n",
        "    if regex.search(feature_name):\n",
        "        del dict_OHE[feature_name[:2]]\n",
        "        \n",
        "    del index_cols[feature_to_remove_index]\n",
        "    \n",
        "    print('      feature to remove index' , liste_index)\n",
        "    print('      Colonnes supprimées : ', X_train_permut.columns[liste_index])\n",
        "    X_train_permut.drop(X_train_permut.columns[liste_index] , axis = 1, inplace=True)\n",
        "    X_validation_permut.drop(X_validation_permut.columns[liste_index] , axis = 1, inplace=True)\n",
        "    #print(X_train_permut.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7pJfOFW0d_28"
      },
      "outputs": [],
      "source": [
        "features_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "24uJWYhueESd"
      },
      "outputs": [],
      "source": [
        "graphe_features = features_score.reset_index().drop(['index'], axis=1).reset_index()\n",
        "graphe_features['index'] +=1\n",
        "graphe_features['RMSE'] = (abs(graphe_features['RMSE'])).apply(math.sqrt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "h--29IAbeICt"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "plt.figure(figsize=(15,5))\n",
        "plt.title('Evolution du score en supprimant des features par permutation')\n",
        "sns.lineplot( x = graphe_features['index'].values,\n",
        "            y = graphe_features[\"RMSE\"])\n",
        "sns.lineplot([1,graphe_features.shape[0]],[graphe_features.iloc[0,2], graphe_features.iloc[0,2]])\n",
        "plt.xlabel('Nombre de features supprimées')\n",
        "plt.ylabel('Score : RMSE')\n",
        "plt.legend(['Score avec suppression des features',\n",
        "           'Score de référence'])\n",
        "ax = plt.gca()\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xrSVPaNePek"
      },
      "source": [
        "On n'observe un minima local pour x = 2 (soit 2 features supprimées). On peut supprimer 5 features en améliorant un peu le modèle.\n",
        "\n",
        "on observe qu'on peut supprimer jusqu'à 8 features avec un impact marginal sur le score.\n",
        "\n",
        "on observe que la suppression des 5 dernières features est particulièrement impactante"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DXKP5Ye5eKkN"
      },
      "outputs": [],
      "source": [
        "graphe_features['feature'].replace({\n",
        "    'x0_' : 'BuildingType', \n",
        "    'x1_' : 'PrimaryPropertyType',\n",
        "    'x2_' : 'Neighborhood',\n",
        "    'x3_' : 'LargestPropertyUseType',\n",
        "    'x4_' : 'SecondLargestPropertyUseType',\n",
        "    'x5_' : 'ThirdLargestPropertyUseType', \n",
        "    'x6_' : 'Outlier'\n",
        "}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Tzh9R9w2eZoC"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.title('RMSE obtenue après suppression successives des features')\n",
        "sns.barplot(x = graphe_features['RMSE'],\n",
        "           y = graphe_features['feature'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pjrjed2lemYs"
      },
      "source": [
        "# **Bilan des permutations**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJTFZsPceplC"
      },
      "source": [
        "Les résultats obtenus ci-dessus sont assez cohérents\n",
        "\n",
        "les 2 features les plus significatives sont la surface dédiée à l'usage principal du bâtiment et son usage principal.\n",
        "\n",
        "La latitude a plus d'importance que la longitude. C'est peut être lié à la configuration de la ville, dont la distance nord sud est plus importante que la distance est-ouest (voir représentation cartographique dans l'analyse de données).\n",
        "\n",
        "certaines features sont d'importance moyenne : type de bâtiment, nombre de bâtiments, nombre d'étages 3ème type d'usage.\n",
        "\n",
        "\n",
        "Pour simplifier la collecte de données, on pourrait éventuellement réduire notre jeu de données aux features suivantes avec une dégradation minimale du score :\n",
        "\n",
        "LargestPropertyUseTypeGFA\n",
        "LargestPropertyUseType\n",
        "Numberoffloors\n",
        "Latitude\n",
        "Building Type\n",
        "\n",
        "On pourrait aussi légèrement améliorer le modèle en supprimant les 5 features suivantes du jeu de données:\n",
        "\n",
        "ThirdLargestPropertyUseType\n",
        "Longitude\n",
        "PrimaryPropertyType\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0w54Kw9e-XP"
      },
      "source": [
        "# **Modèle emissions**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "L6By22R2ebwx"
      },
      "outputs": [],
      "source": [
        "y2 = data.copy()[{'SiteEnergyUseWN(kBtu)', 'TotalGHGEmissions' , 'Log2-SiteEnergyUseWN(kBtu)'}]\n",
        "X2 = data.copy().drop(['SiteEnergyUseWN(kBtu)', 'TotalGHGEmissions', 'Log2-SiteEnergyUseWN(kBtu)'], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "kwR1iV-ufQot"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "ohe = OneHotEncoder(sparse=False)\n",
        "ss = StandardScaler()\n",
        "X2[numerical_columns] = ss.fit_transform(X2[numerical_columns])\n",
        "ohe.fit_transform(X2[categorical_columns])\n",
        "\n",
        "X2 = pd.merge(X[numerical_columns], \n",
        "          pd.DataFrame(columns = ohe.get_feature_names().tolist(),\n",
        "              data = ohe.fit_transform(X2[categorical_columns])),\n",
        "        left_index = True, right_index = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hBgqZnK9fTSs"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train2, X_test2, y_train2, y_test2 = \\\n",
        "                        train_test_split(X2, \n",
        "                                         y['TotalGHGEmissions'],  \n",
        "                                         test_size = 0.3, \n",
        "                                         random_state = 42\n",
        "                                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MTPsEb6gfXra"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "parameters = {\n",
        "    'n_estimators' : [10,50,100,300,500], #nombre d'arbres de décision\n",
        "    'min_samples_leaf' : [1,3,5,10], #nombre de feuilles minimales dans un noeud\n",
        "    'max_features': ['auto', 'sqrt'] #nombre de features observées pour chaque arbre\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JGZIHg2wfcHr"
      },
      "outputs": [],
      "source": [
        "rfr_emissions = GridSearchCV(RandomForestRegressor(),\n",
        "                               param_grid = parameters,\n",
        "                               #scoring='mean_squared_error',\n",
        "                              verbose=2,\n",
        "                               cv=5)\n",
        "\n",
        "rfr_emissions.fit(X_train2, y_train2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AmRq7HTxfexr"
      },
      "outputs": [],
      "source": [
        "model_emissions = rfr_emissions.best_estimator_\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KK5hsoQbfjuH"
      },
      "source": [
        "## **Export du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pbOjxj_jfikX"
      },
      "outputs": [],
      "source": [
        "filename = 'rfr_emissions' \n",
        "with  open(filename, 'wb') as filehandler :\n",
        "    pickle.dump(model_emissions, filehandler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1zM-fnVFfrHB"
      },
      "outputs": [],
      "source": [
        "math.sqrt(mean_squared_error(model_emissions.predict(X_test2), y_test2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "N7mI4MSZftLx"
      },
      "outputs": [],
      "source": [
        "math.sqrt(mean_squared_error(model_emissions.predict(X_test2), y_test2))/y_test2.mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "htppul3Gfurh"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(y2.corr())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTkRigvBf2ng"
      },
      "source": [
        "On observe que les variables d'émissions et de consommation sont fortement corrélées (coefficient de 0,72). Voyons si nous pouvons entrainer un modèle d'estimation des émissions à partir de la sortie du modèle de prédiction des consommations\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "z4CgwZ1_fxJT"
      },
      "outputs": [],
      "source": [
        "rfr_emissions_court = GridSearchCV(RandomForestRegressor(n_jobs=-1),\n",
        "                               param_grid = parameters,\n",
        "                               #scoring='mean_squared_error',\n",
        "                              verbose=2,\n",
        "                               cv=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_T4QSn2mf5Ua"
      },
      "outputs": [],
      "source": [
        "rfr_emissions_court.fit(np.vstack([dict_modeles['Random Forest Regressor'].predict(X_train2), np.ones(X_train2.shape[0])]).T,\n",
        "                          y_train2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5n1TBPx7f7VB"
      },
      "outputs": [],
      "source": [
        "math.sqrt(mean_squared_error(rfr_emissions_court.best_estimator_.predict(\n",
        "    np.vstack([dict_modeles['Random Forest Regressor'].predict(X_test2), np.ones(X_test2.shape[0])]).T),\n",
        "                             y_test2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxHyLQSbgF44"
      },
      "source": [
        "La RMSE est quasiment égale que dans le cas du modèle avec toutes les features\n",
        "\n",
        "Essayons en ajoutant en entrée du modèle de prédiction, la prédiction du modèle initial ainsi que les 5 features importantes du modèle initial, selon la permutation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YLT3hILjf-he"
      },
      "outputs": [],
      "source": [
        "columns_feature_emissions = ['LargestPropertyUseTypeGFA', 'x3_Education', 'x3_Facility', 'x3_Health',\n",
        "       'x3_Leisure', 'x3_Office', 'x3_Offices', 'x3_Other', 'x3_Parking',\n",
        "       'x3_Personal Services (Health/Beauty, Dry Cleaning, etc)',\n",
        "       'x3_Residence/Hotel/Senior Care/Housing', 'x3_Retail', 'x3_Storage', \n",
        "                            'NumberofFloors', 'Latitude', 'x0_Campus', 'x0_Multifamily HR (10+)',\n",
        "       'x0_Multifamily LR (1-4)', 'x0_Multifamily MR (5-9)',\n",
        "       'x0_NonResidential', 'x0_Nonresidential COS', 'x0_Nonresidential WA',\n",
        "       'x0_SPS-District K-12']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "JXulorgrgIqB"
      },
      "outputs": [],
      "source": [
        "dict_modeles['Random Forest Regressor'].predict(X_train2).reshape(-1,1).shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PfvG-UF0gKe5"
      },
      "outputs": [],
      "source": [
        "X_train2[columns_feature_emissions].shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "srmgMswng6A1"
      },
      "outputs": [],
      "source": [
        "rfr_emissions_court2 = GridSearchCV(RandomForestRegressor(n_jobs=-1),\n",
        "                               param_grid = parameters,\n",
        "                               #scoring='mean_squared_error',\n",
        "                              verbose=2,\n",
        "                               cv=5)\n",
        "\n",
        "rfr_emissions_court2.fit(np.hstack([dict_modeles['Random Forest Regressor'].predict(X_train2).reshape(-1,1), X_train2[columns_feature_emissions].to_numpy()]),\n",
        "                          y_train2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Gfh_LxzZg-dT"
      },
      "outputs": [],
      "source": [
        "math.sqrt(mean_squared_error(rfr_emissions_court2.predict(\n",
        "    np.hstack([dict_modeles['Random Forest Regressor'].predict(X_test2).reshape(-1,1), \n",
        "               X_test2[columns_feature_emissions].to_numpy()])),y_test2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2Ia1nNJEhCk1"
      },
      "outputs": [],
      "source": [
        "filename = 'rfr_emission'\n",
        "with  open(filename, 'wb') as filehandler :\n",
        "    pickle.dump(rfr_emissions_court2, filehandler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nHsRbzMphFs8"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.title('Comparaison des émissions prédites et réelles')\n",
        "sns.lineplot(x = [0,1750], y = [0,1750], alpha=0.3)\n",
        "sns.scatterplot(x = model_emissions.predict(X_test2),\n",
        "                y = y_test2, alpha=0.5)\n",
        "sns.scatterplot(x = rfr_emissions_court2.predict(np.hstack([dict_modeles['Random Forest Regressor'].predict(X_test2).reshape(-1,1), \n",
        "               X_test2[columns_feature_emissions].to_numpy()])),\n",
        "                y = y_test2, alpha=0.5)\n",
        "plt.legend(['ligne x=y', 'modèle classique,', 'modèle simplifié'])\n",
        "ax = plt.gca()\n",
        "ax.set_aspect('equal')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kES85XWuhL31"
      },
      "source": [
        "# **Energy Star Score**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1NcQL_NhQ4I"
      },
      "source": [
        "Entrainement d'un nouveau modèle avec le Energy Star Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZjgJOA2mhIGS"
      },
      "outputs": [],
      "source": [
        "X_en = X.copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AQQZPt4HhdHy"
      },
      "outputs": [],
      "source": [
        "X_en['energy_star_score'] = energy_star_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iR1S-3MwheyZ"
      },
      "outputs": [],
      "source": [
        "y_en = y['Log2-SiteEnergyUseWN(kBtu)'].copy()\n",
        "y_en.drop(X_en[X_en['energy_star_score'].isna()].index, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "J6g2K-LwhhYx"
      },
      "outputs": [],
      "source": [
        "X_en.drop(X_en[X_en['energy_star_score'].isna()].index, inplace=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "UbaxH9IHhjLX"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_en_train, X_en_test, y_en_train, y_en_test = \\\n",
        "                        train_test_split(X_en, \n",
        "                                         y_en,  \n",
        "                                         test_size = 0.2, \n",
        "                                         random_state = 42\n",
        "                                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rQM6GnkIhlKK"
      },
      "outputs": [],
      "source": [
        "model_en = GridSearchCV(RandomForestRegressor(),\n",
        "                               param_grid = parameters,\n",
        "                               #scoring='mean_squared_error',\n",
        "                              verbose=2,\n",
        "                               cv=5)\n",
        "\n",
        "model_en.fit(X_en_train, y_en_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_DQNjD0FhndH"
      },
      "outputs": [],
      "source": [
        "math.sqrt(mean_squared_error(model_en.predict(X_en_test), y_en_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chc4luwRJw34"
      },
      "source": [
        "## **Entrainement d'un autre modèle sans le Energy Star Score**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "rE19Xkc-hqXe"
      },
      "outputs": [],
      "source": [
        "X_en_train.drop(['energy_star_score'], axis=1, inplace=True)\n",
        "X_en_test.drop(['energy_star_score'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lX6ZBdCuhsgu"
      },
      "outputs": [],
      "source": [
        "model_en = GridSearchCV(RandomForestRegressor(),\n",
        "                               param_grid = parameters,\n",
        "                               #scoring='mean_squared_error',\n",
        "                              verbose=2,\n",
        "                               cv=5)\n",
        "\n",
        "model_en.fit(X_en_train, y_en_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_KGBArgOhwvQ"
      },
      "outputs": [],
      "source": [
        "math.sqrt(mean_squared_error(model_en.predict(X_en_test), y_en_test))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cNQ-T57nShOp2lm76grctToycJ1qTdBy",
      "authorship_tag": "ABX9TyO8Vzf3qyvTaXxr/hLkTpTg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}